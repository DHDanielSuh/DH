{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "* 1990년대에 개발됨\n",
    "* 당시 가장 핫하던 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강의 순서\n",
    "\n",
    "* Maximal Margin Classifier\n",
    "* Support Vector Classifier : Maximal Margin Classifier의 일반화\n",
    "* Support Vector Machines : Support Vector Classifier의 일반화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직관적으로 이해하기\n",
    "\n",
    "Maximal Margin Classifier\n",
    "\n",
    "* 두가지만 알면 된다.\n",
    "\n",
    "  * 초평면 (hyperplane)\n",
    "  * 최적 분할 초평면 (optimal separating hyperplanne) 또는 최대 마진 초평면 (maximal margin hyperplane)\n",
    "\n",
    "* 초평면 (hyperplane)\n",
    "\n",
    "  * 2차원 평면을 둘로 나누는 데는 1차원 직선이면 된다.\n",
    "  * 3차원 공간을 둘로 나누는 데는 2차원 평면이면 된다.\n",
    "  * p차원 공간을 둘로 나누는 데는 (p-1) 차원 초평면(hyperplane)이면 된다.\n",
    "\n",
    "  ![hyperplane](https://files.slack.com/files-pri/T25783BPY-F64KHFT6J/screenshot.png?pub_secret=50c521c26b)\n",
    "\n",
    "* 분류기(classifier)를 만드는 가장 자연스러운 아이디어는 데이터를 둘로 나누는 초평면을 찾는 것이다.\n",
    "\n",
    "  * 둘로 나누는 초평면이 무수히 많으면 어떻게 하나?\n",
    "\n",
    "* 학습 데이터로부터 가장 거리(margin)가 멀리 떨어져 있는 초평면을 찾는다.\n",
    "\n",
    "  * 초평면에서 가장 가까이 있는 데이터 포인트까지의 거리를 margin이라고 부른다.\n",
    "  * 최대 마진(Maximal Margin)을 가지는 초평면을 찾는다!\n",
    "  * 이렇게 정한 초평면을 최대 마진 초평면 (Maximal margin hyperplane) 또는 최적 분할 초평면 (optimal separating hyperplanne)이라고 부른다.\n",
    "\n",
    "![seperating](https://files.slack.com/files-pri/T25783BPY-F63SEQNQ4/screenshot.png?pub_secret=b2086bf00e)\n",
    "\n",
    "![hy](https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Svm_separating_hyperplanes.png/1280px-Svm_separating_hyperplanes.png)\n",
    "\n",
    "* 이 때, 초평면에서 margin 안에 있는 데이터 포인트들을 support vectors라고 부른다.\n",
    "  * support vectors는 최대 마진 초평면을 지지(support)한다.\n",
    "  * 즉, 얘네들이 움직이면 초평면도 움직인다.\n",
    "\n",
    "- 다른 데이터는 아무리 움직여도 (margin의 경계선을 넘어가지 않는다면) 초평면에는 전혀 영향을 주지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Classifier\n",
    "\n",
    "* 최대 마진 초평면(Maximal Margin Hyperplane)에는 문제가 있다.\n",
    "\n",
    "  * 만약 데이터가 완벽히 두쪽으로 쪼개지지 않는다면?\n",
    "  * 설사 완벽히 둘로 나눠진다고 해도, 너무 한두개 데이터 포인트에 민감해진다. (overfitting)\n",
    "\n",
    "  ![overlapping](https://files.slack.com/files-pri/T25783BPY-F63T9B4BE/screenshot.png?pub_secret=5ffa820547)\n",
    "\n",
    "  ![variance](https://files.slack.com/files-pri/T25783BPY-F63T9H4G4/screenshot.png?pub_secret=88365fc3db)\n",
    "\n",
    "* margin을 너무 깐깐하게 적용하지 말자.\n",
    "\n",
    "  * 몇몇 데이터 포인트에 너무 민감해지지 말자.\n",
    "  * *대부분* 의 데이터를 제대로 분류하는 데 더 신경쓰자.\n",
    "\n",
    "* Soft margin\n",
    "\n",
    "  * 어느 정도는 데이터 포인트가 margin을 넘어가거나, 심지어 hyperplane을 넘어가는 것(violation)도 허용해주겠다.\n",
    "  * 허용해주는 정도는 C라는 hyperparameter로 정하겠다. (detailed later)\n",
    "  * C가 클수록 허용하는 정도가 커지고, C가 작을 수록 더 깐깐해진다. \n",
    "\n",
    "* margin 위에 있거나, margin 안에 있는 데이터 포인트들을 support vectors라고 부른다. 역시 이것들이 support vector classifier를 결정한다.\n",
    "\n",
    "![svc](https://files.slack.com/files-pri/T25783BPY-F63PMKX5H/screenshot.png?pub_secret=40e4e948c3)\n",
    "\n",
    "![C](https://files.slack.com/files-pri/T25783BPY-F63TATU84/screenshot.png?pub_secret=11fd8e019e)\n",
    "\n",
    "\n",
    "\n",
    "Support Vector Machines\n",
    "\n",
    "- Support Vector Classifier의 문제점\n",
    "  - decision boundary가 비선형(non-linear)이면 어떻게 하나?\n",
    "\n",
    "![non-linear](https://files.slack.com/files-pri/T25783BPY-F64LERAAJ/screenshot.png?pub_secret=f2bae55344)\n",
    "\n",
    "* 다항(polynomial) 변수들을 추가하면 되지!\n",
    "  * x1의 제곱, 세제곱, 등등을 feature로 넣기\n",
    "  * 맞는 말이다.\n",
    "  * 하지만 계산이 너무 힘들다.\n",
    "  * 결국 핵심은 변수 공간을 늘리기(enlarging the feature space)\n",
    "  * support vector machinne은 커널(kernel)로 이걸 좀 더 쉽게 한다.\n",
    "* Kernel?\n",
    "  * 커널(kernel)은 고차원에서의 계산을 더 빠르게 할 수 있게 하는 하나의 방법이다.\n",
    "  * x = (x1, x2, x3); y = (y1, y2, y3)이고\n",
    "  * f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1, x3x2, x3x3)이며\n",
    "  * 커널 K(x, y ) = (\\<x, y\\>)^2 라고 해보자.\n",
    "  * 예를 들어, x = (1, 2, 3)이고 y = (4, 5, 6)일 때,\n",
    "  * f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)\n",
    "  * f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)\n",
    "  * \\<f(x), f(y)\\> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024\n",
    "  * 계산을 엄청 많이 해야한다. 3차원을 9차원으로 올렸기 때문이다.\n",
    "  * 하지만 커널을 쓰면\n",
    "  * K(x, y) = (4 + 10 + 18 ) ^2 = 32^2 = 1024\n",
    "  * 계산이 훨씬 쉬워진다.\n",
    "* radial kernel 같은 커널은 수학적으로 변수를 무한 차원까지 올릴 수 있다.\n",
    "* SVM은 커널을 이용해 변수 공간(feature space)를 확장해서 데이터 포인트들이 선형적으로 나뉘어질 수 있게 (linearly separable) 만든다.\n",
    "\n",
    "![non-linear](https://files.slack.com/files-pri/T25783BPY-F64M2F1GF/screenshot.png?pub_secret=c6962e480a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수식으로 이해하기\n",
    "\n",
    "Maximal Margin Classifier\n",
    "\n",
    "* 2차원 초평면(hyperplane)은 다음과 같이 간단하게 정의된다.\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1x_1 + \\beta_2x_2 = 0\n",
    "$$\n",
    "\n",
    "* 위 수식을 만족하는 x1과 x2가 초평면 위에 있는 점들이 된다.\n",
    "  * 생각해보면 그냥 직선의 방정식이다.\n",
    "\n",
    "* 다음과 같이 p차원으로 쉽게 확장할 수 있다.\n",
    "\n",
    "* $$\n",
    "  \\beta_0 + \\beta_1x_1 + \\beta_2x_2 +... +\\beta_px_p= 0\n",
    "  $$\n",
    "\n",
    "* 만약 위의 수식을 만족하지 않고, 다음과 같은 데이터 포인트가 있다면 그 데이터 포인트는 초평면의 한 쪽에 있다는 뜻이 된다.\n",
    "\n",
    "* $$\n",
    "  \\beta_0 + \\beta_1x_1 + \\beta_2x_2 +... +\\beta_px_p > 0\n",
    "  $$\n",
    "\n",
    "* 반대로 아래 식을 만족하면 초평면의 다른 쪽에 있다는 뜻이 된다.\n",
    "\n",
    "* $$\n",
    "  \\beta_0 + \\beta_1x_1 + \\beta_2x_2 +... +\\beta_px_p < 0\n",
    "  $$\n",
    "\n",
    "* 즉, 위 식의 좌변을 계산함으로써 데이터 포인트가 초평면의 어느 쪽에 있는지 쉽게 결정할 수 있다.\n",
    "\n",
    "* 만약 y를 1 또는 -1로 정의한다면, i번째 데이터 포인트에 대해서 다음을 만족해야 한다. (SVM에서는 흔히 0,1 말고 -1, 1로 분류한다.)\n",
    "\n",
    "* $$\n",
    "  y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} +... +\\beta_px_{ip}) > 0\n",
    "  $$\n",
    "\n",
    "* 좌변의 값이 0에서 멀 수록 예측값에 대해서 확신할 수 있고, 0에서 멀수록 신뢰도가 낮아질 것이다.\n",
    "\n",
    "\n",
    "\n",
    "* 최적의 beta 계수들을 구하는 방법은 (fitting or training)\n",
    "\n",
    "$$\n",
    "maximize_{\\beta_0, \\beta_1, ... , \\beta_p}M\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to } \\sum^p_{j=1}\\beta^2_j = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} +... +\\beta_px_{ip}) \\geq M\n",
    "$$\n",
    "\n",
    "* 모든 데이터 포인트가 초평면에서 M만큼 떨어져 있을 때 성립한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Classifiers\n",
    "\n",
    "* $$\n",
    "  maximize_{\\beta_0, \\beta_1, ... , \\beta_p, \\epsilon_1, ..., \\epsilon_n}M\n",
    "  $$\n",
    "\n",
    "* $$\n",
    "  \\text{subject to } \\sum^p_{j=1}\\beta^2_j = 1\n",
    "  $$\n",
    "\n",
    "* $$\n",
    "  y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} +... +\\beta_px_{ip}) \\geq M(1 - \\epsilon_i)\n",
    "  $$\n",
    "\n",
    "* $$\n",
    "  \\epsilon_i \\geq 0, \\sum^n_{i=1}\\epsilon_i \\leq C\n",
    "  $$\n",
    "\n",
    "* C는 tuning parameter 또는 hyperparameter이다.\n",
    "\n",
    "* M은 margin의 크기이다. 우리는 margin을 최대로 만들려고 한다.\n",
    "\n",
    "* epsilon_i는 각각의 데이터 포인트들이 마진 안에 있거나 초평면의 반대편에 있는 것을 허용해주는 변수이다.\n",
    "\n",
    "  * epsilon_i = 0이면 마진 너머에 있다는 뜻\n",
    "  * epsilon_i > 0이면 마진 안에 있다는 뜻. margin을 침범(violate)했다고 말한다.\n",
    "  * epsilon_i > 1이면 초평면의 반대편에 있다는 뜻.\n",
    "\n",
    "* C는 epsilon의 합을 제한한다.\n",
    "\n",
    "  * margin이 침범(violate)될 수 있는 최대의 예산(budget)이라고 볼 수 있다.\n",
    "  * C = 0이면 epsilon을 모두 0으로 강제하게 되고, 최대 마진 초평면(maximal margin hyperplane)으로 돌아가게 된다.\n",
    "  * C > 0이면, C개 이상의 데이터 포인트들은 초평면의 반대편에 있을 수 없다.\n",
    "\n",
    "\n",
    "\n",
    "Support Vector Machines\n",
    "\n",
    "* 수학적으로, 두 벡터 사이의 거리는 지금까지 내적(inner product)으로 계산해왔다.\n",
    "\n",
    "* $$\n",
    "  <x_i, x_{i'}> = \\sum^p_{j=1}x_{ij}x_{i'j}\n",
    "  $$\n",
    "\n",
    "* 그러나 거리를 구하는 방법은 내적(inner product)만 있는 것이 아니다.\n",
    "\n",
    "* 내적의 일반화인 kernel K로 이를 표현해보자. \n",
    "\n",
    "* $$\n",
    "  K(x_i, x_{i'})\n",
    "  $$\n",
    "\n",
    "* kernel은 두 벡터가 얼마나 비슷한지를 나타낸다. \n",
    "\n",
    "* linear kernel\n",
    "\n",
    "  * $$\n",
    "    K(x_i, x_{i'}) = \\sum^p_{j=1}x_{ij}x_{i'j}\n",
    "    $$\n",
    "\n",
    "* polynomial kernel of degree d\n",
    "\n",
    "* $$\n",
    "  K(x_i, x_{i'}) =  (1 + \\sum^p_{j=1}x_{ij}x_{i'j})^d\n",
    "  $$\n",
    "\n",
    "* d = 1일 때 support vector classifier와 같아진다.\n",
    "\n",
    "* radial kernel\n",
    "\n",
    "* $$\n",
    "  K(x_i, x_{i'}) =  exp(-\\gamma \\sum^p_{j=1} (x_{ij} - x_{i'j})^2)\n",
    "  $$\n",
    "\n",
    "* 두 벡터 간의 거리가 멀면 radial kernel의 값은 급격하게 작아진다. 즉, 가까이 있는 벡터밖에 예측에 영향을 주지 않는다. 굉장히 지역적인(local) 모델이 된다.\n",
    "\n",
    "* 그래서 non-linearity가 심해지고, 더욱 복잡한 모델이 되며, 과적합(overfitting)될 수 있다.\n",
    "\n",
    "* 반대로 말하면, 주어진 문제와 데이터가 복잡할 때 그것을 더욱 잘 표현할 수 있는 모델이 된다.\n",
    "\n",
    "* gamma를 조절해서 얼마나 복잡한 모델로 만들지를 결정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀(Logistic Regression)와의 관계\n",
    "\n",
    "* SVM의 식은 다음과 같이 바꾸어서 표현할 수도 있다.\n",
    "\n",
    "![ads](https://files.slack.com/files-pri/T25783BPY-F63BJUL8Y/screenshot.png?pub_secret=370457a109)\n",
    "\n",
    "* lambda는 hyperparameter로, C에 대응된다.\n",
    "\n",
    "* 이 공식은 다음과 같은 일반적인 Loss + Penalty의 형식을 그대로 따르고 있다.\n",
    "\n",
    "* loss function (cost function)만 다른 것이다.\n",
    "\n",
    "* 이런 loss functionn을 hinge loss라고 부른다.\n",
    "\n",
    "* 다음과 같을때, loss는 0이 된다.\n",
    "\n",
    "* $$\n",
    "  y_if(x_i) \\geq 1\n",
    "  $$\n",
    "\n",
    "* 즉, 제대로 예측했을 때 loss는 0인 것이다. (여기서는 margin이 1로 표준화되었다.)\n",
    "\n",
    "* 하지만, 로지스틱 회귀의 loss는 잘 맞춘다고 0이 되지는 않는다.\n",
    "\n",
    "* ![adsf](https://files.slack.com/files-pri/T25783BPY-F64SBF6NB/screenshot.png?pub_secret=0ce25546ba)\n",
    "\n",
    "* 언제 logistic regression을 쓰고 언제 SVM을 쓸까?\n",
    "\n",
    "  * 클래스들이 잘 나눠질 때는 SVM이 좀 더 잘 작동하는 경향이 있다.\n",
    "  * 클래스들이 겹칠 때는 로지스틱 회귀가 좀 더 선호된다.\n",
    "  * 절대적인 것은 아니다.\n",
    "\n",
    "* 그럼 kernel을 이용해서 변수 공간을 늘리는 것(enlarging feature space)을 로지스틱 회귀에도 쓸 수 있지 않나?\n",
    "\n",
    "  * 그렇다.\n",
    "  * 하지만 관습적인 이유로 kernel은 주로 SVM에서만 쓰인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for regression\n",
    "\n",
    "* SVM을 회귀에도 쓸 수 있다.\n",
    "* Support vector regression이라고 불린다.\n",
    "* 선형 회귀에서는 Sum of squared residuals를 최소화했다.\n",
    "* Support vector regression은 일정 수준 이상의 residual을 넘어가는 것만 최소화한다.\n",
    "* 즉, Support vector classifier처럼 margin을 넘어가는 것만 신경쓰는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn에서 회귀를 위한 SVM을 구현한 클래스로는 SVC, NuSVC, 그리고 LinearSVC가 있다. 이 셋은 조금씩 다른 수학적인 형태와 구현을 갖고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C : 높으면 오버피팅, 낮으면 언더피팅 가능성\n",
    "- kernel : ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ 중 하나. 기본은 `rbf`. (Radial basis function)\n",
    "- degree : polynomial kernel을 쓸 때 degree. 기본은 3.\n",
    "- gamma : rbf 등의 커널을 위해서. 기본은 1/n_features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀를 위한 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = svm.SVR()\n",
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[1, 1]])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
