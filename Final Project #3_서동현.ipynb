{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ▣ 비즈니스 데이터마이닝\n",
    "\n",
    "### 문제 3\n",
    "\n",
    "### 목표: 국회 의원들의 Ideo_self 값 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용한 Data set: data1.csv\n",
    "\n",
    "### Ideo_self 에 NA 값이 있는 데이터를 Test set으로 보고 나머지 데이터를 Train set으로 보았다.  Train set을 통해 모델들을 학습시키고 학습한 모델을 토대로 Test set의 Ideo_self 값을 추정해 보았다. \n",
    "\n",
    "다음 모델들의 테스트 성능을 비교해 본다.\n",
    "1. Logistic regression\n",
    "2. k-nearest neighbor classifier\n",
    "3. naive Bayes classifier\n",
    "4. Decision tree\n",
    "5. Random forest\n",
    "6. SVM\n",
    "7. Xgboost\n",
    "8. SoftMax\n",
    "9. Keras + Relu + SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('data1.xlsx', header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sex</th>\n",
       "      <th>birth</th>\n",
       "      <th>age1</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>edu</th>\n",
       "      <th>income</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "      <th>k4</th>\n",
       "      <th>k6</th>\n",
       "      <th>k7</th>\n",
       "      <th>k8</th>\n",
       "      <th>k10</th>\n",
       "      <th>k12</th>\n",
       "      <th>k13</th>\n",
       "      <th>k14</th>\n",
       "      <th>ideo_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281.0</th>\n",
       "      <td>1012.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393.0</th>\n",
       "      <td>556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604.0</th>\n",
       "      <td>642.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956.0</th>\n",
       "      <td>403.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212.0</th>\n",
       "      <td>776.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sex   birth  age1  age  area  edu  income   k2   k3   k4   k6   k7   k8  k10  k12  \\\n",
       "281.0  1012.0  1.0  1994.0  22.0  2.0   1.0  2.0     1.0  1.0  0.0  0.0  1.0  0.0  0.0  NaN  1.0   \n",
       "393.0   556.0  1.0  1957.0  59.0  5.0   8.0  4.0     1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  0.0   \n",
       "604.0   642.0  1.0  1990.0  26.0  2.0   4.0  2.0     1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0   \n",
       "956.0   403.0  2.0  1966.0  50.0  5.0   2.0  4.0     7.0  NaN  1.0  0.0  0.0  NaN  0.0  NaN  NaN   \n",
       "212.0   776.0  1.0  1978.0  38.0  3.0   1.0  4.0     1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0   \n",
       "\n",
       "       k13  k14  ideo_self  \n",
       "281.0  0.0  NaN        5.0  \n",
       "393.0  1.0  1.0        7.0  \n",
       "604.0  0.0  1.0       10.0  \n",
       "956.0  0.0  0.0        3.0  \n",
       "212.0  0.0  1.0        6.0  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID column 앞에 있는 값들은 단순 index 값이므로 ID column을 새로운 index를 값으로 바꿔준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = data.set_index(\"id\").sort_index()\n",
    "data = data.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>birth</th>\n",
       "      <th>age1</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>edu</th>\n",
       "      <th>income</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "      <th>k4</th>\n",
       "      <th>k6</th>\n",
       "      <th>k7</th>\n",
       "      <th>k8</th>\n",
       "      <th>k10</th>\n",
       "      <th>k12</th>\n",
       "      <th>k13</th>\n",
       "      <th>k14</th>\n",
       "      <th>ideo_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sex   birth  age1  age  area  edu  income   k2   k3   k4   k6   k7   k8  k10  k12  k13  \\\n",
       "id                                                                                               \n",
       "1012.0  1.0  1994.0  22.0  2.0   1.0  2.0     1.0  1.0  0.0  0.0  1.0  0.0  0.0  NaN  1.0  0.0   \n",
       "556.0   1.0  1957.0  59.0  5.0   8.0  4.0     1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  0.0  1.0   \n",
       "642.0   1.0  1990.0  26.0  2.0   4.0  2.0     1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "403.0   2.0  1966.0  50.0  5.0   2.0  4.0     7.0  NaN  1.0  0.0  0.0  NaN  0.0  NaN  NaN  0.0   \n",
       "776.0   1.0  1978.0  38.0  3.0   1.0  4.0     1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0   \n",
       "767.0   1.0  1966.0  50.0  5.0   2.0  4.0     1.0  0.0  1.0  0.0  1.0  0.0  0.0  NaN  0.0  0.0   \n",
       "594.0   1.0  1982.0  34.0  3.0   2.0  2.0     1.0  NaN  NaN  1.0  1.0  NaN  0.0  NaN  0.0  0.0   \n",
       "869.0   1.0  1965.0  51.0  5.0   6.0  4.0     2.0  1.0  0.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0   \n",
       "719.0   1.0  1966.0  50.0  5.0   9.0  4.0     1.0  0.0  1.0  1.0  1.0  NaN  0.0  1.0  0.0  0.0   \n",
       "783.0   1.0  1956.0  60.0  6.0   1.0  2.0     2.0  1.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "\n",
       "        k14  ideo_self  \n",
       "id                      \n",
       "1012.0  NaN        5.0  \n",
       "556.0   1.0        7.0  \n",
       "642.0   1.0       10.0  \n",
       "403.0   0.0        3.0  \n",
       "776.0   1.0        6.0  \n",
       "767.0   1.0        5.0  \n",
       "594.0   1.0        5.0  \n",
       "869.0   1.0        5.0  \n",
       "719.0   0.0        4.0  \n",
       "783.0   1.0        8.0  "
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 내에 모든 column 값이 NA인 값을 걸러준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054, 18)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과물을 원하는 모양으로 보여주기 위한 옵션을 건다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>birth</th>\n",
       "      <th>age1</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>edu</th>\n",
       "      <th>income</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "      <th>k4</th>\n",
       "      <th>k6</th>\n",
       "      <th>k7</th>\n",
       "      <th>k8</th>\n",
       "      <th>k10</th>\n",
       "      <th>k12</th>\n",
       "      <th>k13</th>\n",
       "      <th>k14</th>\n",
       "      <th>ideo_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sex   birth  age1  age  area  edu  income   k2   k3   k4   k6   k7   k8  k10  k12  k13  \\\n",
       "id                                                                                               \n",
       "1012.0  1.0  1994.0  22.0  2.0   1.0  2.0     1.0  1.0  0.0  0.0  1.0  0.0  0.0  NaN  1.0  0.0   \n",
       "556.0   1.0  1957.0  59.0  5.0   8.0  4.0     1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  0.0  1.0   \n",
       "642.0   1.0  1990.0  26.0  2.0   4.0  2.0     1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "403.0   2.0  1966.0  50.0  5.0   2.0  4.0     7.0  NaN  1.0  0.0  0.0  NaN  0.0  NaN  NaN  0.0   \n",
       "776.0   1.0  1978.0  38.0  3.0   1.0  4.0     1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0   \n",
       "\n",
       "        k14  ideo_self  \n",
       "id                      \n",
       "1012.0  NaN        5.0  \n",
       "556.0   1.0        7.0  \n",
       "642.0   1.0       10.0  \n",
       "403.0   0.0        3.0  \n",
       "776.0   1.0        6.0  "
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.width', 100)  # 결과물을 잘 보여주기 위한 옵션, column 숫자를 표현한 것 같다. 10 단위로\n",
    "pd.set_option('precision', 3)        # 결과물을 잘 보여주기 위한 옵션, 숫자 소수점 표현하는 것 \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>birth</th>\n",
       "      <th>age1</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>edu</th>\n",
       "      <th>income</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "      <th>k4</th>\n",
       "      <th>k6</th>\n",
       "      <th>k7</th>\n",
       "      <th>k8</th>\n",
       "      <th>k10</th>\n",
       "      <th>k12</th>\n",
       "      <th>k13</th>\n",
       "      <th>k14</th>\n",
       "      <th>ideo_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1054.000</td>\n",
       "      <td>1054.000</td>\n",
       "      <td>1054.000</td>\n",
       "      <td>1054.000</td>\n",
       "      <td>1054.000</td>\n",
       "      <td>1054.000</td>\n",
       "      <td>1054.000</td>\n",
       "      <td>827.000</td>\n",
       "      <td>854.000</td>\n",
       "      <td>843.000</td>\n",
       "      <td>877.000</td>\n",
       "      <td>784.000</td>\n",
       "      <td>921.000</td>\n",
       "      <td>776.000</td>\n",
       "      <td>944.000</td>\n",
       "      <td>966.000</td>\n",
       "      <td>934.000</td>\n",
       "      <td>899.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.415</td>\n",
       "      <td>1971.749</td>\n",
       "      <td>44.251</td>\n",
       "      <td>4.014</td>\n",
       "      <td>5.653</td>\n",
       "      <td>3.124</td>\n",
       "      <td>1.787</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.793</td>\n",
       "      <td>5.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.493</td>\n",
       "      <td>13.580</td>\n",
       "      <td>13.580</td>\n",
       "      <td>1.387</td>\n",
       "      <td>4.406</td>\n",
       "      <td>1.045</td>\n",
       "      <td>2.098</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.405</td>\n",
       "      <td>2.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1937.000</td>\n",
       "      <td>19.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1960.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1970.000</td>\n",
       "      <td>46.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000</td>\n",
       "      <td>1983.000</td>\n",
       "      <td>56.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000</td>\n",
       "      <td>1997.000</td>\n",
       "      <td>79.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>15.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sex     birth      age1       age      area       edu    income       k2       k3  \\\n",
       "count  1054.000  1054.000  1054.000  1054.000  1054.000  1054.000  1054.000  827.000  854.000   \n",
       "mean      1.415  1971.749    44.251     4.014     5.653     3.124     1.787    0.657    0.485   \n",
       "std       0.493    13.580    13.580     1.387     4.406     1.045     2.098    0.475    0.500   \n",
       "min       1.000  1937.000    19.000     2.000     1.000     1.000     1.000    0.000    0.000   \n",
       "25%       1.000  1960.000    33.000     3.000     1.000     2.000     1.000    0.000    0.000   \n",
       "50%       1.000  1970.000    46.000     4.000     5.000     3.000     1.000    1.000    0.000   \n",
       "75%       2.000  1983.000    56.000     5.000     8.000     4.000     1.000    1.000    1.000   \n",
       "max       2.000  1997.000    79.000     6.000    17.000     5.000    15.000    1.000    1.000   \n",
       "\n",
       "            k4       k6       k7       k8      k10      k12      k13      k14  ideo_self  \n",
       "count  843.000  877.000  784.000  921.000  776.000  944.000  966.000  934.000    899.000  \n",
       "mean     0.537    0.719    0.256    0.385    0.454    0.469    0.269    0.793      5.158  \n",
       "std      0.499    0.450    0.437    0.487    0.498    0.499    0.444    0.405      2.270  \n",
       "min      0.000    0.000    0.000    0.000    0.000    0.000    0.000    0.000      0.000  \n",
       "25%      0.000    0.000    0.000    0.000    0.000    0.000    0.000    1.000      4.000  \n",
       "50%      1.000    1.000    0.000    0.000    0.000    0.000    0.000    1.000      5.000  \n",
       "75%      1.000    1.000    1.000    1.000    1.000    1.000    1.000    1.000      6.000  \n",
       "max      1.000    1.000    1.000    1.000    1.000    1.000    1.000    1.000     10.000  "
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex          float64\n",
       "birth        float64\n",
       "age1         float64\n",
       "age          float64\n",
       "area         float64\n",
       "edu          float64\n",
       "income       float64\n",
       "k2           float64\n",
       "k3           float64\n",
       "k4           float64\n",
       "k6           float64\n",
       "k7           float64\n",
       "k8           float64\n",
       "k10          float64\n",
       "k12          float64\n",
       "k13          float64\n",
       "k14          float64\n",
       "ideo_self    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Float64Index: 1054 entries, 1012.0 to 345.0\n",
      "Data columns (total 18 columns):\n",
      "sex          1054 non-null float64\n",
      "birth        1054 non-null float64\n",
      "age1         1054 non-null float64\n",
      "age          1054 non-null float64\n",
      "area         1054 non-null float64\n",
      "edu          1054 non-null float64\n",
      "income       1054 non-null float64\n",
      "k2           827 non-null float64\n",
      "k3           854 non-null float64\n",
      "k4           843 non-null float64\n",
      "k6           877 non-null float64\n",
      "k7           784 non-null float64\n",
      "k8           921 non-null float64\n",
      "k10          776 non-null float64\n",
      "k12          944 non-null float64\n",
      "k13          966 non-null float64\n",
      "k14          934 non-null float64\n",
      "ideo_self    899 non-null float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 156.5 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>birth</th>\n",
       "      <th>age1</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>edu</th>\n",
       "      <th>income</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "      <th>k4</th>\n",
       "      <th>k6</th>\n",
       "      <th>k7</th>\n",
       "      <th>k8</th>\n",
       "      <th>k10</th>\n",
       "      <th>k12</th>\n",
       "      <th>k13</th>\n",
       "      <th>k14</th>\n",
       "      <th>ideo_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345.0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1054 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sex  birth   age1    age   area    edu  income     k2     k3     k4     k6     k7  \\\n",
       "id                                                                                            \n",
       "1012.0  False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "556.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "642.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "403.0   False  False  False  False  False  False   False   True  False  False  False   True   \n",
       "776.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "767.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "594.0   False  False  False  False  False  False   False   True   True  False  False   True   \n",
       "869.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "719.0   False  False  False  False  False  False   False  False  False  False  False   True   \n",
       "783.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "846.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "276.0   False  False  False  False  False  False   False   True  False  False  False  False   \n",
       "754.0   False  False  False  False  False  False   False   True   True  False   True  False   \n",
       "710.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "394.0   False  False  False  False  False  False   False  False  False   True  False  False   \n",
       "1050.0  False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "450.0   False  False  False  False  False  False   False  False   True  False  False   True   \n",
       "536.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "590.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "651.0   False  False  False  False  False  False   False   True  False  False   True   True   \n",
       "687.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "913.0   False  False  False  False  False  False   False   True  False  False  False  False   \n",
       "176.0   False  False  False  False  False  False   False  False  False  False  False   True   \n",
       "165.0   False  False  False  False  False  False   False  False   True  False  False  False   \n",
       "945.0   False  False  False  False  False  False   False  False  False  False  False   True   \n",
       "649.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "86.0    False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "557.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "215.0   False  False  False  False  False  False   False   True  False  False  False  False   \n",
       "561.0   False  False  False  False  False  False   False  False  False   True  False  False   \n",
       "...       ...    ...    ...    ...    ...    ...     ...    ...    ...    ...    ...    ...   \n",
       "982.0   False  False  False  False  False  False   False   True  False  False  False   True   \n",
       "236.0   False  False  False  False  False  False   False  False  False  False  False   True   \n",
       "909.0   False  False  False  False  False  False   False  False   True  False  False  False   \n",
       "166.0   False  False  False  False  False  False   False   True   True  False   True   True   \n",
       "381.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "1016.0  False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "845.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "924.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "72.0    False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "620.0   False  False  False  False  False  False   False   True  False  False  False  False   \n",
       "204.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "228.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "21.0    False  False  False  False  False  False   False  False  False  False   True   True   \n",
       "951.0   False  False  False  False  False  False   False  False  False  False  False   True   \n",
       "612.0   False  False  False  False  False  False   False  False  False  False   True  False   \n",
       "621.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "636.0   False  False  False  False  False  False   False  False   True  False   True  False   \n",
       "326.0   False  False  False  False  False  False   False   True   True   True   True   True   \n",
       "705.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "219.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "222.0   False  False  False  False  False  False   False   True  False  False  False  False   \n",
       "714.0   False  False  False  False  False  False   False   True   True   True   True   True   \n",
       "751.0   False  False  False  False  False  False   False  False  False  False  False   True   \n",
       "654.0   False  False  False  False  False  False   False   True  False  False   True  False   \n",
       "170.0   False  False  False  False  False  False   False   True   True  False  False   True   \n",
       "927.0   False  False  False  False  False  False   False  False  False  False  False   True   \n",
       "646.0   False  False  False  False  False  False   False  False  False  False  False  False   \n",
       "331.0   False  False  False  False  False  False   False  False  False  False  False   True   \n",
       "562.0   False  False  False  False  False  False   False   True  False  False  False  False   \n",
       "345.0   False  False  False  False  False  False   False   True  False  False   True  False   \n",
       "\n",
       "           k8    k10    k12    k13    k14  ideo_self  \n",
       "id                                                    \n",
       "1012.0  False   True  False  False   True      False  \n",
       "556.0   False  False  False  False  False      False  \n",
       "642.0   False  False  False  False  False      False  \n",
       "403.0   False   True   True  False  False      False  \n",
       "776.0   False  False  False  False  False      False  \n",
       "767.0   False   True  False  False  False      False  \n",
       "594.0   False   True  False  False  False      False  \n",
       "869.0   False  False  False  False  False      False  \n",
       "719.0   False  False  False  False  False      False  \n",
       "783.0   False  False  False  False  False      False  \n",
       "846.0   False  False  False  False  False      False  \n",
       "276.0   False   True  False  False  False      False  \n",
       "754.0    True  False  False  False   True      False  \n",
       "710.0   False  False  False  False  False      False  \n",
       "394.0   False  False  False  False  False      False  \n",
       "1050.0  False  False  False  False  False      False  \n",
       "450.0   False  False  False  False  False      False  \n",
       "536.0   False  False  False  False  False      False  \n",
       "590.0   False  False  False  False  False      False  \n",
       "651.0    True   True  False  False  False      False  \n",
       "687.0   False  False  False  False  False      False  \n",
       "913.0   False  False  False  False  False      False  \n",
       "176.0   False  False  False  False  False      False  \n",
       "165.0   False  False  False  False  False      False  \n",
       "945.0   False  False  False  False  False      False  \n",
       "649.0   False  False  False  False  False      False  \n",
       "86.0    False  False  False  False  False      False  \n",
       "557.0   False   True  False  False  False      False  \n",
       "215.0   False   True  False  False  False      False  \n",
       "561.0   False   True  False  False  False      False  \n",
       "...       ...    ...    ...    ...    ...        ...  \n",
       "982.0   False  False  False  False  False       True  \n",
       "236.0   False   True   True  False  False       True  \n",
       "909.0   False  False  False  False  False       True  \n",
       "166.0   False  False   True  False   True       True  \n",
       "381.0   False  False  False  False  False       True  \n",
       "1016.0  False  False  False  False  False       True  \n",
       "845.0   False  False  False  False  False       True  \n",
       "924.0   False   True  False  False  False       True  \n",
       "72.0    False  False  False  False  False       True  \n",
       "620.0   False  False  False  False  False       True  \n",
       "204.0   False  False  False  False  False       True  \n",
       "228.0   False  False  False  False  False       True  \n",
       "21.0    False  False  False  False  False       True  \n",
       "951.0   False  False  False  False  False       True  \n",
       "612.0   False  False  False  False  False       True  \n",
       "621.0   False   True  False   True  False       True  \n",
       "636.0   False  False  False  False  False       True  \n",
       "326.0   False   True   True  False  False       True  \n",
       "705.0   False  False  False  False  False       True  \n",
       "219.0   False  False  False  False  False       True  \n",
       "222.0    True   True   True  False   True       True  \n",
       "714.0    True   True   True   True   True       True  \n",
       "751.0   False  False  False   True  False       True  \n",
       "654.0   False   True   True   True   True       True  \n",
       "170.0   False  False  False  False  False       True  \n",
       "927.0   False  False  False  False  False       True  \n",
       "646.0   False  False  False  False  False       True  \n",
       "331.0   False  False  False  False  False       True  \n",
       "562.0   False   True  False  False  False       True  \n",
       "345.0   False   True  False  False   True       True  \n",
       "\n",
       "[1054 rows x 18 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA 값들을 추측하기 위한 base 데이터를 잡아준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7      True\n",
       "8      True\n",
       "9      True\n",
       "10     True\n",
       "11     True\n",
       "12     True\n",
       "13     True\n",
       "14     True\n",
       "15     True\n",
       "16     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k 질문들만 잡고 NA 값 추측\n",
    "# x_impute = data.values[:,7:-1]\n",
    "\n",
    "# 전체 데이터 잡고 NA값 추측 \n",
    "x_impute = data.values[:,:-1]\n",
    "\n",
    "pd.DataFrame(x_impute).isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054, 17)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_impute.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA의 값으로 KNN, SoftImpute, SimpleFill, MICE 4개의 imputer를 쓸 예정이다.\n",
    "- **Manuel for KNN:** Nearest neighbor imputations which weights samples using the mean squared difference on features for which two rows both have observed data.\n",
    "- **Manuel for SimpleFill:** Replaces missing entries with the mean or median of each column.\n",
    "- **Thesis for SoftImpute:** [click] http://web.stanford.edu/~hastie/Papers/mazumder10a.pdf\n",
    "- **Thesis for MICE:** [click] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fancyimpute import (\n",
    "    KNN,\n",
    "    SoftImpute,\n",
    "    MICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MICE imputer로 missing Value를 처리했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MICE] Completing matrix with shape (1054, 17)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.002\n",
      "[MICE] Starting imputation round 2/110, elapsed time 0.008\n",
      "[MICE] Starting imputation round 3/110, elapsed time 0.014\n",
      "[MICE] Starting imputation round 4/110, elapsed time 0.021\n",
      "[MICE] Starting imputation round 5/110, elapsed time 0.028\n",
      "[MICE] Starting imputation round 6/110, elapsed time 0.035\n",
      "[MICE] Starting imputation round 7/110, elapsed time 0.040\n",
      "[MICE] Starting imputation round 8/110, elapsed time 0.045\n",
      "[MICE] Starting imputation round 9/110, elapsed time 0.050\n",
      "[MICE] Starting imputation round 10/110, elapsed time 0.055\n",
      "[MICE] Starting imputation round 11/110, elapsed time 0.060\n",
      "[MICE] Starting imputation round 12/110, elapsed time 0.066\n",
      "[MICE] Starting imputation round 13/110, elapsed time 0.073\n",
      "[MICE] Starting imputation round 14/110, elapsed time 0.080\n",
      "[MICE] Starting imputation round 15/110, elapsed time 0.085\n",
      "[MICE] Starting imputation round 16/110, elapsed time 0.090\n",
      "[MICE] Starting imputation round 17/110, elapsed time 0.095\n",
      "[MICE] Starting imputation round 18/110, elapsed time 0.102\n",
      "[MICE] Starting imputation round 19/110, elapsed time 0.107\n",
      "[MICE] Starting imputation round 20/110, elapsed time 0.112\n",
      "[MICE] Starting imputation round 21/110, elapsed time 0.116\n",
      "[MICE] Starting imputation round 22/110, elapsed time 0.120\n",
      "[MICE] Starting imputation round 23/110, elapsed time 0.124\n",
      "[MICE] Starting imputation round 24/110, elapsed time 0.129\n",
      "[MICE] Starting imputation round 25/110, elapsed time 0.133\n",
      "[MICE] Starting imputation round 26/110, elapsed time 0.137\n",
      "[MICE] Starting imputation round 27/110, elapsed time 0.142\n",
      "[MICE] Starting imputation round 28/110, elapsed time 0.146\n",
      "[MICE] Starting imputation round 29/110, elapsed time 0.150\n",
      "[MICE] Starting imputation round 30/110, elapsed time 0.154\n",
      "[MICE] Starting imputation round 31/110, elapsed time 0.159\n",
      "[MICE] Starting imputation round 32/110, elapsed time 0.164\n",
      "[MICE] Starting imputation round 33/110, elapsed time 0.169\n",
      "[MICE] Starting imputation round 34/110, elapsed time 0.174\n",
      "[MICE] Starting imputation round 35/110, elapsed time 0.178\n",
      "[MICE] Starting imputation round 36/110, elapsed time 0.183\n",
      "[MICE] Starting imputation round 37/110, elapsed time 0.187\n",
      "[MICE] Starting imputation round 38/110, elapsed time 0.192\n",
      "[MICE] Starting imputation round 39/110, elapsed time 0.196\n",
      "[MICE] Starting imputation round 40/110, elapsed time 0.200\n",
      "[MICE] Starting imputation round 41/110, elapsed time 0.207\n",
      "[MICE] Starting imputation round 42/110, elapsed time 0.211\n",
      "[MICE] Starting imputation round 43/110, elapsed time 0.215\n",
      "[MICE] Starting imputation round 44/110, elapsed time 0.219\n",
      "[MICE] Starting imputation round 45/110, elapsed time 0.224\n",
      "[MICE] Starting imputation round 46/110, elapsed time 0.228\n",
      "[MICE] Starting imputation round 47/110, elapsed time 0.232\n",
      "[MICE] Starting imputation round 48/110, elapsed time 0.237\n",
      "[MICE] Starting imputation round 49/110, elapsed time 0.242\n",
      "[MICE] Starting imputation round 50/110, elapsed time 0.247\n",
      "[MICE] Starting imputation round 51/110, elapsed time 0.251\n",
      "[MICE] Starting imputation round 52/110, elapsed time 0.255\n",
      "[MICE] Starting imputation round 53/110, elapsed time 0.259\n",
      "[MICE] Starting imputation round 54/110, elapsed time 0.264\n",
      "[MICE] Starting imputation round 55/110, elapsed time 0.269\n",
      "[MICE] Starting imputation round 56/110, elapsed time 0.273\n",
      "[MICE] Starting imputation round 57/110, elapsed time 0.276\n",
      "[MICE] Starting imputation round 58/110, elapsed time 0.280\n",
      "[MICE] Starting imputation round 59/110, elapsed time 0.285\n",
      "[MICE] Starting imputation round 60/110, elapsed time 0.289\n",
      "[MICE] Starting imputation round 61/110, elapsed time 0.293\n",
      "[MICE] Starting imputation round 62/110, elapsed time 0.298\n",
      "[MICE] Starting imputation round 63/110, elapsed time 0.302\n",
      "[MICE] Starting imputation round 64/110, elapsed time 0.306\n",
      "[MICE] Starting imputation round 65/110, elapsed time 0.310\n",
      "[MICE] Starting imputation round 66/110, elapsed time 0.315\n",
      "[MICE] Starting imputation round 67/110, elapsed time 0.319\n",
      "[MICE] Starting imputation round 68/110, elapsed time 0.323\n",
      "[MICE] Starting imputation round 69/110, elapsed time 0.327\n",
      "[MICE] Starting imputation round 70/110, elapsed time 0.332\n",
      "[MICE] Starting imputation round 71/110, elapsed time 0.337\n",
      "[MICE] Starting imputation round 72/110, elapsed time 0.341\n",
      "[MICE] Starting imputation round 73/110, elapsed time 0.346\n",
      "[MICE] Starting imputation round 74/110, elapsed time 0.350\n",
      "[MICE] Starting imputation round 75/110, elapsed time 0.354\n",
      "[MICE] Starting imputation round 76/110, elapsed time 0.359\n",
      "[MICE] Starting imputation round 77/110, elapsed time 0.363\n",
      "[MICE] Starting imputation round 78/110, elapsed time 0.367\n",
      "[MICE] Starting imputation round 79/110, elapsed time 0.372\n",
      "[MICE] Starting imputation round 80/110, elapsed time 0.378\n",
      "[MICE] Starting imputation round 81/110, elapsed time 0.384\n",
      "[MICE] Starting imputation round 82/110, elapsed time 0.390\n",
      "[MICE] Starting imputation round 83/110, elapsed time 0.395\n",
      "[MICE] Starting imputation round 84/110, elapsed time 0.400\n",
      "[MICE] Starting imputation round 85/110, elapsed time 0.408\n",
      "[MICE] Starting imputation round 86/110, elapsed time 0.414\n",
      "[MICE] Starting imputation round 87/110, elapsed time 0.421\n",
      "[MICE] Starting imputation round 88/110, elapsed time 0.425\n",
      "[MICE] Starting imputation round 89/110, elapsed time 0.430\n",
      "[MICE] Starting imputation round 90/110, elapsed time 0.434\n",
      "[MICE] Starting imputation round 91/110, elapsed time 0.441\n",
      "[MICE] Starting imputation round 92/110, elapsed time 0.447\n",
      "[MICE] Starting imputation round 93/110, elapsed time 0.453\n",
      "[MICE] Starting imputation round 94/110, elapsed time 0.458\n",
      "[MICE] Starting imputation round 95/110, elapsed time 0.465\n",
      "[MICE] Starting imputation round 96/110, elapsed time 0.470\n",
      "[MICE] Starting imputation round 97/110, elapsed time 0.474\n",
      "[MICE] Starting imputation round 98/110, elapsed time 0.479\n",
      "[MICE] Starting imputation round 99/110, elapsed time 0.484\n",
      "[MICE] Starting imputation round 100/110, elapsed time 0.488\n",
      "[MICE] Starting imputation round 101/110, elapsed time 0.492\n",
      "[MICE] Starting imputation round 102/110, elapsed time 0.497\n",
      "[MICE] Starting imputation round 103/110, elapsed time 0.501\n",
      "[MICE] Starting imputation round 104/110, elapsed time 0.506\n",
      "[MICE] Starting imputation round 105/110, elapsed time 0.511\n",
      "[MICE] Starting imputation round 106/110, elapsed time 0.516\n",
      "[MICE] Starting imputation round 107/110, elapsed time 0.520\n",
      "[MICE] Starting imputation round 108/110, elapsed time 0.524\n",
      "[MICE] Starting imputation round 109/110, elapsed time 0.530\n",
      "[MICE] Starting imputation round 110/110, elapsed time 0.534\n"
     ]
    }
   ],
   "source": [
    "x_impute_filled = MICE().complete(x_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054, 17)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_impute_filled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7     False\n",
       "8     False\n",
       "9     False\n",
       "10    False\n",
       "11    False\n",
       "12    False\n",
       "13    False\n",
       "14    False\n",
       "15    False\n",
       "16    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_impute_filled).isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set에서 Missing Value를 바꿀 colums들을 분류해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054, 17)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = data.values\n",
    "# k 질문들만 잡고 NA 값 찾는 방법\n",
    "# array[:, 7:-1].shape\n",
    "\n",
    "# 전체 데이터를 잡고 NA값 찾는 방법\n",
    "array[:,:-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute 된 데이터를 Data Set에 적용시켜준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array[:, :-1] = x_impute_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideo_self column을 제외한 본 Data Set에 더 이상 Missing Value 가 없다는 것을 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7     False\n",
       "8     False\n",
       "9     False\n",
       "10    False\n",
       "11    False\n",
       "12    False\n",
       "13    False\n",
       "14    False\n",
       "15    False\n",
       "16    False\n",
       "17     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(array).isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>birth</th>\n",
       "      <th>age1</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>edu</th>\n",
       "      <th>income</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "      <th>k4</th>\n",
       "      <th>k6</th>\n",
       "      <th>k7</th>\n",
       "      <th>k8</th>\n",
       "      <th>k10</th>\n",
       "      <th>k12</th>\n",
       "      <th>k13</th>\n",
       "      <th>k14</th>\n",
       "      <th>ideo_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.661</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sex   birth  age1  age  area  edu  income     k2     k3   k4   k6     k7   k8    k10  \\\n",
       "id                                                                                             \n",
       "1012.0  1.0  1994.0  22.0  2.0   1.0  2.0     1.0  1.000  0.000  0.0  1.0  0.000  0.0  0.421   \n",
       "556.0   1.0  1957.0  59.0  5.0   8.0  4.0     1.0  1.000  1.000  1.0  1.0  0.000  1.0  0.000   \n",
       "642.0   1.0  1990.0  26.0  2.0   4.0  2.0     1.0  0.000  1.000  1.0  1.0  0.000  0.0  0.000   \n",
       "403.0   2.0  1966.0  50.0  5.0   2.0  4.0     7.0  0.661  1.000  0.0  0.0  0.264  0.0  0.447   \n",
       "776.0   1.0  1978.0  38.0  3.0   1.0  4.0     1.0  1.000  1.000  1.0  0.0  0.000  0.0  1.000   \n",
       "767.0   1.0  1966.0  50.0  5.0   2.0  4.0     1.0  0.000  1.000  0.0  1.0  0.000  0.0  0.530   \n",
       "594.0   1.0  1982.0  34.0  3.0   2.0  2.0     1.0  0.618  0.509  1.0  1.0  0.229  0.0  0.479   \n",
       "869.0   1.0  1965.0  51.0  5.0   6.0  4.0     2.0  1.000  0.000  1.0  1.0  1.000  0.0  1.000   \n",
       "719.0   1.0  1966.0  50.0  5.0   9.0  4.0     1.0  0.000  1.000  1.0  1.0  0.216  0.0  1.000   \n",
       "783.0   1.0  1956.0  60.0  6.0   1.0  2.0     2.0  1.000  0.000  1.0  1.0  0.000  0.0  0.000   \n",
       "\n",
       "          k12  k13    k14  ideo_self  \n",
       "id                                    \n",
       "1012.0  1.000  0.0  0.765        5.0  \n",
       "556.0   0.000  1.0  1.000        7.0  \n",
       "642.0   0.000  0.0  1.000       10.0  \n",
       "403.0   0.478  0.0  0.000        3.0  \n",
       "776.0   1.000  0.0  1.000        6.0  \n",
       "767.0   0.000  0.0  1.000        5.0  \n",
       "594.0   0.000  0.0  1.000        5.0  \n",
       "869.0   1.000  1.0  1.000        5.0  \n",
       "719.0   0.000  0.0  0.000        4.0  \n",
       "783.0   0.000  1.0  1.000        8.0  "
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute 된 값이 확률로 나왔기 때문에 0.5 이상의 값은 1로 대체하고 0.5 이하의 값은 0으로 대체해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 1054):\n",
    "    for j in range(0, 10):\n",
    "        if np.any(array[:, 7:-1][i][j] >= 0.5):\n",
    "            array[:, 7:-1][i][j] = 1\n",
    "        else:\n",
    "            array[:, 7:-1][i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex          False\n",
       "birth        False\n",
       "age1         False\n",
       "age          False\n",
       "area         False\n",
       "edu          False\n",
       "income       False\n",
       "k2           False\n",
       "k3           False\n",
       "k4           False\n",
       "k6           False\n",
       "k7           False\n",
       "k8           False\n",
       "k10          False\n",
       "k12          False\n",
       "k13          False\n",
       "k14          False\n",
       "ideo_self     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확률로 된 Impute 값이 아닌 0 과 1 로 구성된 값을 볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>birth</th>\n",
       "      <th>age1</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>edu</th>\n",
       "      <th>income</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "      <th>k4</th>\n",
       "      <th>k6</th>\n",
       "      <th>k7</th>\n",
       "      <th>k8</th>\n",
       "      <th>k10</th>\n",
       "      <th>k12</th>\n",
       "      <th>k13</th>\n",
       "      <th>k14</th>\n",
       "      <th>ideo_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sex   birth  age1  age  area  edu  income   k2   k3   k4   k6   k7   k8  k10  k12  k13  \\\n",
       "id                                                                                               \n",
       "1012.0  1.0  1994.0  22.0  2.0   1.0  2.0     1.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0   \n",
       "556.0   1.0  1957.0  59.0  5.0   8.0  4.0     1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  0.0  1.0   \n",
       "642.0   1.0  1990.0  26.0  2.0   4.0  2.0     1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "403.0   2.0  1966.0  50.0  5.0   2.0  4.0     7.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "776.0   1.0  1978.0  38.0  3.0   1.0  4.0     1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0   \n",
       "\n",
       "        k14  ideo_self  \n",
       "id                      \n",
       "1012.0  1.0        5.0  \n",
       "556.0   1.0        7.0  \n",
       "642.0   1.0       10.0  \n",
       "403.0   0.0        3.0  \n",
       "776.0   1.0        6.0  "
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideo_self 가 class 별로 몇 개씩 있는지 확인해본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ideo_self\n",
       "0.0      31\n",
       "1.0      22\n",
       "2.0      41\n",
       "3.0     101\n",
       "4.0      98\n",
       "5.0     282\n",
       "6.0     101\n",
       "7.0      87\n",
       "8.0      63\n",
       "9.0      21\n",
       "10.0     52\n",
       "dtype: int64"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideo_self_counts = data.groupby('ideo_self').size()\n",
    "ideo_self_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>birth</th>\n",
       "      <th>age1</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>edu</th>\n",
       "      <th>income</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "      <th>k4</th>\n",
       "      <th>k6</th>\n",
       "      <th>k7</th>\n",
       "      <th>k8</th>\n",
       "      <th>k10</th>\n",
       "      <th>k12</th>\n",
       "      <th>k13</th>\n",
       "      <th>k14</th>\n",
       "      <th>ideo_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sex   birth  age1  age  area  edu  income   k2   k3   k4   k6   k7   k8  k10  k12  k13  \\\n",
       "id                                                                                               \n",
       "1012.0  1.0  1994.0  22.0  2.0   1.0  2.0     1.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0   \n",
       "556.0   1.0  1957.0  59.0  5.0   8.0  4.0     1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  0.0  1.0   \n",
       "642.0   1.0  1990.0  26.0  2.0   4.0  2.0     1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "403.0   2.0  1966.0  50.0  5.0   2.0  4.0     7.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "776.0   1.0  1978.0  38.0  3.0   1.0  4.0     1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0   \n",
       "\n",
       "        k14  ideo_self  \n",
       "id                      \n",
       "1012.0  1.0        5.0  \n",
       "556.0   1.0        7.0  \n",
       "642.0   1.0       10.0  \n",
       "403.0   0.0        3.0  \n",
       "776.0   1.0        6.0  "
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Encoding categorical variables\n",
    "- Categorical variable: 값이 nominal인 변수\n",
    "- 한 variable에 category가 총 C개가 존재하는 경우, 이를 C개의 binary dummy variables로 변환하여 수치형 데이터로 변환할 수 있다.\n",
    "- 모든 variable의 숫자 값이 category 값이기 때문에 **One hot encoding**을 통해 binary dummy variables로 변환해준다. \n",
    "- Using `pandas.get_dummies`\n",
    "    - [click]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "#encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex_1.0</th>\n",
       "      <th>sex_2.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sex_1.0  sex_2.0\n",
       "id                     \n",
       "471.0        0        1\n",
       "119.0        0        1\n",
       "103.0        0        1\n",
       "301.0        0        1\n",
       "475.0        1        0\n",
       "970.0        1        0\n",
       "100.0        0        1\n",
       "568.0        1        0\n",
       "560.0        1        0\n",
       "442.0        1        0"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sex_dummies = pd.get_dummies(data.sex, prefix = 'sex')\n",
    "Sex_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_2.0</th>\n",
       "      <th>age_3.0</th>\n",
       "      <th>age_4.0</th>\n",
       "      <th>age_5.0</th>\n",
       "      <th>age_6.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>954.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age_2.0  age_3.0  age_4.0  age_5.0  age_6.0\n",
       "id                                                \n",
       "954.0        0        0        1        0        0\n",
       "175.0        0        0        0        1        0\n",
       "410.0        1        0        0        0        0\n",
       "914.0        0        0        1        0        0\n",
       "727.0        0        0        0        0        1\n",
       "886.0        1        0        0        0        0\n",
       "904.0        0        0        1        0        0\n",
       "566.0        0        0        1        0        0\n",
       "777.0        0        0        0        1        0\n",
       "745.0        0        0        0        0        1"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Age_dummies = pd.get_dummies(data.age, prefix = 'age')\n",
    "Age_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_1.0</th>\n",
       "      <th>area_2.0</th>\n",
       "      <th>area_3.0</th>\n",
       "      <th>area_4.0</th>\n",
       "      <th>area_5.0</th>\n",
       "      <th>area_6.0</th>\n",
       "      <th>area_7.0</th>\n",
       "      <th>area_8.0</th>\n",
       "      <th>area_9.0</th>\n",
       "      <th>area_10.0</th>\n",
       "      <th>area_11.0</th>\n",
       "      <th>area_12.0</th>\n",
       "      <th>area_13.0</th>\n",
       "      <th>area_14.0</th>\n",
       "      <th>area_15.0</th>\n",
       "      <th>area_16.0</th>\n",
       "      <th>area_17.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       area_1.0  area_2.0  area_3.0  area_4.0  area_5.0  area_6.0  area_7.0  area_8.0  area_9.0  \\\n",
       "id                                                                                                \n",
       "307.0         0         1         0         0         0         0         0         0         0   \n",
       "735.0         1         0         0         0         0         0         0         0         0   \n",
       "480.0         0         0         1         0         0         0         0         0         0   \n",
       "108.0         0         0         0         0         0         1         0         0         0   \n",
       "27.0          0         0         0         0         0         0         0         1         0   \n",
       "383.0         1         0         0         0         0         0         0         0         0   \n",
       "158.0         0         0         0         0         0         0         0         1         0   \n",
       "174.0         0         0         0         0         0         0         0         1         0   \n",
       "270.0         0         0         0         0         0         0         0         1         0   \n",
       "970.0         0         0         0         0         0         0         0         1         0   \n",
       "\n",
       "       area_10.0  area_11.0  area_12.0  area_13.0  area_14.0  area_15.0  area_16.0  area_17.0  \n",
       "id                                                                                             \n",
       "307.0          0          0          0          0          0          0          0          0  \n",
       "735.0          0          0          0          0          0          0          0          0  \n",
       "480.0          0          0          0          0          0          0          0          0  \n",
       "108.0          0          0          0          0          0          0          0          0  \n",
       "27.0           0          0          0          0          0          0          0          0  \n",
       "383.0          0          0          0          0          0          0          0          0  \n",
       "158.0          0          0          0          0          0          0          0          0  \n",
       "174.0          0          0          0          0          0          0          0          0  \n",
       "270.0          0          0          0          0          0          0          0          0  \n",
       "970.0          0          0          0          0          0          0          0          0  "
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Area_dummies = pd.get_dummies(data.area, prefix = 'area')\n",
    "Area_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>education_1.0</th>\n",
       "      <th>education_2.0</th>\n",
       "      <th>education_3.0</th>\n",
       "      <th>education_4.0</th>\n",
       "      <th>education_5.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       education_1.0  education_2.0  education_3.0  education_4.0  education_5.0\n",
       "id                                                                              \n",
       "147.0              0              0              0              1              0\n",
       "891.0              1              0              0              0              0\n",
       "944.0              0              1              0              0              0\n",
       "27.0               0              0              0              1              0\n",
       "424.0              0              0              0              1              0\n",
       "11.0               0              0              0              1              0\n",
       "462.0              0              0              0              1              0\n",
       "812.0              0              0              1              0              0\n",
       "517.0              0              0              0              1              0\n",
       "361.0              0              1              0              0              0"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Edu_dummies = pd.get_dummies(data.edu, prefix = 'education')\n",
    "Edu_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income_1.0</th>\n",
       "      <th>income_2.0</th>\n",
       "      <th>income_3.0</th>\n",
       "      <th>income_4.0</th>\n",
       "      <th>income_5.0</th>\n",
       "      <th>income_6.0</th>\n",
       "      <th>income_7.0</th>\n",
       "      <th>income_8.0</th>\n",
       "      <th>income_9.0</th>\n",
       "      <th>income_10.0</th>\n",
       "      <th>income_11.0</th>\n",
       "      <th>income_12.0</th>\n",
       "      <th>income_14.0</th>\n",
       "      <th>income_15.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       income_1.0  income_2.0  income_3.0  income_4.0  income_5.0  income_6.0  income_7.0  \\\n",
       "id                                                                                          \n",
       "261.0           0           0           1           0           0           0           0   \n",
       "954.0           1           0           0           0           0           0           0   \n",
       "451.0           0           0           0           1           0           0           0   \n",
       "97.0            1           0           0           0           0           0           0   \n",
       "541.0           1           0           0           0           0           0           0   \n",
       "573.0           0           0           1           0           0           0           0   \n",
       "154.0           0           0           0           1           0           0           0   \n",
       "65.0            0           0           0           1           0           0           0   \n",
       "821.0           1           0           0           0           0           0           0   \n",
       "199.0           0           0           1           0           0           0           0   \n",
       "\n",
       "       income_8.0  income_9.0  income_10.0  income_11.0  income_12.0  income_14.0  income_15.0  \n",
       "id                                                                                              \n",
       "261.0           0           0            0            0            0            0            0  \n",
       "954.0           0           0            0            0            0            0            0  \n",
       "451.0           0           0            0            0            0            0            0  \n",
       "97.0            0           0            0            0            0            0            0  \n",
       "541.0           0           0            0            0            0            0            0  \n",
       "573.0           0           0            0            0            0            0            0  \n",
       "154.0           0           0            0            0            0            0            0  \n",
       "65.0            0           0            0            0            0            0            0  \n",
       "821.0           0           0            0            0            0            0            0  \n",
       "199.0           0           0            0            0            0            0            0  "
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Income_dummies = pd.get_dummies(data.income, prefix = 'income')\n",
    "Income_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k2_0.0</th>\n",
       "      <th>k2_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>606.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        k2_0.0  k2_1.0\n",
       "id                    \n",
       "606.0        0       1\n",
       "681.0        0       1\n",
       "736.0        0       1\n",
       "482.0        1       0\n",
       "995.0        0       1\n",
       "281.0        0       1\n",
       "1046.0       0       1\n",
       "919.0        0       1\n",
       "1031.0       0       1\n",
       "968.0        0       1"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k2_dummies = pd.get_dummies(data.k2, prefix = 'k2')\n",
    "k2_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k3_0.0</th>\n",
       "      <th>k3_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       k3_0.0  k3_1.0\n",
       "id                   \n",
       "500.0       0       1\n",
       "182.0       0       1\n",
       "867.0       1       0\n",
       "170.0       0       1\n",
       "666.0       0       1\n",
       "155.0       1       0\n",
       "865.0       1       0\n",
       "807.0       1       0\n",
       "88.0        0       1\n",
       "432.0       1       0"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k3_dummies = pd.get_dummies(data.k3, prefix = 'k3')\n",
    "k3_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k4_0.0</th>\n",
       "      <th>k4_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        k4_0.0  k4_1.0\n",
       "id                    \n",
       "224.0        0       1\n",
       "647.0        0       1\n",
       "493.0        0       1\n",
       "1043.0       0       1\n",
       "832.0        1       0\n",
       "653.0        0       1\n",
       "765.0        0       1\n",
       "103.0        0       1\n",
       "782.0        0       1\n",
       "504.0        0       1"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k4_dummies = pd.get_dummies(data.k4, prefix = 'k4')\n",
    "k4_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k6_0.0</th>\n",
       "      <th>k6_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       k6_0.0  k6_1.0\n",
       "id                   \n",
       "24.0        0       1\n",
       "472.0       0       1\n",
       "944.0       0       1\n",
       "430.0       1       0\n",
       "561.0       0       1\n",
       "501.0       0       1\n",
       "271.0       0       1\n",
       "866.0       0       1\n",
       "822.0       1       0\n",
       "487.0       0       1"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k6_dummies = pd.get_dummies(data.k6, prefix = 'k6')\n",
    "k6_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k7_0.0</th>\n",
       "      <th>k7_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>808.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       k7_0.0  k7_1.0\n",
       "id                   \n",
       "808.0       1       0\n",
       "310.0       1       0\n",
       "926.0       1       0\n",
       "283.0       1       0\n",
       "717.0       1       0\n",
       "912.0       1       0\n",
       "757.0       0       1\n",
       "400.0       1       0\n",
       "833.0       0       1\n",
       "634.0       1       0"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k7_dummies = pd.get_dummies(data.k7, prefix = 'k7')\n",
    "k7_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k8_0.0</th>\n",
       "      <th>k8_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>627.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       k8_0.0  k8_1.0\n",
       "id                   \n",
       "627.0       1       0\n",
       "670.0       0       1\n",
       "423.0       1       0\n",
       "594.0       1       0\n",
       "145.0       1       0\n",
       "451.0       1       0\n",
       "521.0       0       1\n",
       "915.0       0       1\n",
       "983.0       0       1\n",
       "325.0       1       0"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k8_dummies = pd.get_dummies(data.k8, prefix = 'k8')\n",
    "k8_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k10_0.0</th>\n",
       "      <th>k10_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       k10_0.0  k10_1.0\n",
       "id                     \n",
       "463.0        1        0\n",
       "360.0        0        1\n",
       "382.0        1        0\n",
       "75.0         0        1\n",
       "606.0        1        0\n",
       "204.0        1        0\n",
       "482.0        0        1\n",
       "336.0        1        0\n",
       "370.0        1        0\n",
       "549.0        1        0"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k10_dummies = pd.get_dummies(data.k10, prefix = 'k10')\n",
    "k10_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k12_0.0</th>\n",
       "      <th>k12_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       k12_0.0  k12_1.0\n",
       "id                     \n",
       "463.0        0        1\n",
       "337.0        1        0\n",
       "74.0         1        0\n",
       "552.0        1        0\n",
       "809.0        1        0\n",
       "170.0        0        1\n",
       "823.0        1        0\n",
       "208.0        1        0\n",
       "331.0        0        1\n",
       "546.0        1        0"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k12_dummies = pd.get_dummies(data.k12, prefix = 'k12')\n",
    "k12_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k13_0.0</th>\n",
       "      <th>k13_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>484.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        k13_0.0  k13_1.0\n",
       "id                      \n",
       "484.0         1        0\n",
       "681.0         0        1\n",
       "187.0         0        1\n",
       "109.0         1        0\n",
       "1027.0        1        0\n",
       "68.0          1        0\n",
       "244.0         1        0\n",
       "579.0         1        0\n",
       "905.0         1        0\n",
       "941.0         1        0"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k13_dummies = pd.get_dummies(data.k13, prefix = 'k13')\n",
    "k13_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k14_0.0</th>\n",
       "      <th>k14_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>713.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       k14_0.0  k14_1.0\n",
       "id                     \n",
       "713.0        0        1\n",
       "657.0        0        1\n",
       "709.0        0        1\n",
       "234.0        1        0\n",
       "571.0        0        1\n",
       "957.0        0        1\n",
       "187.0        0        1\n",
       "710.0        0        1\n",
       "915.0        0        1\n",
       "427.0        0        1"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k14_dummies = pd.get_dummies(data.k14, prefix = 'k14')\n",
    "k14_dummies.sample(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variable로 변경된 variables를 Data set에 추가해주고 그 파일 이름을 new_data라고 칭한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>birth</th>\n",
       "      <th>age1</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>edu</th>\n",
       "      <th>income</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "      <th>k4</th>\n",
       "      <th>...</th>\n",
       "      <th>k8_0.0</th>\n",
       "      <th>k8_1.0</th>\n",
       "      <th>k10_0.0</th>\n",
       "      <th>k10_1.0</th>\n",
       "      <th>k12_0.0</th>\n",
       "      <th>k12_1.0</th>\n",
       "      <th>k13_0.0</th>\n",
       "      <th>k13_1.0</th>\n",
       "      <th>k14_0.0</th>\n",
       "      <th>k14_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sex   birth  age1  age  area  edu  income   k2   k3   k4   ...     k8_0.0  k8_1.0  \\\n",
       "id                                                                 ...                      \n",
       "1012.0  1.0  1994.0  22.0  2.0   1.0  2.0     1.0  1.0  0.0  0.0   ...          1       0   \n",
       "556.0   1.0  1957.0  59.0  5.0   8.0  4.0     1.0  1.0  1.0  1.0   ...          0       1   \n",
       "642.0   1.0  1990.0  26.0  2.0   4.0  2.0     1.0  0.0  1.0  1.0   ...          1       0   \n",
       "403.0   2.0  1966.0  50.0  5.0   2.0  4.0     7.0  1.0  1.0  0.0   ...          1       0   \n",
       "776.0   1.0  1978.0  38.0  3.0   1.0  4.0     1.0  1.0  1.0  1.0   ...          1       0   \n",
       "767.0   1.0  1966.0  50.0  5.0   2.0  4.0     1.0  0.0  1.0  0.0   ...          1       0   \n",
       "594.0   1.0  1982.0  34.0  3.0   2.0  2.0     1.0  1.0  1.0  1.0   ...          1       0   \n",
       "869.0   1.0  1965.0  51.0  5.0   6.0  4.0     2.0  1.0  0.0  1.0   ...          1       0   \n",
       "719.0   1.0  1966.0  50.0  5.0   9.0  4.0     1.0  0.0  1.0  1.0   ...          1       0   \n",
       "783.0   1.0  1956.0  60.0  6.0   1.0  2.0     2.0  1.0  0.0  1.0   ...          1       0   \n",
       "\n",
       "        k10_0.0  k10_1.0  k12_0.0  k12_1.0  k13_0.0  k13_1.0  k14_0.0  k14_1.0  \n",
       "id                                                                              \n",
       "1012.0        1        0        0        1        1        0        0        1  \n",
       "556.0         1        0        1        0        0        1        0        1  \n",
       "642.0         1        0        1        0        1        0        0        1  \n",
       "403.0         1        0        1        0        1        0        1        0  \n",
       "776.0         0        1        0        1        1        0        0        1  \n",
       "767.0         0        1        1        0        1        0        0        1  \n",
       "594.0         1        0        1        0        1        0        0        1  \n",
       "869.0         0        1        0        1        0        1        0        1  \n",
       "719.0         0        1        1        0        1        0        1        0  \n",
       "783.0         1        0        1        0        0        1        0        1  \n",
       "\n",
       "[10 rows x 81 columns]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.concat([data,  Sex_dummies, Age_dummies, Area_dummies, Edu_dummies, Income_dummies, k2_dummies, k3_dummies, k4_dummies, k6_dummies, k7_dummies, k8_dummies, k10_dummies, k12_dummies, k13_dummies, k14_dummies], axis = 1)\n",
    "new_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variable로 이미 변경된 variables들은 더 이상 필요하지 않기 때문에 new_data set에서 제거해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ideo_self</th>\n",
       "      <th>sex_1.0</th>\n",
       "      <th>sex_2.0</th>\n",
       "      <th>age_2.0</th>\n",
       "      <th>age_3.0</th>\n",
       "      <th>age_4.0</th>\n",
       "      <th>age_5.0</th>\n",
       "      <th>age_6.0</th>\n",
       "      <th>area_1.0</th>\n",
       "      <th>area_2.0</th>\n",
       "      <th>...</th>\n",
       "      <th>k8_0.0</th>\n",
       "      <th>k8_1.0</th>\n",
       "      <th>k10_0.0</th>\n",
       "      <th>k10_1.0</th>\n",
       "      <th>k12_0.0</th>\n",
       "      <th>k12_1.0</th>\n",
       "      <th>k13_0.0</th>\n",
       "      <th>k13_1.0</th>\n",
       "      <th>k14_0.0</th>\n",
       "      <th>k14_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783.0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ideo_self  sex_1.0  sex_2.0  age_2.0  age_3.0  age_4.0  age_5.0  age_6.0  area_1.0  \\\n",
       "id                                                                                           \n",
       "1012.0        5.0        1        0        1        0        0        0        0         1   \n",
       "556.0         7.0        1        0        0        0        0        1        0         0   \n",
       "642.0        10.0        1        0        1        0        0        0        0         0   \n",
       "403.0         3.0        0        1        0        0        0        1        0         0   \n",
       "776.0         6.0        1        0        0        1        0        0        0         1   \n",
       "767.0         5.0        1        0        0        0        0        1        0         0   \n",
       "594.0         5.0        1        0        0        1        0        0        0         0   \n",
       "869.0         5.0        1        0        0        0        0        1        0         0   \n",
       "719.0         4.0        1        0        0        0        0        1        0         0   \n",
       "783.0         8.0        1        0        0        0        0        0        1         1   \n",
       "\n",
       "        area_2.0   ...     k8_0.0  k8_1.0  k10_0.0  k10_1.0  k12_0.0  k12_1.0  k13_0.0  k13_1.0  \\\n",
       "id                 ...                                                                            \n",
       "1012.0         0   ...          1       0        1        0        0        1        1        0   \n",
       "556.0          0   ...          0       1        1        0        1        0        0        1   \n",
       "642.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "403.0          1   ...          1       0        1        0        1        0        1        0   \n",
       "776.0          0   ...          1       0        0        1        0        1        1        0   \n",
       "767.0          1   ...          1       0        0        1        1        0        1        0   \n",
       "594.0          1   ...          1       0        1        0        1        0        1        0   \n",
       "869.0          0   ...          1       0        0        1        0        1        0        1   \n",
       "719.0          0   ...          1       0        0        1        1        0        1        0   \n",
       "783.0          0   ...          1       0        1        0        1        0        0        1   \n",
       "\n",
       "        k14_0.0  k14_1.0  \n",
       "id                        \n",
       "1012.0        0        1  \n",
       "556.0         0        1  \n",
       "642.0         0        1  \n",
       "403.0         1        0  \n",
       "776.0         0        1  \n",
       "767.0         0        1  \n",
       "594.0         0        1  \n",
       "869.0         0        1  \n",
       "719.0         1        0  \n",
       "783.0         0        1  \n",
       "\n",
       "[10 rows x 64 columns]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = new_data.drop(['age', 'birth', 'area', 'sex', 'age1','edu', 'income','k2','k3','k4','k6','k7','k8','k10','k12','k13','k14'], axis = 1)\n",
    "new_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New_data에서 Ideo_self의 값이 NA인 row가 ID Number 24 부터이기 때문에 NA 값인 rows들을 따로 분류해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ideo_self</th>\n",
       "      <th>sex_1.0</th>\n",
       "      <th>sex_2.0</th>\n",
       "      <th>age_2.0</th>\n",
       "      <th>age_3.0</th>\n",
       "      <th>age_4.0</th>\n",
       "      <th>age_5.0</th>\n",
       "      <th>age_6.0</th>\n",
       "      <th>area_1.0</th>\n",
       "      <th>area_2.0</th>\n",
       "      <th>...</th>\n",
       "      <th>k8_0.0</th>\n",
       "      <th>k8_1.0</th>\n",
       "      <th>k10_0.0</th>\n",
       "      <th>k10_1.0</th>\n",
       "      <th>k12_0.0</th>\n",
       "      <th>k12_1.0</th>\n",
       "      <th>k13_0.0</th>\n",
       "      <th>k13_1.0</th>\n",
       "      <th>k14_0.0</th>\n",
       "      <th>k14_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ideo_self  sex_1.0  sex_2.0  age_2.0  age_3.0  age_4.0  age_5.0  age_6.0  area_1.0  \\\n",
       "id                                                                                           \n",
       "24.0          NaN        1        0        0        0        0        0        1         1   \n",
       "107.0         NaN        1        0        0        0        0        0        1         1   \n",
       "111.0         NaN        1        0        0        0        0        0        1         1   \n",
       "118.0         NaN        1        0        0        0        0        0        1         1   \n",
       "182.0         NaN        1        0        0        0        0        0        1         1   \n",
       "281.0         NaN        1        0        0        0        0        0        1         1   \n",
       "289.0         NaN        1        0        0        0        0        0        1         1   \n",
       "339.0         NaN        1        0        0        0        0        0        1         1   \n",
       "357.0         NaN        1        0        0        0        0        0        1         1   \n",
       "415.0         NaN        1        0        0        0        0        0        1         1   \n",
       "447.0         NaN        1        0        0        0        0        0        1         1   \n",
       "629.0         NaN        1        0        0        0        0        0        1         1   \n",
       "727.0         NaN        1        0        0        0        0        0        1         1   \n",
       "755.0         NaN        1        0        0        0        0        0        1         1   \n",
       "120.0         NaN        0        1        0        0        0        0        1         1   \n",
       "129.0         NaN        0        1        0        0        0        0        1         1   \n",
       "168.0         NaN        0        1        0        0        0        0        1         1   \n",
       "202.0         NaN        0        1        0        0        0        0        1         1   \n",
       "499.0         NaN        1        0        0        0        0        1        0         1   \n",
       "613.0         NaN        1        0        0        0        0        1        0         1   \n",
       "801.0         NaN        1        0        0        0        0        1        0         1   \n",
       "809.0         NaN        1        0        0        0        0        1        0         1   \n",
       "986.0         NaN        1        0        0        0        0        1        0         1   \n",
       "87.0          NaN        0        1        0        0        0        1        0         1   \n",
       "103.0         NaN        0        1        0        0        0        1        0         1   \n",
       "119.0         NaN        0        1        0        0        0        1        0         1   \n",
       "141.0         NaN        0        1        0        0        0        1        0         1   \n",
       "768.0         NaN        1        0        0        0        1        0        0         1   \n",
       "805.0         NaN        1        0        0        0        1        0        0         1   \n",
       "857.0         NaN        1        0        0        0        1        0        0         1   \n",
       "...           ...      ...      ...      ...      ...      ...      ...      ...       ...   \n",
       "982.0         NaN        0        1        0        0        1        0        0         0   \n",
       "236.0         NaN        1        0        0        1        0        0        0         0   \n",
       "909.0         NaN        1        0        0        1        0        0        0         0   \n",
       "166.0         NaN        0        1        0        1        0        0        0         0   \n",
       "381.0         NaN        1        0        1        0        0        0        0         0   \n",
       "1016.0        NaN        1        0        1        0        0        0        0         0   \n",
       "845.0         NaN        0        1        1        0        0        0        0         0   \n",
       "924.0         NaN        0        1        1        0        0        0        0         0   \n",
       "72.0          NaN        1        0        0        0        0        0        1         0   \n",
       "620.0         NaN        1        0        0        0        0        0        1         0   \n",
       "204.0         NaN        1        0        0        0        0        1        0         0   \n",
       "228.0         NaN        1        0        0        0        0        1        0         0   \n",
       "21.0          NaN        0        1        0        0        0        1        0         0   \n",
       "951.0         NaN        0        1        0        0        0        1        0         0   \n",
       "612.0         NaN        1        0        0        0        1        0        0         0   \n",
       "621.0         NaN        1        0        0        0        1        0        0         0   \n",
       "636.0         NaN        1        0        0        0        1        0        0         0   \n",
       "326.0         NaN        0        1        0        0        1        0        0         0   \n",
       "705.0         NaN        1        0        0        1        0        0        0         0   \n",
       "219.0         NaN        0        1        0        1        0        0        0         0   \n",
       "222.0         NaN        0        1        0        1        0        0        0         0   \n",
       "714.0         NaN        0        1        0        1        0        0        0         0   \n",
       "751.0         NaN        0        1        0        1        0        0        0         0   \n",
       "654.0         NaN        1        0        1        0        0        0        0         0   \n",
       "170.0         NaN        0        1        1        0        0        0        0         0   \n",
       "927.0         NaN        0        1        1        0        0        0        0         0   \n",
       "646.0         NaN        1        0        0        0        0        1        0         0   \n",
       "331.0         NaN        1        0        0        0        1        0        0         0   \n",
       "562.0         NaN        1        0        0        0        1        0        0         0   \n",
       "345.0         NaN        1        0        0        1        0        0        0         0   \n",
       "\n",
       "        area_2.0   ...     k8_0.0  k8_1.0  k10_0.0  k10_1.0  k12_0.0  k12_1.0  k13_0.0  k13_1.0  \\\n",
       "id                 ...                                                                            \n",
       "24.0           0   ...          0       1        1        0        1        0        0        1   \n",
       "107.0          0   ...          0       1        0        1        0        1        0        1   \n",
       "111.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "118.0          0   ...          0       1        1        0        1        0        0        1   \n",
       "182.0          0   ...          0       1        1        0        1        0        1        0   \n",
       "281.0          0   ...          0       1        1        0        1        0        0        1   \n",
       "289.0          0   ...          0       1        1        0        1        0        0        1   \n",
       "339.0          0   ...          1       0        0        1        1        0        1        0   \n",
       "357.0          0   ...          1       0        0        1        0        1        1        0   \n",
       "415.0          0   ...          1       0        1        0        1        0        0        1   \n",
       "447.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "629.0          0   ...          0       1        1        0        1        0        1        0   \n",
       "727.0          0   ...          1       0        1        0        0        1        0        1   \n",
       "755.0          0   ...          1       0        1        0        1        0        0        1   \n",
       "120.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "129.0          0   ...          1       0        1        0        1        0        0        1   \n",
       "168.0          0   ...          0       1        1        0        1        0        0        1   \n",
       "202.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "499.0          0   ...          0       1        0        1        1        0        0        1   \n",
       "613.0          0   ...          1       0        1        0        1        0        0        1   \n",
       "801.0          0   ...          1       0        0        1        0        1        1        0   \n",
       "809.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "986.0          0   ...          0       1        1        0        1        0        0        1   \n",
       "87.0           0   ...          0       1        1        0        1        0        1        0   \n",
       "103.0          0   ...          0       1        1        0        1        0        1        0   \n",
       "119.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "141.0          0   ...          1       0        0        1        0        1        1        0   \n",
       "768.0          0   ...          0       1        1        0        0        1        1        0   \n",
       "805.0          0   ...          0       1        0        1        1        0        1        0   \n",
       "857.0          0   ...          0       1        1        0        1        0        1        0   \n",
       "...          ...   ...        ...     ...      ...      ...      ...      ...      ...      ...   \n",
       "982.0          0   ...          0       1        1        0        0        1        1        0   \n",
       "236.0          0   ...          1       0        0        1        0        1        1        0   \n",
       "909.0          0   ...          1       0        0        1        0        1        1        0   \n",
       "166.0          0   ...          1       0        0        1        1        0        1        0   \n",
       "381.0          0   ...          1       0        0        1        0        1        1        0   \n",
       "1016.0         0   ...          1       0        0        1        0        1        1        0   \n",
       "845.0          0   ...          1       0        1        0        0        1        1        0   \n",
       "924.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "72.0           1   ...          0       1        0        1        1        0        0        1   \n",
       "620.0          1   ...          1       0        1        0        1        0        0        1   \n",
       "204.0          1   ...          0       1        1        0        1        0        0        1   \n",
       "228.0          1   ...          1       0        1        0        1        0        1        0   \n",
       "21.0           0   ...          1       0        1        0        1        0        1        0   \n",
       "951.0          1   ...          1       0        1        0        1        0        1        0   \n",
       "612.0          1   ...          0       1        1        0        1        0        0        1   \n",
       "621.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "636.0          1   ...          0       1        1        0        1        0        0        1   \n",
       "326.0          1   ...          1       0        1        0        0        1        1        0   \n",
       "705.0          0   ...          0       1        1        0        0        1        1        0   \n",
       "219.0          0   ...          1       0        0        1        1        0        1        0   \n",
       "222.0          1   ...          1       0        1        0        1        0        1        0   \n",
       "714.0          1   ...          1       0        1        0        1        0        1        0   \n",
       "751.0          0   ...          0       1        0        1        1        0        1        0   \n",
       "654.0          1   ...          0       1        1        0        0        1        1        0   \n",
       "170.0          1   ...          1       0        0        1        0        1        1        0   \n",
       "927.0          0   ...          1       0        0        1        1        0        1        0   \n",
       "646.0          0   ...          0       1        1        0        1        0        1        0   \n",
       "331.0          0   ...          0       1        0        1        0        1        1        0   \n",
       "562.0          0   ...          0       1        1        0        0        1        1        0   \n",
       "345.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "\n",
       "        k14_0.0  k14_1.0  \n",
       "id                        \n",
       "24.0          0        1  \n",
       "107.0         0        1  \n",
       "111.0         0        1  \n",
       "118.0         1        0  \n",
       "182.0         0        1  \n",
       "281.0         1        0  \n",
       "289.0         0        1  \n",
       "339.0         0        1  \n",
       "357.0         0        1  \n",
       "415.0         0        1  \n",
       "447.0         0        1  \n",
       "629.0         1        0  \n",
       "727.0         0        1  \n",
       "755.0         0        1  \n",
       "120.0         0        1  \n",
       "129.0         0        1  \n",
       "168.0         0        1  \n",
       "202.0         0        1  \n",
       "499.0         0        1  \n",
       "613.0         0        1  \n",
       "801.0         0        1  \n",
       "809.0         0        1  \n",
       "986.0         1        0  \n",
       "87.0          0        1  \n",
       "103.0         0        1  \n",
       "119.0         0        1  \n",
       "141.0         0        1  \n",
       "768.0         1        0  \n",
       "805.0         1        0  \n",
       "857.0         0        1  \n",
       "...         ...      ...  \n",
       "982.0         1        0  \n",
       "236.0         0        1  \n",
       "909.0         0        1  \n",
       "166.0         0        1  \n",
       "381.0         0        1  \n",
       "1016.0        0        1  \n",
       "845.0         0        1  \n",
       "924.0         0        1  \n",
       "72.0          0        1  \n",
       "620.0         1        0  \n",
       "204.0         1        0  \n",
       "228.0         0        1  \n",
       "21.0          0        1  \n",
       "951.0         0        1  \n",
       "612.0         0        1  \n",
       "621.0         1        0  \n",
       "636.0         0        1  \n",
       "326.0         0        1  \n",
       "705.0         1        0  \n",
       "219.0         0        1  \n",
       "222.0         0        1  \n",
       "714.0         0        1  \n",
       "751.0         1        0  \n",
       "654.0         0        1  \n",
       "170.0         0        1  \n",
       "927.0         0        1  \n",
       "646.0         0        1  \n",
       "331.0         1        0  \n",
       "562.0         0        1  \n",
       "345.0         0        1  \n",
       "\n",
       "[155 rows x 64 columns]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New_data의 ID Number 24 부터는 모두 NA 값이 포함되어 있는 것을 볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "24.0      True\n",
       "107.0     True\n",
       "111.0     True\n",
       "118.0     True\n",
       "182.0     True\n",
       "281.0     True\n",
       "289.0     True\n",
       "339.0     True\n",
       "357.0     True\n",
       "415.0     True\n",
       "447.0     True\n",
       "629.0     True\n",
       "727.0     True\n",
       "755.0     True\n",
       "120.0     True\n",
       "129.0     True\n",
       "168.0     True\n",
       "202.0     True\n",
       "499.0     True\n",
       "613.0     True\n",
       "801.0     True\n",
       "809.0     True\n",
       "986.0     True\n",
       "87.0      True\n",
       "103.0     True\n",
       "119.0     True\n",
       "141.0     True\n",
       "768.0     True\n",
       "805.0     True\n",
       "857.0     True\n",
       "          ... \n",
       "982.0     True\n",
       "236.0     True\n",
       "909.0     True\n",
       "166.0     True\n",
       "381.0     True\n",
       "1016.0    True\n",
       "845.0     True\n",
       "924.0     True\n",
       "72.0      True\n",
       "620.0     True\n",
       "204.0     True\n",
       "228.0     True\n",
       "21.0      True\n",
       "951.0     True\n",
       "612.0     True\n",
       "621.0     True\n",
       "636.0     True\n",
       "326.0     True\n",
       "705.0     True\n",
       "219.0     True\n",
       "222.0     True\n",
       "714.0     True\n",
       "751.0     True\n",
       "654.0     True\n",
       "170.0     True\n",
       "927.0     True\n",
       "646.0     True\n",
       "331.0     True\n",
       "562.0     True\n",
       "345.0     True\n",
       "Name: ideo_self, Length: 155, dtype: bool"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[24:].ideo_self.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideo_self의 NA 값을 기준으로 Train data와 Test data를 나눠준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = new_data[:199]\n",
    "test_data = new_data[24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ideo_self</th>\n",
       "      <th>sex_1.0</th>\n",
       "      <th>sex_2.0</th>\n",
       "      <th>age_2.0</th>\n",
       "      <th>age_3.0</th>\n",
       "      <th>age_4.0</th>\n",
       "      <th>age_5.0</th>\n",
       "      <th>age_6.0</th>\n",
       "      <th>area_1.0</th>\n",
       "      <th>area_2.0</th>\n",
       "      <th>...</th>\n",
       "      <th>k8_0.0</th>\n",
       "      <th>k8_1.0</th>\n",
       "      <th>k10_0.0</th>\n",
       "      <th>k10_1.0</th>\n",
       "      <th>k12_0.0</th>\n",
       "      <th>k12_1.0</th>\n",
       "      <th>k13_0.0</th>\n",
       "      <th>k13_1.0</th>\n",
       "      <th>k14_0.0</th>\n",
       "      <th>k14_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ideo_self  sex_1.0  sex_2.0  age_2.0  age_3.0  age_4.0  age_5.0  age_6.0  area_1.0  \\\n",
       "id                                                                                           \n",
       "1012.0        5.0        1        0        1        0        0        0        0         1   \n",
       "556.0         7.0        1        0        0        0        0        1        0         0   \n",
       "642.0        10.0        1        0        1        0        0        0        0         0   \n",
       "403.0         3.0        0        1        0        0        0        1        0         0   \n",
       "776.0         6.0        1        0        0        1        0        0        0         1   \n",
       "\n",
       "        area_2.0   ...     k8_0.0  k8_1.0  k10_0.0  k10_1.0  k12_0.0  k12_1.0  k13_0.0  k13_1.0  \\\n",
       "id                 ...                                                                            \n",
       "1012.0         0   ...          1       0        1        0        0        1        1        0   \n",
       "556.0          0   ...          0       1        1        0        1        0        0        1   \n",
       "642.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "403.0          1   ...          1       0        1        0        1        0        1        0   \n",
       "776.0          0   ...          1       0        0        1        0        1        1        0   \n",
       "\n",
       "        k14_0.0  k14_1.0  \n",
       "id                        \n",
       "1012.0        0        1  \n",
       "556.0         0        1  \n",
       "642.0         0        1  \n",
       "403.0         1        0  \n",
       "776.0         0        1  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ideo_self</th>\n",
       "      <th>sex_1.0</th>\n",
       "      <th>sex_2.0</th>\n",
       "      <th>age_2.0</th>\n",
       "      <th>age_3.0</th>\n",
       "      <th>age_4.0</th>\n",
       "      <th>age_5.0</th>\n",
       "      <th>age_6.0</th>\n",
       "      <th>area_1.0</th>\n",
       "      <th>area_2.0</th>\n",
       "      <th>...</th>\n",
       "      <th>k8_0.0</th>\n",
       "      <th>k8_1.0</th>\n",
       "      <th>k10_0.0</th>\n",
       "      <th>k10_1.0</th>\n",
       "      <th>k12_0.0</th>\n",
       "      <th>k12_1.0</th>\n",
       "      <th>k13_0.0</th>\n",
       "      <th>k13_1.0</th>\n",
       "      <th>k14_0.0</th>\n",
       "      <th>k14_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ideo_self  sex_1.0  sex_2.0  age_2.0  age_3.0  age_4.0  age_5.0  age_6.0  area_1.0  \\\n",
       "id                                                                                          \n",
       "24.0         NaN        1        0        0        0        0        0        1         1   \n",
       "107.0        NaN        1        0        0        0        0        0        1         1   \n",
       "111.0        NaN        1        0        0        0        0        0        1         1   \n",
       "118.0        NaN        1        0        0        0        0        0        1         1   \n",
       "182.0        NaN        1        0        0        0        0        0        1         1   \n",
       "\n",
       "       area_2.0   ...     k8_0.0  k8_1.0  k10_0.0  k10_1.0  k12_0.0  k12_1.0  k13_0.0  k13_1.0  \\\n",
       "id                ...                                                                            \n",
       "24.0          0   ...          0       1        1        0        1        0        0        1   \n",
       "107.0         0   ...          0       1        0        1        0        1        0        1   \n",
       "111.0         0   ...          1       0        1        0        1        0        1        0   \n",
       "118.0         0   ...          0       1        1        0        1        0        0        1   \n",
       "182.0         0   ...          0       1        1        0        1        0        1        0   \n",
       "\n",
       "       k14_0.0  k14_1.0  \n",
       "id                       \n",
       "24.0         0        1  \n",
       "107.0        0        1  \n",
       "111.0        0        1  \n",
       "118.0        1        0  \n",
       "182.0        0        1  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train Data를 X와 Y를 나눠주기 위해 Train data의 columns 이름과 갯수를 확인한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 변수명 가져오기\n",
    "col_names = train_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ideo_self'], dtype=object)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ideo_self column은 첫 번째 column이다. \n",
    "col_names[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ideo_self를 기준으로 X와 Y를 나눠준다. \n",
    "train_X = train_data[col_names[1:]]\n",
    "train_Y = train_data[col_names[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex_1.0</th>\n",
       "      <th>sex_2.0</th>\n",
       "      <th>age_2.0</th>\n",
       "      <th>age_3.0</th>\n",
       "      <th>age_4.0</th>\n",
       "      <th>age_5.0</th>\n",
       "      <th>age_6.0</th>\n",
       "      <th>area_1.0</th>\n",
       "      <th>area_2.0</th>\n",
       "      <th>area_3.0</th>\n",
       "      <th>...</th>\n",
       "      <th>k8_0.0</th>\n",
       "      <th>k8_1.0</th>\n",
       "      <th>k10_0.0</th>\n",
       "      <th>k10_1.0</th>\n",
       "      <th>k12_0.0</th>\n",
       "      <th>k12_1.0</th>\n",
       "      <th>k13_0.0</th>\n",
       "      <th>k13_1.0</th>\n",
       "      <th>k14_0.0</th>\n",
       "      <th>k14_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sex_1.0  sex_2.0  age_2.0  age_3.0  age_4.0  age_5.0  age_6.0  area_1.0  area_2.0  \\\n",
       "id                                                                                          \n",
       "1012.0        1        0        1        0        0        0        0         1         0   \n",
       "556.0         1        0        0        0        0        1        0         0         0   \n",
       "642.0         1        0        1        0        0        0        0         0         0   \n",
       "403.0         0        1        0        0        0        1        0         0         1   \n",
       "776.0         1        0        0        1        0        0        0         1         0   \n",
       "\n",
       "        area_3.0   ...     k8_0.0  k8_1.0  k10_0.0  k10_1.0  k12_0.0  k12_1.0  k13_0.0  k13_1.0  \\\n",
       "id                 ...                                                                            \n",
       "1012.0         0   ...          1       0        1        0        0        1        1        0   \n",
       "556.0          0   ...          0       1        1        0        1        0        0        1   \n",
       "642.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "403.0          0   ...          1       0        1        0        1        0        1        0   \n",
       "776.0          0   ...          1       0        0        1        0        1        1        0   \n",
       "\n",
       "        k14_0.0  k14_1.0  \n",
       "id                        \n",
       "1012.0        0        1  \n",
       "556.0         0        1  \n",
       "642.0         0        1  \n",
       "403.0         1        0  \n",
       "776.0         0        1  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ideo_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012.0</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556.0</th>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642.0</th>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403.0</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776.0</th>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ideo_self\n",
       "id               \n",
       "1012.0        5.0\n",
       "556.0         7.0\n",
       "642.0        10.0\n",
       "403.0         3.0\n",
       "776.0         6.0"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data를 X와 Y를 나눠주기 위해 Test data의 columns 이름과 갯수를 확인한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 변수명 가져오기\n",
    "col_names = test_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ideo_self'], dtype=object)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ideo_self column은 첫 번째 column이다. \n",
    "col_names[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ideo_self를 기준으로 X와 Y를 나눠준다. \n",
    "test_X = test_data[col_names[1:]]\n",
    "test_Y = test_data[col_names[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex_1.0</th>\n",
       "      <th>sex_2.0</th>\n",
       "      <th>age_2.0</th>\n",
       "      <th>age_3.0</th>\n",
       "      <th>age_4.0</th>\n",
       "      <th>age_5.0</th>\n",
       "      <th>age_6.0</th>\n",
       "      <th>area_1.0</th>\n",
       "      <th>area_2.0</th>\n",
       "      <th>area_3.0</th>\n",
       "      <th>...</th>\n",
       "      <th>k8_0.0</th>\n",
       "      <th>k8_1.0</th>\n",
       "      <th>k10_0.0</th>\n",
       "      <th>k10_1.0</th>\n",
       "      <th>k12_0.0</th>\n",
       "      <th>k12_1.0</th>\n",
       "      <th>k13_0.0</th>\n",
       "      <th>k13_1.0</th>\n",
       "      <th>k14_0.0</th>\n",
       "      <th>k14_1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182.0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sex_1.0  sex_2.0  age_2.0  age_3.0  age_4.0  age_5.0  age_6.0  area_1.0  area_2.0  \\\n",
       "id                                                                                         \n",
       "24.0         1        0        0        0        0        0        1         1         0   \n",
       "107.0        1        0        0        0        0        0        1         1         0   \n",
       "111.0        1        0        0        0        0        0        1         1         0   \n",
       "118.0        1        0        0        0        0        0        1         1         0   \n",
       "182.0        1        0        0        0        0        0        1         1         0   \n",
       "\n",
       "       area_3.0   ...     k8_0.0  k8_1.0  k10_0.0  k10_1.0  k12_0.0  k12_1.0  k13_0.0  k13_1.0  \\\n",
       "id                ...                                                                            \n",
       "24.0          0   ...          0       1        1        0        1        0        0        1   \n",
       "107.0         0   ...          0       1        0        1        0        1        0        1   \n",
       "111.0         0   ...          1       0        1        0        1        0        1        0   \n",
       "118.0         0   ...          0       1        1        0        1        0        0        1   \n",
       "182.0         0   ...          0       1        1        0        1        0        1        0   \n",
       "\n",
       "       k14_0.0  k14_1.0  \n",
       "id                       \n",
       "24.0         0        1  \n",
       "107.0        0        1  \n",
       "111.0        0        1  \n",
       "118.0        1        0  \n",
       "182.0        0        1  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ideo_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182.0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ideo_self\n",
       "id              \n",
       "24.0         NaN\n",
       "107.0        NaN\n",
       "111.0        NaN\n",
       "118.0        NaN\n",
       "182.0        NaN"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Split Train data\n",
    "1. Training set (70%)\n",
    "2. Validation set (30%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model에 학습시키기 위해 Train set의 X와 Y 값을 array 값으로 바꿔준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = train_X.values\n",
    "train_Y = train_Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Y 값을 numpy.ravel 함수를 써서 reshape 시켜준다. Return a contiguous flattened array.\n",
    "\n",
    "train_Y = np.ravel(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 1, 0, 1],\n",
       "       [1, 0, 1, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [0, 1, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 1, ..., 1, 1, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.,   7.,  10.,   3.,   6.,   5.,   5.,   5.,   4.,   8.,   3.,\n",
       "         3.,   5.,   1.,   7.,   4.,   3.,   4.,   9.,   4.,   5.,   3.,\n",
       "         7.,   6.,   3.,   3.,   9.,   5.,   3.,   4.,   4.,   5.,   5.,\n",
       "         5.,  10.,   5.,   0.,   2.,   5.,   5.,   6.,   0.,   2.,   3.,\n",
       "         2.,   2.,   5.,   4.,   6.,   2.,   5.,   6.,   4.,   5.,   6.,\n",
       "         1.,   8.,   0.,   4.,   3.,   3.,   4.,   5.,   8.,   7.,   5.,\n",
       "         5.,   5.,   5.,   3.,   6.,   6.,   8.,   3.,   5.,   0.,   7.,\n",
       "         5.,   6.,   2.,   8.,   4.,   3.,   7.,   3.,   5.,   4.,   0.,\n",
       "         5.,   5.,   6.,   8.,   5.,   2.,   5.,   5.,   6.,   3.,   4.,\n",
       "         7.,   6.,   5.,   5.,   6.,   4.,   5.,   6.,   5.,   5.,   1.,\n",
       "         7.,   5.,   4.,   3.,   7.,   6.,  10.,   5.,   5.,   5.,   6.,\n",
       "         6.,   0.,   4.,   3.,  10.,   3.,   1.,   7.,   4.,   6.,   5.,\n",
       "         6.,   4.,   5.,   5.,   5.,   5.,   5.,   1.,   2.,   3.,   4.,\n",
       "         6.,   5.,   4.,  10.,   8.,   7.,   6.,   6.,   5.,   5.,   7.,\n",
       "         4.,   8.,  10.,  10.,   3.,   7.,   5.,   6.,  10.,   7.,   5.,\n",
       "         4.,   8.,   3.,   5.,   6.,   7.,   8.,   7.,   7.,   6.,   4.,\n",
       "         5.,   5.,   0.,   5.,   5.,   0.,   3.,   6.,   5.,   2.,   5.,\n",
       "         2.,   6.,   3.,   5.,   2.,   4.,   5.,   3.,   2.,   6.,   5.,\n",
       "         5.,  10.,   5.,   1.,   7.,   5.,   6.,   8.,   5.,   7.,   3.,\n",
       "         3.,   5.,   7.,   4.,   5.,   6.,   0.,   1.,   8.,   4.,   5.,\n",
       "         5.,   6.,   6.,   5.,   3.,   6.,   5.,   4.,   5.,   5.,   5.,\n",
       "         7.,   6.,   4.,   8.,   4.,   5.,   1.,   5.,   4.,   5.,   6.,\n",
       "         3.,   5.,   4.,   4.,   8.,  10.,   5.,   5.,   5.,   7.,   7.,\n",
       "         7.,   5.,   4.,   5.,   5.,   5.,   8.,   3.,   4.,   6.,   6.,\n",
       "         6.,   7.,   9.,   6.,   5.,   5.,   7.,   0.,   5.,   5.,   6.,\n",
       "         1.,  10.,   4.,   7.,   8.,   7.,   3.,   4.,   3.,   7.,   8.,\n",
       "         5.,   3.,   5.,   3.,   5.,   6.,   7.,   3.,   4.,   8.,   5.,\n",
       "         8.,   2.,   8.,   4.,   6.,   5.,   0.,   6.,  10.,   2.,   5.,\n",
       "         7.,   3.,   6.,   6.,   5.,   5.,   7.,   4.,   8.,   5.,   5.,\n",
       "         5.,   7.,   3.,   5.,   3.,   3.,   6.,  10.,   6.,   6.,   7.,\n",
       "         2.,   5.,  10.,   9.,   9.,   3.,   5.,   5.,   6.,   5.,   5.,\n",
       "         9.,   7.,   5.,   6.,  10.,   6.,   4.,   6.,   5.,   5.,   7.,\n",
       "         3.,   4.,   6.,   4.,   5.,   1.,   5.,   7.,   3.,   3.,   4.,\n",
       "         4.,   8.,   4.,   5.,   5.,   5.,   5.,   7.,   9.,   6.,   5.,\n",
       "         6.,   5.,   4.,   7.,   2.,   3.,   4.,   6.,   5.,   4.,   3.,\n",
       "         7.,   7.,   5.,   6.,  10.,   7.,   5.,  10.,   7.,   5.,   5.,\n",
       "         5.,   5.,   3.,   6.,   0.,   5.,   5.,   5.,   4.,   0.,   2.,\n",
       "        10.,   6.,   6.,   7.,   1.,  10.,   5.,   7.,   1.,   5.,   0.,\n",
       "         1.,   4.,   5.,   5.,   8.,   7.,   5.,   5.,   9.,   2.,   5.,\n",
       "         7.,   2.,   3.,   4.,   8.,   5.,   1.,   6.,   5.,   3.,   4.,\n",
       "         9.,   2.,  10.,   8.,   7.,  10.,   5.,   5.,   5.,   4.,   8.,\n",
       "         3.,   5.,   5.,   5.,   6.,   3.,   3.,   7.,   7.,   5.,   3.,\n",
       "         8.,   6.,   2.,   7.,   3.,   3.,   4.,   7.,   6.,   8.,   5.,\n",
       "         5.,   8.,   3.,   3.,   6.,   5.,   5.,   5.,   8.,   5.,  10.,\n",
       "         5.,  10.,   8.,   5.,   5.,   8.,   6.,   6.,   6.,   6.,   5.,\n",
       "         5.,   5.,   5.,   5.,   8.,   7.,   5.,   0.,   5.,   5.,   5.,\n",
       "         7.,   5.,   5.,   4.,   6.,   5.,   3.,   8.,   7.,   8.,   6.,\n",
       "         5.,   4.,   9.,   7.,   5.,   5.,   0.,   5.,   5.,   3.,   5.,\n",
       "         4.,   7.,   5.,   8.,   4.,   3.,  10.,   5.,   3.,   4.,   9.,\n",
       "         5.,   0.,   5.,   5.,   5.,  10.,   2.,   5.,   5.,   8.,  10.,\n",
       "         5.,   6.,   3.,   5.,   5.,   5.,  10.,   5.,   0.,   0.,   8.,\n",
       "         6.,   2.,  10.,   5.,   0.,   5.,  10.,   4.,  10.,   4.,   9.,\n",
       "         6.,   8.,   3.,   5.,   0.,  10.,   5.,   7.,   4.,   3.,   4.,\n",
       "         6.,   8.,   2.,   5.,   6.,   8.,  10.,   5.,   4.,   5.,   2.,\n",
       "         5.,   4.,   5.,   5.,   8.,   4.,   3.,   1.,   6.,   5.,   8.,\n",
       "         8.,   4.,   0.,   6.,   3.,   3.,   5.,   2.,   5.,   6.,   7.,\n",
       "         5.,   5.,   5.,   2.,   3.,   6.,   5.,   5.,   7.,   5.,   5.,\n",
       "         5.,   6.,   4.,   6.,   5.,   5.,   5.,   4.,   8.,   5.,   0.,\n",
       "        10.,   8.,   5.,   5.,   6.,   4.,   5.,   9.,   3.,   3.,   2.,\n",
       "         4.,   9.,   5.,   2.,   8.,   0.,   4.,   3.,   3.,   6.,   3.,\n",
       "         5.,   9.,   4.,   3.,   9.,   4.,   4.,   8.,   5.,  10.,   3.,\n",
       "        10.,   6.,   2.,   6.,   0.,   1.,   7.,   1.,   5.,   1.,   7.,\n",
       "         7.,   5.,   5.,   3.,   7.,   5.,   2.,   7.,   5.,   0.,   4.,\n",
       "         3.,   5.,   6.,   8.,   3.,   5.,   5.,   3.,   7.,   5.,   4.,\n",
       "         5.,   7.,   1.,   8.,   3.,   5.,   5.,  10.,   8.,   0.,   5.,\n",
       "         9.,   4.,   3.,   7.,   3.,   3.,   4.,   1.,   5.,   2.,   0.,\n",
       "        10.,   2.,   5.,   6.,   2.,   5.,   3.,  10.,   5.,   6.,   7.,\n",
       "         5.,   6.,   6.,   7.,   5.,   3.,   8.,   4.,  10.,   4.,   8.,\n",
       "         5.,   9.,   4.,   8.,   3.,   1.,   5.,   5.,   5.,  10.,   5.,\n",
       "        10.,   5.,   5.,   9.,   5.,   5.,  10.,   3.,   2.,   4.,   5.,\n",
       "         5.,  10.,   3.,   5.,   7.,   4.,   4.,   6.,   4.,   7.,   3.,\n",
       "         6.,   5.,   5.,   3.,   5.,   5.,   6.,   5.,   3.,   6.,   9.,\n",
       "         7.,   0.,   7.,   4.,   3.,   5.,   5.,   5.,   8.,   5.,   4.,\n",
       "         8.,   4.,   8.,   7.,   5.,   5.,   7.,   3.,   3.,   5.,   8.,\n",
       "         8.,   7.,   7.,   5.,   5.,   4.,   5.,   5.,   6.,   5.,   3.,\n",
       "         4.,   2.,  10.,   3.,  10.,   5.,   4.,   8.,  10.,   8.,   5.,\n",
       "         2.,   5.,   5.,   6.,   5.,   7.,   2.,   5.,   7.,   5.,   7.,\n",
       "         0.,   2.,   4.,   5.,   7.,   7.,   6.,   6.,   4.,  10.,   0.,\n",
       "         4.,   6.,  10.,   2.,   5.,   8.,   3.,   3.,   4.,   3.,   5.,\n",
       "         7.,   5.,   5.,  10.,   8.,  10.,   5.,   6.,   5.,   1.,   3.,\n",
       "         5.,   5.,   5.,  10.,   5.,   5.,   9.,   3.,   7.,   5.,   7.,\n",
       "         4.,  10.,   7.,   5.,   3.,   2.,   4.,   5.])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model에 학습시키기 위해 Test set의 X와 Y 값을 array 값으로 바꿔준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = test_X.values\n",
    "test_Y = test_Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Y 값을 numpy.ravel 함수를 써서 reshape 시켜준다. Return a contiguous flattened array.\n",
    "\n",
    "test_Y = np.ravel(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 1, 0, 1],\n",
       "       [1, 0, 0, ..., 1, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [1, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skitlearn library를 통해 Train set을 Train 과 Validataion 으로 나눠준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_train, train_X_val, train_Y_train, train_Y_val = train_test_split(train_X, train_Y, \n",
    "                                                        test_size=0.3, \n",
    "                                                        random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(629, 63)\n",
      "(270, 63)\n",
      "(629,)\n",
      "(270,)\n"
     ]
    }
   ],
   "source": [
    "# Train, Validation Set의 shape을 확인해준다.\n",
    "\n",
    "print(train_X_train.shape)\n",
    "print(train_X_val.shape)\n",
    "print(train_Y_train.shape)\n",
    "print(train_Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fit the model and compare validation AUCs and prediction probability\n",
    "비교하고자 하는 classifiers들은 다음과 같음\n",
    "1. Logistic regression\n",
    "2. k-nearest neighbor classifier\n",
    "3. naive Bayes classifier\n",
    "4. Decision tree\n",
    "5. Random forest\n",
    "6. SVM\n",
    "7. Xgboost\n",
    "8. SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Logistic regression\n",
    "Manual for `sklearn.linear_model.LogisticRegression`: [click](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "다음 parameter들에 대해 validation data에 대한 Score값과 AUC값을 이용해 최적 모형 parameter를 찾는다. \n",
    "1. penalty\n",
    "2. C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C가 클수록 weak regularization\n",
    "penalty_set = ['l1', 'l2']\n",
    "C_set = [0.1, 1, 10, 1e2, 1e3, 1e4, 1e5, 1e6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\python36\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "result1 = []\n",
    "for penalty in penalty_set:\n",
    "    for C in C_set:\n",
    "        logreg_model = LogisticRegression(penalty=penalty, C=C, class_weight='balanced', multi_class=\"multinomial\", solver='saga', max_iter=10000)\n",
    "        logreg_model = logreg_model.fit(train_X_train, train_Y_train)\n",
    "#         Y_val_score = model.decision_function(train_X_val)\n",
    "        Y_val_score = logreg_model.predict_proba(train_X_val)[:, 1]\n",
    "        val_proba = \"{:.4f}\".format(logreg_model.score(train_X_val, train_Y_val))\n",
    "        fpr, tpr, _ = roc_curve(train_Y_val, Y_val_score, pos_label=True)\n",
    "        result1.append((logreg_model, penalty, C, val_proba, auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 0.1, '0.0926', 0.70113207547169809),\n",
       " (LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 1, '0.1704', 0.62792452830188683),\n",
       " (LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 10, '0.1630', 0.64830188679245282),\n",
       " (LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 100.0, '0.1333', 0.49886792452830186),\n",
       " (LogisticRegression(C=1000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 1000.0, '0.1407', 0.59849056603773587),\n",
       " (LogisticRegression(C=10000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 10000.0, '0.1111', 0.49660377358490571),\n",
       " (LogisticRegression(C=100000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 100000.0, '0.1444', 0.50641509433962262),\n",
       " (LogisticRegression(C=1000000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 1000000.0, '0.1444', 0.49660377358490571),\n",
       " (LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 0.1, '0.1741', 0.71169811320754717),\n",
       " (LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 1, '0.1556', 0.65811320754716984),\n",
       " (LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 10, '0.1519', 0.64000000000000001),\n",
       " (LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 100.0, '0.1444', 0.53811320754716985),\n",
       " (LogisticRegression(C=1000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 1000.0, '0.1667', 0.49056603773584911),\n",
       " (LogisticRegression(C=10000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 10000.0, '0.1889', 0.51018867924528299),\n",
       " (LogisticRegression(C=100000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 100000.0, '0.1481', 0.44150943396226416),\n",
       " (LogisticRegression(C=1000000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 1000000.0, '0.1296', 0.49358490566037738)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg_result = sorted(result1, key=lambda x: x[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(LogisticRegression(C=10000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 10000.0, '0.1889', 0.51018867924528299),\n",
       " (LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 0.1, '0.1741', 0.71169811320754717),\n",
       " (LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 1, '0.1704', 0.62792452830188683),\n",
       " (LogisticRegression(C=1000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 1000.0, '0.1667', 0.49056603773584911),\n",
       " (LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 10, '0.1630', 0.64830188679245282),\n",
       " (LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 1, '0.1556', 0.65811320754716984),\n",
       " (LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 10, '0.1519', 0.64000000000000001),\n",
       " (LogisticRegression(C=100000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 100000.0, '0.1481', 0.44150943396226416),\n",
       " (LogisticRegression(C=100000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 100000.0, '0.1444', 0.50641509433962262),\n",
       " (LogisticRegression(C=1000000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 1000000.0, '0.1444', 0.49660377358490571),\n",
       " (LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 100.0, '0.1444', 0.53811320754716985),\n",
       " (LogisticRegression(C=1000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 1000.0, '0.1407', 0.59849056603773587),\n",
       " (LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 100.0, '0.1333', 0.49886792452830186),\n",
       " (LogisticRegression(C=1000000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l2', 1000000.0, '0.1296', 0.49358490566037738),\n",
       " (LogisticRegression(C=10000.0, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 10000.0, '0.1111', 0.49660377358490571),\n",
       " (LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "            multi_class='multinomial', n_jobs=1, penalty='l1',\n",
       "            random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "            warm_start=False), 'l1', 0.1, '0.0926', 0.70113207547169809)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result에 대해 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LogisticRegression(C=10000.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
      "          multi_class='multinomial', n_jobs=1, penalty='l2',\n",
      "          random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
      "          warm_start=False), 'l2', 10000.0, '0.1889', 0.51018867924528299)\n"
     ]
    }
   ],
   "source": [
    "best_logreg_result = logreg_result[0]\n",
    "print(best_logreg_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model의 MAE 값을 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.60740740741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\python36\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "best_logreg_model = best_logreg_result[0]\n",
    "best_logreg_model = best_logreg_model.fit(train_X_train, train_Y_train)\n",
    "print(metrics.mean_absolute_error(best_logreg_model.predict(train_X_val), train_Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확률:\n",
      "[[  5.60834164e-04   1.55556775e-08   6.58633385e-09   3.23814553e-04\n",
      "    5.79178781e-01   3.20140647e-01   9.96933820e-02   3.37284159e-05\n",
      "    5.76094152e-05   4.79657571e-16   1.11815127e-05]\n",
      " [  1.84085578e-01   4.88950725e-02   2.68903498e-02   1.63449381e-01\n",
      "    1.20663205e-01   1.70211796e-01   7.29739931e-02   1.10316393e-01\n",
      "    8.20697705e-02   7.56245155e-09   2.04444534e-02]\n",
      " [  3.39018770e-02   9.52643399e-08   1.38967107e-02   5.22160024e-01\n",
      "    6.00760717e-02   1.40356018e-02   2.74834495e-02   1.34108169e-01\n",
      "    1.83073561e-02   6.78903050e-14   1.76030645e-01]\n",
      " [  9.13434298e-05   7.85303602e-06   8.77125666e-06   8.50996102e-05\n",
      "    3.69908209e-01   5.63249447e-02   3.60182850e-01   2.13325140e-01\n",
      "    6.21700301e-05   1.28692605e-12   3.61884931e-06]\n",
      " [  4.62929946e-02   1.99816391e-06   2.37491031e-03   7.91961128e-02\n",
      "    1.35791031e-02   3.49176640e-02   1.80278575e-02   2.89808081e-01\n",
      "    8.65812317e-02   2.30851196e-12   4.29220047e-01]\n",
      " [  4.52292278e-03   2.23997166e-04   4.20848947e-06   9.25041495e-04\n",
      "    2.91216322e-03   4.06737635e-03   3.11385827e-02   3.35893829e-02\n",
      "    1.54662511e-01   4.15376567e-01   3.52577247e-01]]\n",
      "합: [ 1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# predict_proba 결과 중 앞부분 6개에 대해서만 확인한다.\n",
    "print(\"예측 확률:\\n{}\".format(best_logreg_model.predict_proba(train_X_val)[:6]))\n",
    "\n",
    "# 행 방향으로 확률을 더하면 모두 1이 된다.\n",
    "print(\"합: {}\".format(best_logreg_model.predict_proba(train_X_val)[:6].sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_proba의 결과에 argmax 함수를 적용해서 예측을 재연할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 큰 예측 확률의 인덱스:\n",
      "[ 4  0  3  4 10  9  6  4 10  5  2  2  7  8  0  1  1  4  8  9  8  0  0  6 10\n",
      "  8  0  0  3  0  1  5  6  5  1  4  7  0  8  5  0  5  4  5  4 10  3  5  3  1\n",
      "  3  4 10  5  8  9  2  7  8  4  5  0  6  5 10  7  0  6  0  8  8  0  3  1  1\n",
      "  5  7 10  1  7  0  5  7  5  0 10  9  3 10  5 10  2  1  8  2  1  6  8  7  3\n",
      "  0  6  5  5  3  7  2  4  7  6  0  7  2  1 10  3  6  7  4  8  9  8  5  5  0\n",
      "  0  1  3  1  0  5  9  0  8  3  0 10  0  1  8  8  2  6  1  5  3  4  7  0  0\n",
      "  6  9  3  3  4  1  2  0  1  8  6  8  1  1  0  3  0  7  5  2  2  0 10  6 10\n",
      "  9 10 10  2  0  7  0  9 10 10  7  1  7  7  5  2  9  0  4  0  3 10 10  2  6\n",
      "  7  1  1 10  2  9  1  9  5  8  3  8 10  3  3  0  7  6  2  0  0  5  2  0  3\n",
      "  3  3  8  1  4  6  7  1  6  2  8  6  7  8  0  0  9  6  8  3 10  6  1  8  8\n",
      "  4  1 10  0  5  1  9  8  2  0  2  5  4  3  8  9  0  4  5  7]\n",
      "예측:\n",
      "[  4.   0.   3.   4.  10.   9.   6.   4.  10.   5.   2.   2.   7.   8.   0.\n",
      "   1.   1.   4.   8.   9.   8.   0.   0.   6.  10.   8.   0.   0.   3.   0.\n",
      "   1.   5.   6.   5.   1.   4.   7.   0.   8.   5.   0.   5.   4.   5.   4.\n",
      "  10.   3.   5.   3.   1.   3.   4.  10.   5.   8.   9.   2.   7.   8.   4.\n",
      "   5.   0.   6.   5.  10.   7.   0.   6.   0.   8.   8.   0.   3.   1.   1.\n",
      "   5.   7.  10.   1.   7.   0.   5.   7.   5.   0.  10.   9.   3.  10.   5.\n",
      "  10.   2.   1.   8.   2.   1.   6.   8.   7.   3.   0.   6.   5.   5.   3.\n",
      "   7.   2.   4.   7.   6.   0.   7.   2.   1.  10.   3.   6.   7.   4.   8.\n",
      "   9.   8.   5.   5.   0.   0.   1.   3.   1.   0.   5.   9.   0.   8.   3.\n",
      "   0.  10.   0.   1.   8.   8.   2.   6.   1.   5.   3.   4.   7.   0.   0.\n",
      "   6.   9.   3.   3.   4.   1.   2.   0.   1.   8.   6.   8.   1.   1.   0.\n",
      "   3.   0.   7.   5.   2.   2.   0.  10.   6.  10.   9.  10.  10.   2.   0.\n",
      "   7.   0.   9.  10.  10.   7.   1.   7.   7.   5.   2.   9.   0.   4.   0.\n",
      "   3.  10.  10.   2.   6.   7.   1.   1.  10.   2.   9.   1.   9.   5.   8.\n",
      "   3.   8.  10.   3.   3.   0.   7.   6.   2.   0.   0.   5.   2.   0.   3.\n",
      "   3.   3.   8.   1.   4.   6.   7.   1.   6.   2.   8.   6.   7.   8.   0.\n",
      "   0.   9.   6.   8.   3.  10.   6.   1.   8.   8.   4.   1.  10.   0.   5.\n",
      "   1.   9.   8.   2.   0.   2.   5.   4.   3.   8.   9.   0.   4.   5.   7.]\n"
     ]
    }
   ],
   "source": [
    "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(np.argmax(best_logreg_model.predict_proba(train_X_val), axis=1)))\n",
    "print(\"예측:\\n{}\".format(best_logreg_model.predict(train_X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      "가장 큰 결정 함수의 인덱스: [ 0  1 10  8  0  6  9  1  2  3]\n",
      "인덱스를 classses_에 연결: [  0.   1.  10.   8.   0.   6.   9.   1.   2.   3.]\n",
      "Validation set의 예측: [  5.   5.   5.   3.  10.  10.   4.   5.  10.   5.]\n",
      "실제 Validation set: [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
      "Validation Set의 정확도: 0.14\n",
      "Test set의 예측: [  8.  10.   1.   8.   7.  10.   8.   0.   1.   9.]\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 데이터에 있는 클래스 종류: {}\".format(best_logreg_model.classes_))\n",
    "argmax_dec_func = np.argmax(best_logreg_model.decision_function(train_X_train), axis=1)\n",
    "print(\"가장 큰 결정 함수의 인덱스: {}\".format(argmax_dec_func[:10]))\n",
    "print(\"인덱스를 classses_에 연결: {}\".format(best_logreg_model.classes_[argmax_dec_func][:10]))\n",
    "print(\"Validation set의 예측: {}\".format(best_knn_model.predict(train_X_val)[:10]))\n",
    "print(\"실제 Validation set: {}\".format(train_Y_val[:10]))\n",
    "print(\"Validation Set의 정확도: {:.2f}\".format(best_logreg_model.score(train_X_val, train_Y_val)))\n",
    "print(\"Test set의 예측: {}\".format(best_logreg_model.predict(test_X)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set의 예측: [  8.  10.   1.   8.   7.  10.   8.   0.   1.   9.   1.   4.   9.   8.   0.\n",
      "  10.  10.  10.  10.   9.   3.   7.   8.   7.   7.   0.   2.   3.   2.  10.\n",
      "   5.  10.   0.   2.   0.   3.   3.   3.   6.   6.   2.   6.  10.   5.   2.\n",
      "   0.   8.  10.   7.   8.   8.  10.   9.   1.   8.   0.   5.   4.   9.   7.\n",
      "   2.   2.   7.   7.   2.   2.   0.   0.   5.   0.   1.   1.   9.   1.   1.\n",
      "   2.   0.  10.   1.   3.   6.   0.   4.   8.   5.   5.   3.   1.   3.   4.\n",
      "   0.   0.   5.  10.   6.   3.   5.   5.   4.   5.   4.   3.  10.   6.   5.\n",
      "   5.   3.  10.  10.  10.   2.  10.  10.   3.   7.   1.   3.   7.   6.   5.\n",
      "   6.   6.   4.   8.   7.   5.   8.   3.   3.   2.   0.   5.   0.  10.   6.\n",
      "   6.   6.   6.   5.   7.   0.   7.   1.   3.   0.   0.   5.   7.   5.   1.\n",
      "   0.   7.   5.   5.   7.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set의 전체 예측: {}\".format(best_logreg_model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. k-nearest neighbor classifier\n",
    "Manual for `sklearn.neighbors.KNeighborsClassifier`: [click](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "다음 parameter들에 대해 validation data에 대한 Score값과 AUC값을 이용해 최적 모형 parameter를 찾는다. \n",
    "1. n_neighbors\n",
    "2. weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_set = ['uniform', 'distance']\n",
    "n_neighbors_set = [1, 3, 5, 7, 9, 11, 13, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result2 = []\n",
    "for weights in weights_set:\n",
    "    for n_neighbors in n_neighbors_set:\n",
    "        knn_model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
    "        knn_model = knn_model.fit(train_X_train, train_Y_train)\n",
    "        Y_val_score = knn_model.predict_proba(train_X_val)[:, 1]\n",
    "        val_proba = \"{:.4f}\".format(knn_model.score(train_X_val, train_Y_val))\n",
    "        fpr, tpr, _ = roc_curve(train_Y_val, Y_val_score, pos_label=True)\n",
    "        #result.append(\"모델: {}, 최적 Weight: {}, 최적 N_neighbor: {}, 최적 AUC 값: {}\".format(model, weights, n_neighbors, auc(fpr, tpr)))\n",
    "        #result.append((\"모델\", model, \"최적 Weigth\", weights, \"최적 N_neighbor\", n_neighbors, \"최적 AUC 값\", auc(fpr, tpr)))       \n",
    "        result2.append((knn_model, weights, n_neighbors, val_proba, auc(fpr, tpr)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "             weights='uniform'), 'uniform', 1, '0.2222', 0.48490566037735849),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "             weights='uniform'), 'uniform', 3, '0.1889', 0.46981132075471699),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='uniform'), 'uniform', 5, '0.2148', 0.45283018867924529),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "             weights='uniform'), 'uniform', 7, '0.2556', 0.43018867924528303),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
       "             weights='uniform'), 'uniform', 9, '0.2778', 0.40754716981132078),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=11, p=2,\n",
       "             weights='uniform'), 'uniform', 11, '0.2741', 0.48905660377358495),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=13, p=2,\n",
       "             weights='uniform'), 'uniform', 13, '0.2815', 0.46188679245283021),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n",
       "             weights='uniform'), 'uniform', 15, '0.2889', 0.45169811320754716),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  1,\n",
       "  '0.2222',\n",
       "  0.48490566037735849),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  3,\n",
       "  '0.2111',\n",
       "  0.46981132075471699),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  5,\n",
       "  '0.2259',\n",
       "  0.45283018867924529),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  7,\n",
       "  '0.2556',\n",
       "  0.43018867924528303),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  9,\n",
       "  '0.2778',\n",
       "  0.40754716981132078),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=11, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  11,\n",
       "  '0.2852',\n",
       "  0.4747169811320755),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=13, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  13,\n",
       "  '0.3000',\n",
       "  0.44452830188679249),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  15,\n",
       "  '0.2889',\n",
       "  0.43622641509433963)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_result = sorted(result2, key=lambda x: x[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=13, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  13,\n",
       "  '0.3000',\n",
       "  0.44452830188679249),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n",
       "             weights='uniform'), 'uniform', 15, '0.2889', 0.45169811320754716),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  15,\n",
       "  '0.2889',\n",
       "  0.43622641509433963),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=11, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  11,\n",
       "  '0.2852',\n",
       "  0.4747169811320755),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=13, p=2,\n",
       "             weights='uniform'), 'uniform', 13, '0.2815', 0.46188679245283021),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
       "             weights='uniform'), 'uniform', 9, '0.2778', 0.40754716981132078),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  9,\n",
       "  '0.2778',\n",
       "  0.40754716981132078),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=11, p=2,\n",
       "             weights='uniform'), 'uniform', 11, '0.2741', 0.48905660377358495),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "             weights='uniform'), 'uniform', 7, '0.2556', 0.43018867924528303),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  7,\n",
       "  '0.2556',\n",
       "  0.43018867924528303),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  5,\n",
       "  '0.2259',\n",
       "  0.45283018867924529),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "             weights='uniform'), 'uniform', 1, '0.2222', 0.48490566037735849),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  1,\n",
       "  '0.2222',\n",
       "  0.48490566037735849),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='uniform'), 'uniform', 5, '0.2148', 0.45283018867924529),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "             weights='distance'),\n",
       "  'distance',\n",
       "  3,\n",
       "  '0.2111',\n",
       "  0.46981132075471699),\n",
       " (KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "             weights='uniform'), 'uniform', 3, '0.1889', 0.46981132075471699)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result에 대해 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=13, p=2,\n",
      "           weights='distance'), 'distance', 13, '0.3000', 0.44452830188679249)\n"
     ]
    }
   ],
   "source": [
    "best_knn_result = knn_result[0]\n",
    "print(best_knn_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model의 MAE 값을 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44444444444\n"
     ]
    }
   ],
   "source": [
    "best_knn_model = best_knn_result[0]\n",
    "best_knn_model = best_knn_model.fit(train_X_train, train_Y_train)\n",
    "print(metrics.mean_absolute_error(best_knn_model.predict(train_X_val), train_Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확률:\n",
      "[[ 0.          0.          0.07789386  0.16783795  0.14756425  0.53703356\n",
      "   0.          0.06967039  0.          0.          0.        ]\n",
      " [ 0.07667647  0.          0.          0.16521483  0.13716304  0.39901119\n",
      "   0.          0.          0.06858152  0.07667647  0.07667647]\n",
      " [ 0.          0.          0.          0.31415892  0.2975756   0.38826547\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.06825092  0.          0.          0.41175885  0.07880937  0.36237149\n",
      "   0.          0.07880937  0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.24078923  0.24078923  0.15184215  0.08894707  0.27763231]]\n",
      "합: [ 1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# predict_proba 결과 중 앞부분 6개에 대해서만 확인한다.\n",
    "print(\"예측 확률:\\n{}\".format(best_knn_model.predict_proba(train_X_val)[:6]))\n",
    "\n",
    "# 행 방향으로 확률을 더하면 모두 1이 된다.\n",
    "print(\"합: {}\".format(best_knn_model.predict_proba(train_X_val)[:6].sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_proba의 결과에 argmax 함수를 적용해서 예측을 재연할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 큰 예측 확률의 인덱스:\n",
      "[ 5  5  5  3 10 10  4  5 10  5  3  5  3  7  5  1  5  5  5  8  5  5  5  7  5\n",
      "  8  5  5  5  5  5  4  7  5  5  4  7  5  8  5  3  5  4  5  5  8  4  5  4  5\n",
      "  4  4  5  5  6  8  8  6  8  3  4  5  5  5  6  7  3  5  5  6  5  3  5  4  5\n",
      "  5  7  5  5  4  5  5  5  5  5  5  5  5  8  5  5  4  4  7  5  5  5  7  7  3\n",
      "  5  3  5  5  5  7  4  3  5  5  5  7  4  0  5  4  5  5  4  5  7  8  5  5  5\n",
      "  5  1  3  5  5  5  8  5  6  2  5  5  5  5  8  7  5  5  5  5  5  4  4  5  5\n",
      "  3  8  4  3  4  5  5  5  5 10  7  8  5  5  5  5  5  7  5  5  4  5  9  5  8\n",
      "  9  7 10  4  5  5  3  5  5  7  5  5  5  5  5  5  6  5  5  5  2  5  5  5 10\n",
      "  7  5  5  7  5  7  5  7  5  8  4  7  5  3  3  5  5  7  5  3  5  5  3  4  3\n",
      "  3  5  8  5  5  7  5  3  5  2  3  5  7  6  5  4  5  6  6  5  4  5  5  5  5\n",
      "  5  5  8  5  5  5  9  9  5  4  2  5  5  5  5  5  5  3  3  8]\n",
      "예측:\n",
      "[  5.   5.   5.   3.  10.  10.   4.   5.  10.   5.   3.   5.   3.   7.   5.\n",
      "   1.   5.   5.   5.   8.   5.   5.   5.   7.   5.   8.   5.   5.   5.   5.\n",
      "   5.   4.   7.   5.   5.   4.   7.   5.   8.   5.   3.   5.   4.   5.   5.\n",
      "   8.   4.   5.   4.   5.   4.   4.   5.   5.   6.   8.   8.   6.   8.   3.\n",
      "   4.   5.   5.   5.   6.   7.   3.   5.   5.   6.   5.   3.   5.   4.   5.\n",
      "   5.   7.   5.   5.   4.   5.   5.   5.   5.   5.   5.   5.   5.   8.   5.\n",
      "   5.   4.   4.   7.   5.   5.   5.   7.   7.   3.   5.   3.   5.   5.   5.\n",
      "   7.   4.   3.   5.   5.   5.   7.   4.   0.   5.   4.   5.   5.   4.   5.\n",
      "   7.   8.   5.   5.   5.   5.   1.   3.   5.   5.   5.   8.   5.   6.   2.\n",
      "   5.   5.   5.   5.   8.   7.   5.   5.   5.   5.   5.   4.   4.   5.   5.\n",
      "   3.   8.   4.   3.   4.   5.   5.   5.   5.  10.   7.   8.   5.   5.   5.\n",
      "   5.   5.   7.   5.   5.   4.   5.   9.   5.   8.   9.   7.  10.   4.   5.\n",
      "   5.   3.   5.   5.   7.   5.   5.   5.   5.   5.   5.   6.   5.   5.   5.\n",
      "   2.   5.   5.   5.  10.   7.   5.   5.   7.   5.   7.   5.   7.   5.   8.\n",
      "   4.   7.   5.   3.   3.   5.   5.   7.   5.   3.   5.   5.   3.   4.   3.\n",
      "   3.   5.   8.   5.   5.   7.   5.   3.   5.   2.   3.   5.   7.   6.   5.\n",
      "   4.   5.   6.   6.   5.   4.   5.   5.   5.   5.   5.   5.   8.   5.   5.\n",
      "   5.   9.   9.   5.   4.   2.   5.   5.   5.   5.   5.   5.   3.   3.   8.]\n"
     ]
    }
   ],
   "source": [
    "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(np.argmax(best_knn_model.predict_proba(train_X_val), axis=1)))\n",
    "print(\"예측:\\n{}\".format(best_knn_model.predict(train_X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      "Validation set의 예측: [  5.   5.   5.   3.  10.  10.   4.   5.  10.   5.]\n",
      "실제 Validation set: [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
      "Validation Set의 정확도: 0.30\n",
      "Test set의 예측: [ 8.  8.  5.  8.  5.  8.  6.  5.  5.  9.]\n"
     ]
    }
   ],
   "source": [
    "# KNN 에는 decision function이 없어 predict_proba만 실행한다.\n",
    "print(\"훈련 데이터에 있는 클래스 종류: {}\".format(best_knn_model.classes_))\n",
    "print(\"Validation set의 예측: {}\".format(best_knn_model.predict(train_X_val)[:10]))\n",
    "print(\"실제 Validation set: {}\".format(train_Y_val[:10]))\n",
    "print(\"Validation Set의 정확도: {:.2f}\".format(best_knn_model.score(train_X_val, train_Y_val)))\n",
    "print(\"Test set의 예측: {}\".format(best_knn_model.predict(test_X)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set의 전체 예측: [  8.   8.   5.   8.   5.   8.   6.   5.   5.   9.   5.   7.   9.   9.   4.\n",
      "   5.  10.   5.   7.   9.   2.   7.   7.   5.   7.   5.   3.   4.   5.   5.\n",
      "   5.   5.   3.   5.   5.   3.   5.   5.   5.   5.   5.   5.   6.   5.   3.\n",
      "   3.   6.   8.   7.   8.   6.   8.   8.   3.   7.   5.   5.   4.   7.   5.\n",
      "   3.   3.   5.   5.   3.   5.   5.   3.   5.   5.   7.   3.   5.   5.   5.\n",
      "   5.   5.   5.   4.   3.   5.   5.   4.   3.   5.   5.   4.   3.   3.   5.\n",
      "   5.   5.   5.   5.   5.   3.   5.   5.   3.   5.   7.  10.   5.   7.   5.\n",
      "   5.   5.   5.   3.   6.   5.   5.   5.   3.   7.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   7.   5.   5.   4.   5.   3.   5.   5.   5.  10.   7.\n",
      "   6.   6.   3.   5.   7.   5.   7.   5.   3.   5.   4.   5.   5.   5.   5.\n",
      "   5.   5.   3.   5.   5.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set의 전체 예측: {}\".format(best_knn_model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Naive Bayes classifier\n",
    "Manual for `sklearn.naive_bayes.GaussianNB`: [click](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "클래스에 대한 prior 정보를 조절하여 fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "priors_set = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result3 = []\n",
    "for priors in priors_set:\n",
    "    nb_model = GaussianNB(priors=priors)\n",
    "    nb_model = nb_model.fit(train_X_train, train_Y_train)\n",
    "    Y_val_score = nb_model.predict_proba(train_X_val)[:, 1]\n",
    "    val_proba = \"{:.4f}\".format(nb_model.score(train_X_val, train_Y_val))\n",
    "    fpr, tpr, _ = roc_curve(train_Y_val, Y_val_score, pos_label=True)\n",
    "    result3.append((nb_model, priors, val_proba, auc(fpr, tpr)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_result = sorted(result3, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(GaussianNB(priors=None), None, '0.0407', 0.41509433962264147)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GaussianNB(priors=None), None, '0.0407', 0.41509433962264147)\n"
     ]
    }
   ],
   "source": [
    "best_nb_result = nb_result[0]\n",
    "print(best_nb_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model의 MAE 값을 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.63333333333\n"
     ]
    }
   ],
   "source": [
    "best_nb_model = best_nb_result[0]\n",
    "best_nb_model = best_nb_model.fit(train_X_train, train_Y_train)\n",
    "print(metrics.mean_absolute_error(best_nb_model.predict(train_X_val), train_Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확률:\n",
      "[[  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000\n",
      "    1.00000000e+000   1.01965017e-021   1.10735610e-010   0.00000000e+000\n",
      "    0.00000000e+000   0.00000000e+000   0.00000000e+000]\n",
      " [  6.74125497e-021   1.00000000e+000   2.34265007e-026   1.19001308e-048\n",
      "    4.63787703e-057   4.20133704e-075   5.41084928e-051   2.32878512e-048\n",
      "    1.22315821e-026   0.00000000e+000   8.86517791e-017]\n",
      " [  7.80119602e-012   0.00000000e+000   6.42837030e-024   5.62250955e-039\n",
      "    3.61514108e-060   5.05984467e-072   6.01166964e-048   4.18391334e-036\n",
      "    1.50208970e-021   0.00000000e+000   1.00000000e+000]\n",
      " [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000\n",
      "    4.64192734e-012   5.47937668e-056   1.43183284e-002   9.85681672e-001\n",
      "    0.00000000e+000   0.00000000e+000   0.00000000e+000]\n",
      " [  5.50349893e-015   0.00000000e+000   4.11658468e-027   1.48661662e-046\n",
      "    4.02357104e-066   1.72442672e-073   3.64016074e-052   4.50974671e-037\n",
      "    4.10174579e-019   0.00000000e+000   1.00000000e+000]\n",
      " [  1.10225358e-054   5.19572500e-040   3.91866432e-079   7.37580945e-087\n",
      "    5.50883743e-090   8.04661256e-107   4.17917728e-075   9.69523269e-070\n",
      "    1.07790387e-043   1.00000000e+000   2.45257679e-032]]\n",
      "합: [ 1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# predict_proba 결과 중 앞부분 6개에 대해서만 확인한다.\n",
    "print(\"예측 확률:\\n{}\".format(best_nb_model.predict_proba(train_X_val)[:6]))\n",
    "\n",
    "# 행 방향으로 확률을 더하면 모두 1이 된다.\n",
    "print(\"합: {}\".format(best_nb_model.predict_proba(train_X_val)[:6].sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_proba의 결과에 argmax 함수를 적용해서 예측을 재연할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 큰 예측 확률의 인덱스:\n",
      "[ 4  1 10  7 10  9  0  1 10  1  1  0  7  9  1  1  1  4  1  9  1  1  1  9  9\n",
      "  8  0  1  1  1  1  5  1 10  9  8  9  0  9  9  0  9  1  5 10 10  3  3  1  1\n",
      "  1 10  1 10  0  9  0  9 10  1  5  9  9  1  1  9  0  7  9  9  9  0  1  1  1\n",
      " 10  9  9  1  1  9  1 10  9  1  1  9  0 10 10  1  1  1 10  2  1  9 10  1  0\n",
      "  0  6  9  6  3  9  0  7  7  1  1  7  0  1 10  1  1 10  2 10  9  9  1  9  9\n",
      " 10  1  1  9  1  5  9  9  1 10  9  1  9  9  1  9  1  9  1 10  1  1  7  0  1\n",
      "  1  9  0  1  7  9  0 10  1 10  9  1  1  9  1  3 10  9  1  1  1 10 10  1  9\n",
      "  9 10 10 10  9  7  0  9  9 10  7  1  1  7 10  1  0  0  1  0 10 10  1  1  1\n",
      "  1  9  1 10  2  9  1  9  7  9 10  9  1  1  1 10  9  6  1  0  1 10  1  1  1\n",
      "  3  1  9  1 10  9  7  1  9  0  1 10  9  1  1  0  9  1  9  1 10  6  1 10  1\n",
      "  9  1  9  1  0  1  9  1  1  0  1  9  9  1  1  9  9 10  9  7]\n",
      "예측:\n",
      "[  4.   1.  10.   7.  10.   9.   0.   1.  10.   1.   1.   0.   7.   9.   1.\n",
      "   1.   1.   4.   1.   9.   1.   1.   1.   9.   9.   8.   0.   1.   1.   1.\n",
      "   1.   5.   1.  10.   9.   8.   9.   0.   9.   9.   0.   9.   1.   5.  10.\n",
      "  10.   3.   3.   1.   1.   1.  10.   1.  10.   0.   9.   0.   9.  10.   1.\n",
      "   5.   9.   9.   1.   1.   9.   0.   7.   9.   9.   9.   0.   1.   1.   1.\n",
      "  10.   9.   9.   1.   1.   9.   1.  10.   9.   1.   1.   9.   0.  10.  10.\n",
      "   1.   1.   1.  10.   2.   1.   9.  10.   1.   0.   0.   6.   9.   6.   3.\n",
      "   9.   0.   7.   7.   1.   1.   7.   0.   1.  10.   1.   1.  10.   2.  10.\n",
      "   9.   9.   1.   9.   9.  10.   1.   1.   9.   1.   5.   9.   9.   1.  10.\n",
      "   9.   1.   9.   9.   1.   9.   1.   9.   1.  10.   1.   1.   7.   0.   1.\n",
      "   1.   9.   0.   1.   7.   9.   0.  10.   1.  10.   9.   1.   1.   9.   1.\n",
      "   3.  10.   9.   1.   1.   1.  10.  10.   1.   9.   9.  10.  10.  10.   9.\n",
      "   7.   0.   9.   9.  10.   7.   1.   1.   7.  10.   1.   0.   0.   1.   0.\n",
      "  10.  10.   1.   1.   1.   1.   9.   1.  10.   2.   9.   1.   9.   7.   9.\n",
      "  10.   9.   1.   1.   1.  10.   9.   6.   1.   0.   1.  10.   1.   1.   1.\n",
      "   3.   1.   9.   1.  10.   9.   7.   1.   9.   0.   1.  10.   9.   1.   1.\n",
      "   0.   9.   1.   9.   1.  10.   6.   1.  10.   1.   9.   1.   9.   1.   0.\n",
      "   1.   9.   1.   1.   0.   1.   9.   9.   1.   1.   9.   9.  10.   9.   7.]\n"
     ]
    }
   ],
   "source": [
    "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(np.argmax(best_nb_model.predict_proba(train_X_val), axis=1)))\n",
    "print(\"예측:\\n{}\".format(best_nb_model.predict(train_X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      "Validation set의 예측: [  4.   1.  10.   7.  10.   9.   0.   1.  10.   1.]\n",
      "실제 Validation set: [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
      "Validation Set의 정확도: 0.04\n",
      "Test set의 예측: [  9.  10.   1.   1.   9.   9.   9.   1.   1.   9.]\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB는 Decision Function이 없어 predict_proba만 실행한다.\n",
    "print(\"훈련 데이터에 있는 클래스 종류: {}\".format(best_nb_model.classes_))\n",
    "print(\"Validation set의 예측: {}\".format(best_nb_model.predict(train_X_val)[:10]))\n",
    "print(\"실제 Validation set: {}\".format(train_Y_val[:10]))\n",
    "print(\"Validation Set의 정확도: {:.2f}\".format(best_nb_model.score(train_X_val, train_Y_val)))\n",
    "print(\"Test set의 예측: {}\".format(best_nb_model.predict(test_X)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set의 전체 예측: [  9.  10.   1.   1.   9.   9.   9.   1.   1.   9.   1.  10.   9.   1.   9.\n",
      "   9.   9.   1.  10.   9.   1.   9.   9.   9.   9.   9.   1.   3.   1.   9.\n",
      "   1.   1.   1.   9.  10.   1.   1.   1.   1.   1.   1.   0.   9.  10.   1.\n",
      "   1.   9.  10.   9.  10.   9.  10.   9.   1.   1.  10.  10.   0.   9.   1.\n",
      "   1.   1.   1.   9.   1.   0.   1.  10.   9.   0.   9.   1.   9.   1.   1.\n",
      "   1.   9.   1.   1.   1.  10.  10.   0.   1.   1.   9.   1.   1.   9.   1.\n",
      "   1.   9.   9.   1.   9.   2.   9.   1.   1.   8.   4.  10.  10.   6.  10.\n",
      "  10.  10.  10.  10.  10.  10.  10.   1.   0.   2.   1.   8.   2.   0.   5.\n",
      "  10.  10.  10.   9.  10.   9.  10.  10.   8.  10.   0.   9.   0.  10.   9.\n",
      "   1.   9.   6.   9.   9.   1.   9.   9.   1.   1.   1.   1.   7.   9.   1.\n",
      "   1.   7.   7.   7.   7.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set의 전체 예측: {}\".format(best_nb_model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Decision tree\n",
    "Manual for `sklearn.tree.DecisionTreeClassifier`: [click](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "다음 parameter들에 대해 validation data에 대한 Score값과 AUC값을 이용해 최적 모형 parameter를 찾는다. \n",
    "1. max_depth\n",
    "2. class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight_set = [None, 'balanced']\n",
    "max_depth_set = [3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result4 = []\n",
    "\n",
    "for class_weight in class_weight_set:\n",
    "    for max_depth in max_depth_set:\n",
    "        dt_model = DecisionTreeClassifier(class_weight=class_weight, max_depth=max_depth)\n",
    "        dt_model = dt_model.fit(train_X_train, train_Y_train)\n",
    "        Y_val_score = dt_model.predict_proba(train_X_val)[:, 1]\n",
    "        val_proba = \"{:.4f}\".format(dt_model.score(train_X_val, train_Y_val))\n",
    "        fpr, tpr, _ = roc_curve(train_Y_val, Y_val_score, pos_label=True)\n",
    "        result4.append((dt_model, class_weight, max_depth, val_proba, auc(fpr, tpr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_result = sorted(result4, key=lambda x: x[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), None, 3, '0.2889', 0.82415094339622641),\n",
       " (DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), None, 4, '0.2667', 0.789056603773585),\n",
       " (DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), None, 7, '0.2593', 0.54113207547169817),\n",
       " (DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), None, 6, '0.2481', 0.61018867924528308),\n",
       " (DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), None, 5, '0.2444', 0.7652830188679246),\n",
       " (DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=4,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), 'balanced', 4, '0.2296', 0.72377358490566035),\n",
       " (DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=6,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), 'balanced', 6, '0.1185', 0.34452830188679245),\n",
       " (DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=7,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), 'balanced', 7, '0.1074', 0.41207547169811326),\n",
       " (DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), 'balanced', 3, '0.0815', 0.85056603773584916),\n",
       " (DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), 'balanced', 5, '0.0815', 0.78000000000000003)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result에 대해 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), None, 3, '0.2889', 0.82415094339622641)\n"
     ]
    }
   ],
   "source": [
    "best_dt_result = dt_result[0]\n",
    "print(best_dt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model의 MAE 값을 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.61111111111\n"
     ]
    }
   ],
   "source": [
    "best_dt_model = best_dt_result[0]\n",
    "best_dt_model = best_dt_model.fit(train_X_train, train_Y_train)\n",
    "print(metrics.mean_absolute_error(best_dt_model.predict(train_X_val), train_Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확률:\n",
      "[[ 0.          0.          0.          0.          0.8         0.2         0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.04433498  0.02955665  0.02463054  0.07881773  0.08866995  0.49753695\n",
      "   0.09359606  0.08866995  0.01477833  0.00492611  0.03448276]\n",
      " [ 0.03809524  0.00952381  0.01904762  0.16190476  0.17142857  0.31428571\n",
      "   0.1047619   0.12380952  0.02857143  0.00952381  0.01904762]\n",
      " [ 0.03809524  0.00952381  0.01904762  0.16190476  0.17142857  0.31428571\n",
      "   0.1047619   0.12380952  0.02857143  0.00952381  0.01904762]\n",
      " [ 0.04433498  0.02955665  0.02463054  0.07881773  0.08866995  0.49753695\n",
      "   0.09359606  0.08866995  0.01477833  0.00492611  0.03448276]\n",
      " [ 0.01680672  0.00840336  0.          0.02521008  0.04201681  0.14285714\n",
      "   0.13445378  0.19327731  0.22689076  0.1092437   0.10084034]]\n",
      "합: [ 1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# predict_proba 결과 중 앞부분 6개에 대해서만 확인한다.\n",
    "print(\"예측 확률:\\n{}\".format(best_dt_model.predict_proba(train_X_val)[:6]))\n",
    "\n",
    "# 행 방향으로 확률을 더하면 모두 1이 된다.\n",
    "print(\"합: {}\".format(best_dt_model.predict_proba(train_X_val)[:6].sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_proba의 결과에 argmax 함수를 적용해서 예측을 재연할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 큰 예측 확률의 인덱스:\n",
      "[ 4  5  5  5  5  8  5  3 10  5  3  3  5  8  8  3  3  8  8  8  5  5  5  5  5\n",
      "  8  5  5  3  5  3  5  5  5  5  3  5  3  8  5  5  5  3  5  5 10  5  5  5  3\n",
      "  5  3  5  5  8  8  3  5  8  5  4  5  5  5  5  5  5  8  5  8  8  3  5  3  3\n",
      "  5  5  8  3  5  5  5  5  5  5  5  5  3 10  5  5  3  3  8  3  5  8  8  5  3\n",
      "  3  5  5  5  5  5  3  3  5  5  5  8  3  3  5  3 10  5  5  8  8  8  5  5  5\n",
      "  5  3  3  5  5  3  8  5  5  3  5  5  5  5  8  8  3  5  5  8  3  5  3  3  5\n",
      "  5  8  3  3  5  5  5  5  5 10  5  8  3  5  8  3  5  5  5  5  5  5  8  5  8\n",
      "  8  8  8  3  5  3  3  5  5 10  5  5  5  5  5  3  8  5 10  5  3  5  3  3  8\n",
      "  5  5  5  8  3  8  3  8  3  8  3  5  5  3  3  3  5 10  3  5  5  5  3  8  3\n",
      "  3  3  8  5  5  5  5  3  5  3  8  5  5  8  5  5  8  8  8  3  3  4  5 10 10\n",
      "  5  5  8  5  5  5  8  8  5  3  3  5  5  3 10  5  5  5  5  8]\n",
      "예측:\n",
      "[  4.   5.   5.   5.   5.   8.   5.   3.  10.   5.   3.   3.   5.   8.   8.\n",
      "   3.   3.   8.   8.   8.   5.   5.   5.   5.   5.   8.   5.   5.   3.   5.\n",
      "   3.   5.   5.   5.   5.   3.   5.   3.   8.   5.   5.   5.   3.   5.   5.\n",
      "  10.   5.   5.   5.   3.   5.   3.   5.   5.   8.   8.   3.   5.   8.   5.\n",
      "   4.   5.   5.   5.   5.   5.   5.   8.   5.   8.   8.   3.   5.   3.   3.\n",
      "   5.   5.   8.   3.   5.   5.   5.   5.   5.   5.   5.   5.   3.  10.   5.\n",
      "   5.   3.   3.   8.   3.   5.   8.   8.   5.   3.   3.   5.   5.   5.   5.\n",
      "   5.   3.   3.   5.   5.   5.   8.   3.   3.   5.   3.  10.   5.   5.   8.\n",
      "   8.   8.   5.   5.   5.   5.   3.   3.   5.   5.   3.   8.   5.   5.   3.\n",
      "   5.   5.   5.   5.   8.   8.   3.   5.   5.   8.   3.   5.   3.   3.   5.\n",
      "   5.   8.   3.   3.   5.   5.   5.   5.   5.  10.   5.   8.   3.   5.   8.\n",
      "   3.   5.   5.   5.   5.   5.   5.   8.   5.   8.   8.   8.   8.   3.   5.\n",
      "   3.   3.   5.   5.  10.   5.   5.   5.   5.   5.   3.   8.   5.  10.   5.\n",
      "   3.   5.   3.   3.   8.   5.   5.   5.   8.   3.   8.   3.   8.   3.   8.\n",
      "   3.   5.   5.   3.   3.   3.   5.  10.   3.   5.   5.   5.   3.   8.   3.\n",
      "   3.   3.   8.   5.   5.   5.   5.   3.   5.   3.   8.   5.   5.   8.   5.\n",
      "   5.   8.   8.   8.   3.   3.   4.   5.  10.  10.   5.   5.   8.   5.   5.\n",
      "   5.   8.   8.   5.   3.   3.   5.   5.   3.  10.   5.   5.   5.   5.   8.]\n"
     ]
    }
   ],
   "source": [
    "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(np.argmax(best_dt_model.predict_proba(train_X_val), axis=1)))\n",
    "print(\"예측:\\n{}\".format(best_dt_model.predict(train_X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      "Validation set의 예측: [  4.   5.   5.   5.   5.   8.   5.   3.  10.   5.]\n",
      "실제 Validation set: [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
      "Validation Set의 정확도: 0.29\n",
      "Test set의 예측: [  8.  10.   3.   8.   5.   8.   8.   3.   5.   8.]\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier은 Decision Function이 없어 predict_proba만 실행한다. \n",
    "print(\"훈련 데이터에 있는 클래스 종류: {}\".format(best_dt_model.classes_))\n",
    "print(\"Validation set의 예측: {}\".format(best_dt_model.predict(train_X_val)[:10]))\n",
    "print(\"실제 Validation set: {}\".format(train_Y_val[:10]))\n",
    "print(\"Validation Set의 정확도: {:.2f}\".format(best_dt_model.score(train_X_val, train_Y_val)))\n",
    "print(\"Test set의 예측: {}\".format(best_dt_model.predict(test_X)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set의 전체 예측: [  8.  10.   3.   8.   5.   8.   8.   3.   5.   8.   5.   3.   8.   8.   5.\n",
      "   8.   8.   3.  10.   8.   3.   5.   8.   5.   5.   5.   3.   3.   3.   5.\n",
      "   5.  10.   3.   5.   5.   3.   5.   3.   5.   5.   5.   5.   8.   5.   3.\n",
      "   5.   8.   8.   5.  10.   8.  10.   8.   3.   8.   5.   5.   3.   8.   5.\n",
      "   3.   3.   5.   5.   3.   5.   5.   3.   5.   5.   5.   3.   5.   3.   3.\n",
      "   3.   5.   5.   3.   3.   5.   3.   3.   5.   5.   5.   3.   3.   5.   3.\n",
      "   5.   8.   5.   8.   5.   4.   5.   5.  10.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   3.  10.   5.  10.   3.   3.   5.   5.   5.   5.   3.   4.\n",
      "   5.   5.   5.   5.   5.   5.   5.   3.   5.   3.   5.   5.   5.  10.   8.\n",
      "   8.   5.   3.   5.   8.   3.   8.   5.   3.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   3.   5.   5.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set의 전체 예측: {}\".format(best_dt_model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Random forest\n",
    "Manual for `sklearn.ensemble.RandomForestClassifier`: [click](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "다음 parameter들에 대해 validation data에 대한 Score값과 AUC값을 이용해 최적 모형 parameter를 찾는다. \n",
    "1. n_estimators\n",
    "2. max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators_set = [40, 60, 90, 100, 1000, 2000, 5000]\n",
    "max_depth_set = [3, 4, 5, 6, 7]\n",
    "max_features_set = ['auto', 'sqrt', 'log2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result5 = []\n",
    "for n_estimators in n_estimators_set:\n",
    "    for max_features in max_features_set:\n",
    "        for max_depth in max_depth_set:\n",
    "            rf_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, random_state=0)\n",
    "            rf_model = rf_model.fit(train_X_train, train_Y_train)\n",
    "            Y_val_score = rf_model.predict_proba(train_X_val)[:, 1]\n",
    "            val_proba = \"{:.4f}\".format(rf_model.score(train_X_val, train_Y_val))\n",
    "            fpr, tpr, _ = roc_curve(train_Y_val, Y_val_score, pos_label=True)\n",
    "            result5.append((rf_model, n_estimators, max_features, max_depth, val_proba, auc(fpr, tpr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_result = sorted(result5, key=lambda x: x[4], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'auto',\n",
       "  3,\n",
       "  '0.3481',\n",
       "  0.81433962264150939),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'sqrt',\n",
       "  3,\n",
       "  '0.3481',\n",
       "  0.81433962264150939),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'log2',\n",
       "  4,\n",
       "  '0.3481',\n",
       "  0.83849056603773586),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'log2',\n",
       "  4,\n",
       "  '0.3481',\n",
       "  0.82339622641509436),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'log2',\n",
       "  5,\n",
       "  '0.3444',\n",
       "  0.75547169811320758),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'auto',\n",
       "  3,\n",
       "  '0.3407',\n",
       "  0.80301886792452837),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'sqrt',\n",
       "  3,\n",
       "  '0.3407',\n",
       "  0.80301886792452837),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'log2',\n",
       "  4,\n",
       "  '0.3407',\n",
       "  0.81283018867924539),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'auto',\n",
       "  3,\n",
       "  '0.3407',\n",
       "  0.82339622641509436),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'sqrt',\n",
       "  3,\n",
       "  '0.3407',\n",
       "  0.82339622641509436),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'auto',\n",
       "  3,\n",
       "  '0.3407',\n",
       "  0.81962264150943398),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'sqrt',\n",
       "  3,\n",
       "  '0.3407',\n",
       "  0.81962264150943398),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'log2',\n",
       "  4,\n",
       "  '0.3370',\n",
       "  0.85358490566037726),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'auto',\n",
       "  3,\n",
       "  '0.3370',\n",
       "  0.80528301886792453),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'sqrt',\n",
       "  3,\n",
       "  '0.3370',\n",
       "  0.80528301886792453),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'log2',\n",
       "  4,\n",
       "  '0.3370',\n",
       "  0.77735849056603767),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'auto',\n",
       "  3,\n",
       "  '0.3370',\n",
       "  0.82566037735849063),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'sqrt',\n",
       "  3,\n",
       "  '0.3370',\n",
       "  0.82566037735849063),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'log2',\n",
       "  4,\n",
       "  '0.3370',\n",
       "  0.77886792452830189),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'auto',\n",
       "  4,\n",
       "  '0.3370',\n",
       "  0.77584905660377357),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'sqrt',\n",
       "  4,\n",
       "  '0.3370',\n",
       "  0.77584905660377357),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'log2',\n",
       "  4,\n",
       "  '0.3370',\n",
       "  0.77056603773584909),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'log2',\n",
       "  5,\n",
       "  '0.3333',\n",
       "  0.78113207547169816),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'log2',\n",
       "  3,\n",
       "  '0.3333',\n",
       "  0.8143396226415095),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'log2',\n",
       "  5,\n",
       "  '0.3333',\n",
       "  0.7449056603773585),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'auto',\n",
       "  3,\n",
       "  '0.3333',\n",
       "  0.80679245283018874),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'auto',\n",
       "  4,\n",
       "  '0.3333',\n",
       "  0.75018867924528299),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'auto',\n",
       "  5,\n",
       "  '0.3333',\n",
       "  0.75924528301886796),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'sqrt',\n",
       "  3,\n",
       "  '0.3333',\n",
       "  0.80679245283018874),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'sqrt',\n",
       "  4,\n",
       "  '0.3333',\n",
       "  0.75018867924528299),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'sqrt',\n",
       "  5,\n",
       "  '0.3333',\n",
       "  0.75924528301886796),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'log2',\n",
       "  5,\n",
       "  '0.3333',\n",
       "  0.73207547169811327),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'auto',\n",
       "  4,\n",
       "  '0.3333',\n",
       "  0.76754716981132076),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'sqrt',\n",
       "  4,\n",
       "  '0.3333',\n",
       "  0.76754716981132076),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'log2',\n",
       "  5,\n",
       "  '0.3333',\n",
       "  0.7313207547169811),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'auto',\n",
       "  4,\n",
       "  '0.3296',\n",
       "  0.7871698113207547),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'sqrt',\n",
       "  4,\n",
       "  '0.3296',\n",
       "  0.7871698113207547),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'log2',\n",
       "  3,\n",
       "  '0.3296',\n",
       "  0.8260377358490566),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'log2',\n",
       "  7,\n",
       "  '0.3296',\n",
       "  0.63471698113207542),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'log2',\n",
       "  3,\n",
       "  '0.3296',\n",
       "  0.80150943396226415),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'auto',\n",
       "  4,\n",
       "  '0.3296',\n",
       "  0.74641509433962261),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'sqrt',\n",
       "  4,\n",
       "  '0.3296',\n",
       "  0.74641509433962261),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'log2',\n",
       "  3,\n",
       "  '0.3296',\n",
       "  0.8075471698113208),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'log2',\n",
       "  5,\n",
       "  '0.3296',\n",
       "  0.73358490566037737),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'auto',\n",
       "  4,\n",
       "  '0.3296',\n",
       "  0.7735849056603773),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'sqrt',\n",
       "  4,\n",
       "  '0.3296',\n",
       "  0.7735849056603773),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'log2',\n",
       "  3,\n",
       "  '0.3296',\n",
       "  0.81811320754716987),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'log2',\n",
       "  5,\n",
       "  '0.3296',\n",
       "  0.73283018867924532),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'auto',\n",
       "  5,\n",
       "  '0.3259',\n",
       "  0.7939622641509434),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'sqrt',\n",
       "  5,\n",
       "  '0.3259',\n",
       "  0.7939622641509434),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'auto',\n",
       "  4,\n",
       "  '0.3259',\n",
       "  0.77509433962264151),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=4, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'sqrt',\n",
       "  4,\n",
       "  '0.3259',\n",
       "  0.77509433962264151),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'log2',\n",
       "  6,\n",
       "  '0.3259',\n",
       "  0.72905660377358494),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'log2',\n",
       "  7,\n",
       "  '0.3259',\n",
       "  0.63471698113207542),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'auto',\n",
       "  5,\n",
       "  '0.3259',\n",
       "  0.75622641509433963),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'auto',\n",
       "  6,\n",
       "  '0.3259',\n",
       "  0.66339622641509433),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'sqrt',\n",
       "  5,\n",
       "  '0.3259',\n",
       "  0.75622641509433963),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'sqrt',\n",
       "  6,\n",
       "  '0.3259',\n",
       "  0.66339622641509433),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'log2',\n",
       "  3,\n",
       "  '0.3259',\n",
       "  0.8158490566037736),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=3, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'log2',\n",
       "  3,\n",
       "  '0.3259',\n",
       "  0.82037735849056603),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'auto',\n",
       "  6,\n",
       "  '0.3222',\n",
       "  0.67471698113207546),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'auto',\n",
       "  7,\n",
       "  '0.3222',\n",
       "  0.65962264150943395),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'sqrt',\n",
       "  6,\n",
       "  '0.3222',\n",
       "  0.67471698113207546),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'sqrt',\n",
       "  7,\n",
       "  '0.3222',\n",
       "  0.65962264150943395),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'auto',\n",
       "  6,\n",
       "  '0.3222',\n",
       "  0.66339622641509433),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'sqrt',\n",
       "  6,\n",
       "  '0.3222',\n",
       "  0.66339622641509433),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'auto',\n",
       "  6,\n",
       "  '0.3222',\n",
       "  0.6807547169811321),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'auto',\n",
       "  7,\n",
       "  '0.3222',\n",
       "  0.60452830188679241),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'sqrt',\n",
       "  6,\n",
       "  '0.3222',\n",
       "  0.6807547169811321),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'sqrt',\n",
       "  7,\n",
       "  '0.3222',\n",
       "  0.60452830188679241),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'auto',\n",
       "  5,\n",
       "  '0.3222',\n",
       "  0.72754716981132073),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'sqrt',\n",
       "  5,\n",
       "  '0.3222',\n",
       "  0.72754716981132073),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'log2',\n",
       "  6,\n",
       "  '0.3222',\n",
       "  0.67547169811320762),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'auto',\n",
       "  5,\n",
       "  '0.3222',\n",
       "  0.72754716981132073),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'sqrt',\n",
       "  5,\n",
       "  '0.3222',\n",
       "  0.72754716981132073),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'log2',\n",
       "  6,\n",
       "  '0.3222',\n",
       "  0.67698113207547173),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'auto',\n",
       "  5,\n",
       "  '0.3222',\n",
       "  0.72830188679245289),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'sqrt',\n",
       "  5,\n",
       "  '0.3222',\n",
       "  0.72830188679245289),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'log2',\n",
       "  6,\n",
       "  '0.3222',\n",
       "  0.67773584905660378),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'auto',\n",
       "  5,\n",
       "  '0.3185',\n",
       "  0.78188679245283033),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'sqrt',\n",
       "  5,\n",
       "  '0.3185',\n",
       "  0.78188679245283033),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'auto',\n",
       "  7,\n",
       "  '0.3185',\n",
       "  0.60679245283018868),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'sqrt',\n",
       "  7,\n",
       "  '0.3185',\n",
       "  0.60679245283018868),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  40,\n",
       "  'log2',\n",
       "  6,\n",
       "  '0.3148',\n",
       "  0.76150943396226412),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'log2',\n",
       "  6,\n",
       "  '0.3148',\n",
       "  0.74415094339622645),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'auto',\n",
       "  6,\n",
       "  '0.3148',\n",
       "  0.66415094339622649),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'sqrt',\n",
       "  6,\n",
       "  '0.3148',\n",
       "  0.66415094339622649),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'auto',\n",
       "  6,\n",
       "  '0.3148',\n",
       "  0.66943396226415097),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'sqrt',\n",
       "  6,\n",
       "  '0.3148',\n",
       "  0.66943396226415097),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'auto',\n",
       "  7,\n",
       "  '0.3111',\n",
       "  0.62641509433962261),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  60,\n",
       "  'sqrt',\n",
       "  7,\n",
       "  '0.3111',\n",
       "  0.62641509433962261),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'log2',\n",
       "  6,\n",
       "  '0.3111',\n",
       "  0.74867924528301888),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=90, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  90,\n",
       "  'log2',\n",
       "  7,\n",
       "  '0.3111',\n",
       "  0.62867924528301888),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'auto',\n",
       "  7,\n",
       "  '0.3111',\n",
       "  0.62037735849056608),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'sqrt',\n",
       "  7,\n",
       "  '0.3111',\n",
       "  0.62037735849056608),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'auto',\n",
       "  6,\n",
       "  '0.3111',\n",
       "  0.6724528301886793),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'auto',\n",
       "  7,\n",
       "  '0.3111',\n",
       "  0.60830188679245289),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=6, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'sqrt',\n",
       "  6,\n",
       "  '0.3111',\n",
       "  0.6724528301886793),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'sqrt',\n",
       "  7,\n",
       "  '0.3111',\n",
       "  0.60830188679245289),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'auto',\n",
       "  7,\n",
       "  '0.3111',\n",
       "  0.60754716981132084),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'sqrt',\n",
       "  7,\n",
       "  '0.3111',\n",
       "  0.60754716981132084),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  5000,\n",
       "  'log2',\n",
       "  7,\n",
       "  '0.3074',\n",
       "  0.62113207547169813),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  100,\n",
       "  'log2',\n",
       "  7,\n",
       "  '0.3037',\n",
       "  0.61056603773584905),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  1000,\n",
       "  'log2',\n",
       "  7,\n",
       "  '0.3037',\n",
       "  0.60075471698113214),\n",
       " (RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=7, max_features='log2', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "              oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  2000,\n",
       "  'log2',\n",
       "  7,\n",
       "  '0.3037',\n",
       "  0.62113207547169813)]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result에 대해 보여준다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False), 40, 'auto', 3, '0.3481', 0.81433962264150939)\n"
     ]
    }
   ],
   "source": [
    "best_rf_result = rf_result[0]\n",
    "print(best_rf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model의 MAE 값을 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.47777777778\n"
     ]
    }
   ],
   "source": [
    "best_rf_model = best_rf_result[0]\n",
    "best_rf_model = best_rf_model.fit(train_X_train, train_Y_train)\n",
    "print(metrics.mean_absolute_error(best_rf_model.predict(train_X_val), train_Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확률:\n",
      "[[ 0.037794    0.04134552  0.0810511   0.15799053  0.12667703  0.33696219\n",
      "   0.07172005  0.06360817  0.02906457  0.00861552  0.04517131]\n",
      " [ 0.0317589   0.02746014  0.04240984  0.13615567  0.12928781  0.34644488\n",
      "   0.08536066  0.08555521  0.0400309   0.01227162  0.06326436]\n",
      " [ 0.0299223   0.0180217   0.03360752  0.17642014  0.14916486  0.28758057\n",
      "   0.09755792  0.08362591  0.0404012   0.01998417  0.06371372]\n",
      " [ 0.04156436  0.02741359  0.05705389  0.14171847  0.13672469  0.34018336\n",
      "   0.08785312  0.07537955  0.04012328  0.01363228  0.0383534 ]\n",
      " [ 0.02729711  0.01906506  0.02240029  0.0864641   0.09085905  0.33668817\n",
      "   0.09152521  0.12495486  0.07482323  0.02754979  0.09837313]\n",
      " [ 0.01261111  0.00719262  0.00925334  0.04176916  0.05503545  0.1780624\n",
      "   0.13145728  0.16474736  0.1857275   0.06627385  0.14786994]]\n",
      "합: [ 1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# predict_proba 결과 중 앞부분 6개에 대해서만 확인한다.\n",
    "print(\"예측 확률:\\n{}\".format(best_rf_model.predict_proba(train_X_val)[:6]))\n",
    "\n",
    "# 행 방향으로 확률을 더하면 모두 1이 된다.\n",
    "print(\"합: {}\".format(best_rf_model.predict_proba(train_X_val)[:6].sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_proba의 결과에 argmax 함수를 적용해서 예측을 재연할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 큰 예측 확률의 인덱스:\n",
      "[5 5 5 5 5 8 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 4 5 5 5 5 5 5 5 5 3 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 8 5 5 5 3\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 8 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3\n",
      " 5 5 5 5 3 5 5 5 5 5 5 5 5 8 5 5 5 5 5 5 5 5 5 5 5 5 5 5 7 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 5 5 5 4 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5]\n",
      "예측:\n",
      "[ 5.  5.  5.  5.  5.  8.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  4.  5.  5.  5.  5.  5.  5.  5.  5.  3.  5.  5.\n",
      "  5.  5.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  8.  5.  5.\n",
      "  5.  3.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  4.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  8.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  3.  5.  5.  5.  5.  3.  5.  5.  5.  5.  5.  5.  5.  5.  8.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  7.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  3.  5.  5.  5.  4.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  3.  3.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.]\n"
     ]
    }
   ],
   "source": [
    "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(np.argmax(best_rf_model.predict_proba(train_X_val), axis=1)))\n",
    "print(\"예측:\\n{}\".format(best_rf_model.predict(train_X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      "Validation set의 예측: [ 5.  5.  5.  5.  5.  8.  5.  5.  5.  5.]\n",
      "실제 Validation set: [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
      "Validation Set의 정확도: 0.34\n",
      "Test set의 예측: [  8.  10.   1.   8.   7.  10.   8.   0.   1.   9.]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier은 Decision Function이 없어 predict_proba만 실행한다. \n",
    "print(\"훈련 데이터에 있는 클래스 종류: {}\".format(best_rf_model.classes_))\n",
    "print(\"Validation set의 예측: {}\".format(best_rf_model.predict(train_X_val)[:10]))\n",
    "print(\"실제 Validation set: {}\".format(train_Y_val[:10]))\n",
    "print(\"Validation Set의 정확도: {:.2f}\".format(best_rf_model.score(train_X_val, train_Y_val)))\n",
    "print(\"Test set의 예측: {}\".format(best_logreg_model.predict(test_X)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set의 전체 예측: [  8.  10.   1.   8.   7.  10.   8.   0.   1.   9.   1.   4.   9.   8.   0.\n",
      "  10.  10.  10.  10.   9.   3.   7.   8.   7.   7.   0.   2.   3.   2.  10.\n",
      "   5.  10.   0.   2.   0.   3.   3.   3.   6.   6.   2.   6.  10.   5.   2.\n",
      "   0.   8.  10.   7.   8.   8.  10.   9.   1.   8.   0.   5.   4.   9.   7.\n",
      "   2.   2.   7.   7.   2.   2.   0.   0.   5.   0.   1.   1.   9.   1.   1.\n",
      "   2.   0.  10.   1.   3.   6.   0.   4.   8.   5.   5.   3.   1.   3.   4.\n",
      "   0.   0.   5.  10.   6.   3.   5.   5.   4.   5.   4.   3.  10.   6.   5.\n",
      "   5.   3.  10.  10.  10.   2.  10.  10.   3.   7.   1.   3.   7.   6.   5.\n",
      "   6.   6.   4.   8.   7.   5.   8.   3.   3.   2.   0.   5.   0.  10.   6.\n",
      "   6.   6.   6.   5.   7.   0.   7.   1.   3.   0.   0.   5.   7.   5.   1.\n",
      "   0.   7.   5.   5.   7.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set의 전체 예측: {}\".format(best_logreg_model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. SVM\n",
    "\n",
    "Manual for `Support Vector Machines`: [click](http://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    "다음 parameter들에 대해 validation data에 대한 Score값과 AUC값을 이용해 최적 모형 parameter를 찾는다. \n",
    "1. gamma\n",
    "2. C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma_set = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "c_set = [0.001, 0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result6 = []\n",
    "for gamma in gamma_set:\n",
    "    for C in c_set:\n",
    "        svm_model = SVC(decision_function_shape='ovo', gamma=gamma, C=C, probability = True, max_iter=10000)\n",
    "        svm_model = svm_model.fit(train_X_train, train_Y_train)\n",
    "        Y_val_score = svm_model.predict_proba(train_X_val)[:, 1]\n",
    "        val_proba = \"{:.4f}\".format(svm_model.score(train_X_val, train_Y_val))\n",
    "        fpr, tpr, _ = roc_curve(train_Y_val, Y_val_score, pos_label=True)\n",
    "        result6.append((svm_model, gamma, C,val_proba, auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_result = sorted(result6, key=lambda x: x[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.1, 1, '0.3370', 0.67320754716981135),\n",
       " (SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 1, 1, '0.3296', 0.75471698113207553),\n",
       " (SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 1, 10, '0.3296', 0.6807547169811321),\n",
       " (SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 1, 100, '0.3296', 0.64226415094339617),\n",
       " (SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=10, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 10, 1, '0.3296', 0.71471698113207549),\n",
       " (SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=10, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 10, 10, '0.3296', 0.7011320754716982),\n",
       " (SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=10, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 10, 100, '0.3296', 0.74188679245283018),\n",
       " (SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=100, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 100, 1, '0.3296', 0.68679245283018864),\n",
       " (SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=100, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 100, 10, '0.3296', 0.68830188679245285),\n",
       " (SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=100, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 100, 100, '0.3296', 0.68830188679245285),\n",
       " (SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.001, 0.001, '0.3259', 0.77207547169811319),\n",
       " (SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.001, 0.01, '0.3259', 0.74641509433962261),\n",
       " (SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.001, 0.1, '0.3259', 0.77735849056603779),\n",
       " (SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.001, 1, '0.3259', 0.82188679245283025),\n",
       " (SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.001, 10, '0.3259', 0.79471698113207556),\n",
       " (SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.01, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.01, 0.001, '0.3259', 0.77811320754716973),\n",
       " (SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.01, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.01, 0.01, '0.3259', 0.76452830188679255),\n",
       " (SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.01, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.01, 0.1, '0.3259', 0.77283018867924524),\n",
       " (SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.01, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.01, 1, '0.3259', 0.77283018867924524),\n",
       " (SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.1, 0.001, '0.3259', 0.64000000000000001),\n",
       " (SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.1, 0.01, '0.3259', 0.6807547169811321),\n",
       " (SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.1, 0.1, '0.3259', 0.67094339622641508),\n",
       " (SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 1, 0.001, '0.3259', 0.69735849056603771),\n",
       " (SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 1, 0.01, '0.3259', 0.60075471698113203),\n",
       " (SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 1, 0.1, '0.3259', 0.57358490566037734),\n",
       " (SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=10, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 10, 0.001, '0.3259', 0.58264150943396231),\n",
       " (SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=10, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 10, 0.01, '0.3259', 0.7181132075471699),\n",
       " (SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=10, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 10, 0.1, '0.3259', 0.59094339622641523),\n",
       " (SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=100, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 100, 0.001, '0.3259', 0.68830188679245285),\n",
       " (SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=100, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 100, 0.01, '0.3259', 0.68905660377358491),\n",
       " (SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=100, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 100, 0.1, '0.3259', 0.69056603773584901),\n",
       " (SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.01, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.01, 10, '0.3148', 0.75698113207547169),\n",
       " (SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.001, 100, '0.3074', 0.78792452830188675),\n",
       " (SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.01, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.01, 100, '0.2963', 0.73433962264150943),\n",
       " (SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.1, 10, '0.2741', 0.57811320754716977),\n",
       " (SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=0.1, kernel='rbf',\n",
       "    max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.1, 100, '0.2630', 0.6724528301886793)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result에 대해 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovo', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=10000, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), 0.1, 1, '0.3370', 0.67320754716981135)\n"
     ]
    }
   ],
   "source": [
    "best_svm_result = svm_result[0]\n",
    "print(best_svm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model의 MAE 값을 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41481481481\n"
     ]
    }
   ],
   "source": [
    "best_svm_model = best_svm_result[0]\n",
    "best_svm_model = best_svm_model.fit(train_X_train, train_Y_train)\n",
    "print(metrics.mean_absolute_error(best_svm_model.predict(train_X_val), train_Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확률:\n",
      "[[ 0.03209334  0.03704663  0.07595958  0.1672928   0.12689136  0.37956085\n",
      "   0.06020029  0.05359231  0.02573618  0.00974114  0.03188551]\n",
      " [ 0.03236424  0.02113014  0.03650773  0.1447285   0.12281208  0.4516385\n",
      "   0.0553937   0.05106364  0.02763964  0.01017355  0.04654828]\n",
      " [ 0.03337782  0.01505569  0.03225198  0.17332638  0.15115217  0.35532731\n",
      "   0.07319967  0.06985921  0.03595549  0.00993884  0.05055545]\n",
      " [ 0.03269102  0.02321026  0.05559016  0.15125295  0.14084524  0.39428478\n",
      "   0.06125568  0.08062878  0.02552933  0.00670129  0.02801052]\n",
      " [ 0.02737244  0.01370363  0.02056459  0.07529734  0.08683564  0.26291957\n",
      "   0.09831925  0.16135768  0.08147286  0.02749569  0.1446613 ]\n",
      " [ 0.01826279  0.00902472  0.0099281   0.01610559  0.02908623  0.03503992\n",
      "   0.17253747  0.2001462   0.2054169   0.06558695  0.23886513]]\n",
      "합: [ 1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# predict_proba 결과 중 앞부분 6개에 대해서만 확인한다.\n",
    "print(\"예측 확률:\\n{}\".format(best_svm_model.predict_proba(train_X_val)[:6]))\n",
    "\n",
    "# 행 방향으로 확률을 더하면 모두 1이 된다.\n",
    "print(\"합: {}\".format(best_svm_model.predict_proba(train_X_val)[:6].sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_proba의 결과에 argmax 함수를 적용해서 예측을 재연할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 큰 예측 확률의 인덱스:\n",
      "[ 5  5  5  5  5 10  5  5  5  5  5  5  5  7  5  3  5  5  5  8  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  3  5  5  8  5  5  5  3  5  5  5  5  5  5  5\n",
      "  3  3  5  5  5  7  4  7  8  5  5  5  5  5  5  5  5  5  5  8  7  5  5  3  5\n",
      "  5  7  5  5  5  5  5  5  5  5  5  5  5  5  5  5  3  5  5  5  5  5  5  5  3\n",
      "  5  5  5  5  5  5  5  3  5  5  5  8  3  3  5  3  5  5  5  5  7  8  5  5  5\n",
      "  5  5  3  5  5  5  8  5  5  3  5  5  5  5  7  7  5  5  5  5  5  5  3  5  5\n",
      "  5  7  3  3  5  5  5  5  5 10  5  7  5  5  5  5  5  7  5  5  5  5 10  5  8\n",
      "  7  7 10  3  5  5  5  5  5  5  5  5  5  5  5  5  6  5  5  5  3  5  5  5  7\n",
      "  5  5  5  5  5  5  5  7  5  5  3  7  5  3  3  5  5  7  3  5  5  5  5  5  3\n",
      "  5  5  8  5  5  5  5  3  5  3  3  5  7  7  5  5  5  5  8  5  5  5  5  5  5\n",
      "  5  5 10  5  5  5  7  6  5  3  5  5  5  5  5  5  5  3  5  8]\n",
      "예측:\n",
      "[  5.   5.   5.   5.   5.   8.   5.   5.   5.   5.   5.   5.   5.   8.   5.\n",
      "   3.   5.   5.   5.   8.   5.   5.   5.   5.   5.   8.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   4.   7.   5.   8.   5.   3.   5.   4.   5.   5.\n",
      "   5.   5.   5.   5.   5.   3.   4.   5.   5.   5.   7.   4.   7.   8.   5.\n",
      "   5.   5.   5.   5.   5.   7.   5.   5.   5.   8.   6.   5.   5.   3.   5.\n",
      "   5.   7.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   3.   5.   7.   5.   5.   5.   5.   5.   3.   5.   5.   5.   5.   5.\n",
      "   7.   5.   3.   5.   5.   5.   8.   4.   4.   5.   3.   5.   5.   5.   5.\n",
      "   7.   8.   5.   5.   5.   5.   5.   3.   5.   5.   5.   8.   5.   5.   3.\n",
      "   5.   5.   5.   5.   8.   7.   5.   5.   5.   5.   5.   4.   3.   5.   5.\n",
      "   5.   8.   3.   3.   5.   5.   5.   5.   5.  10.   5.   7.   5.   5.   5.\n",
      "   5.   5.   7.   5.   5.   5.   5.   7.   5.   8.   8.   7.  10.   3.   5.\n",
      "   5.   5.   5.   5.  10.   5.   5.   5.   5.   5.   5.   6.   5.   5.   5.\n",
      "   3.   5.   5.   5.   4.   5.   5.   5.   7.   5.   5.   5.   8.   5.   5.\n",
      "   3.   7.   5.   3.   3.   5.   5.   5.   3.   5.   5.   5.   5.   5.   3.\n",
      "   5.   5.   8.   5.   5.   5.   5.   3.   5.   3.   3.   5.   7.   7.   5.\n",
      "   5.   5.   5.   8.   5.   5.   5.   5.   5.   5.   5.   5.   8.   5.   5.\n",
      "   5.   6.   6.   5.   3.   5.   5.   5.   5.   5.   5.   5.   3.   5.   8.]\n"
     ]
    }
   ],
   "source": [
    "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(np.argmax(best_svm_model.predict_proba(train_X_val), axis=1)))\n",
    "print(\"예측:\\n{}\".format(best_svm_model.predict(train_X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      "가장 큰 결정 함수의 인덱스: [31 40 38 43 50 52 43 43 41 43]\n",
      "Validation set의 예측: [ 5.  5.  5.  5.  5.  8.  5.  5.  5.  5.]\n",
      "실제 Validation set: [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
      "Validation Set의 정확도: 0.34\n",
      "Test set의 예측: [  8.  10.   1.   8.   7.  10.   8.   0.   1.   9.]\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 데이터에 있는 클래스 종류: {}\".format(best_svm_model.classes_))\n",
    "argmax_dec_func = np.argmax(best_svm_model.decision_function(train_X_val), axis=1)\n",
    "print(\"가장 큰 결정 함수의 인덱스: {}\".format(argmax_dec_func[:10]))\n",
    "print(\"Validation set의 예측: {}\".format(best_svm_model.predict(train_X_val)[:10]))\n",
    "print(\"실제 Validation set: {}\".format(train_Y_val[:10]))\n",
    "print(\"Validation Set의 정확도: {:.2f}\".format(best_svm_model.score(train_X_val, train_Y_val)))\n",
    "print(\"Test set의 예측: {}\".format(best_logreg_model.predict(test_X)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set의 예측: [  8.  10.   1.   8.   7.  10.   8.   0.   1.   9.   1.   4.   9.   8.   0.\n",
      "  10.  10.  10.  10.   9.   3.   7.   8.   7.   7.   0.   2.   3.   2.  10.\n",
      "   5.  10.   0.   2.   0.   3.   3.   3.   6.   6.   2.   6.  10.   5.   2.\n",
      "   0.   8.  10.   7.   8.   8.  10.   9.   1.   8.   0.   5.   4.   9.   7.\n",
      "   2.   2.   7.   7.   2.   2.   0.   0.   5.   0.   1.   1.   9.   1.   1.\n",
      "   2.   0.  10.   1.   3.   6.   0.   4.   8.   5.   5.   3.   1.   3.   4.\n",
      "   0.   0.   5.  10.   6.   3.   5.   5.   4.   5.   4.   3.  10.   6.   5.\n",
      "   5.   3.  10.  10.  10.   2.  10.  10.   3.   7.   1.   3.   7.   6.   5.\n",
      "   6.   6.   4.   8.   7.   5.   8.   3.   3.   2.   0.   5.   0.  10.   6.\n",
      "   6.   6.   6.   5.   7.   0.   7.   1.   3.   0.   0.   5.   7.   5.   1.\n",
      "   0.   7.   5.   5.   7.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set의 예측: {}\".format(best_logreg_model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Xgboost\n",
    "\n",
    "Manual for `Xgboost`: [click](http://xgboost.readthedocs.io/en/latest/how_to/param_tuning.html)\n",
    "\n",
    "다음 parameter들에 대해 data에 대한 Score값과 AUC값을 이용해 최적 모형 parameter를 찾는다.\n",
    "\n",
    "1. max_depth\n",
    "2. min_child_weight\n",
    "3. gamma\n",
    "4. subsample\n",
    "5. colsample_bytree\n",
    "6. reg_alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search CV 를 통해 parameter들을 최적의 값으로 조정해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_depth와 min_child_weight 범위 설정을 통해 최적 parameter 값을 찾는다. \n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(1,20),\n",
    " 'min_child_weight':range(1,20)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': range(1, 20), 'min_child_weight': range(1, 20)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search를 통해 param_test1에 대한 최적 값을 찾는다. \n",
    "\n",
    "grid_search1 = GridSearchCV(XGBClassifier(), param_test1, cv=5)\n",
    "grid_search1.fit(train_X_train, train_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 매개변수: {'max_depth': 2, 'min_child_weight': 19}\n",
      "최고 교차 검증 점수: 0.36\n"
     ]
    }
   ],
   "source": [
    "print(\"최적 매개변수: {}\".format(grid_search1.best_params_))\n",
    "print(\"최고 교차 검증 점수: {:.2f}\".format(grid_search1.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gamma 범위 설정을 통해 최적 parameter 값을 찾는다. \n",
    "\n",
    "param_test2 = {\n",
    " 'gamma': [i/100.0 for i in range(0,5)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=8, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'gamma': [0.0, 0.01, 0.02, 0.03, 0.04]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search를 통해 param_test2에 대한 최적 값을 찾는다. \n",
    "\n",
    "grid_search2 = GridSearchCV(XGBClassifier(max_depth = 2, min_child_weight= 8), param_test2, cv=5)\n",
    "grid_search2.fit(train_X_train, train_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 매개변수: {'gamma': 0.02}\n",
      "최고 교차 검증 점수: 0.34\n"
     ]
    }
   ],
   "source": [
    "print(\"최적 매개변수: {}\".format(grid_search2.best_params_))\n",
    "print(\"최고 교차 검증 점수: {:.2f}\".format(grid_search2.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subsample 과 colsample_bytree 범위 설정을 통해 최적 parameter 값을 찾는다. \n",
    "\n",
    "param_test3 = {\n",
    " 'subsample':[i/100.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(6,10)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=8, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'subsample': [0.06, 0.07, 0.08, 0.09], 'colsample_bytree': [0.06, 0.07, 0.08, 0.09]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search를 통해 param_test3에 대한 최적 값을 찾는다. \n",
    "\n",
    "grid_search3 = GridSearchCV(XGBClassifier(gamma = 0.01, max_depth = 2, min_child_weight= 8), param_test3, cv=5)\n",
    "grid_search3.fit(train_X_train, train_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 매개변수: {'colsample_bytree': 0.06, 'subsample': 0.08}\n",
      "최고 교차 검증 점수: 0.31\n"
     ]
    }
   ],
   "source": [
    "print(\"최적 매개변수: {}\".format(grid_search3.best_params_))\n",
    "print(\"최고 교차 검증 점수: {:.2f}\".format(grid_search3.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reg_alpha 범위 설정을 통해 최적 parameter 값을 찾는다. \n",
    "\n",
    "param_test4 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100, 0, 0.001, 0.005, 0.01, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.06,\n",
       "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=8, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.09),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'reg_alpha': [1e-05, 0.01, 0.1, 1, 100, 0, 0.001, 0.005, 0.01, 0.05]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search를 통해 param_test4에 대한 최적 값을 찾는다. \n",
    "\n",
    "grid_search4 = GridSearchCV(XGBClassifier(colsample_bytree = 0.06, subsample = 0.09, gamma = 0.01, max_depth = 2, min_child_weight= 8), param_test4, cv=5)\n",
    "grid_search4.fit(train_X_train, train_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 매개변수: {'reg_alpha': 1e-05}\n",
      "최고 교차 검증 점수: 0.31\n"
     ]
    }
   ],
   "source": [
    "print(\"최적 매개변수: {}\".format(grid_search4.best_params_))\n",
    "print(\"최고 교차 검증 점수: {:.2f}\".format(grid_search4.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 최적 parameter 값을 이용해 Grid Search를 한 번 더 한다. \n",
    "\n",
    "param_test5 = {\n",
    " 'reg_alpha':[1e-05], \n",
    " 'colsample_bytree':[0.06], \n",
    " 'subsample':[0.09], \n",
    " 'gamma':[0.01], \n",
    " 'max_depth':[2],\n",
    " 'min_child_weight':[8]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.06,\n",
       "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=8, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=1e-05, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.09),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'reg_alpha': [1e-05], 'colsample_bytree': [0.06], 'subsample': [0.09], 'gamma': [0.01], 'max_depth': [2], 'min_child_weight': [8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(XGBClassifier(reg_alpha = 1e-05, colsample_bytree = 0.06, subsample = 0.09, gamma = 0.01, max_depth = 2, min_child_weight= 8), param_test5, cv=5)\n",
    "grid_search.fit(train_X_train, train_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최고 성능 모델:\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.06,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
      "       min_child_weight=8, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=1e-05, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.09)\n",
      "최고 교차 검증 점수: 0.31\n"
     ]
    }
   ],
   "source": [
    "print(\"최고 성능 모델:\\n{}\".format(grid_search.best_estimator_))\n",
    "print(\"최고 교차 검증 점수: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost Classifier에 최적 parameter 값들을  apply해 fitting 시킨다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=2,\n",
    " min_child_weight=8,\n",
    " gamma=0.01,\n",
    " subsample=0.09,\n",
    " colsample_bytree=0.06,\n",
    " reg_alpha = 1e-05,\n",
    " objective= 'multi:softmax',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.06,\n",
       "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=8, missing=None, n_estimators=1000, nthread=4,\n",
       "       objective='multi:softprob', reg_alpha=1e-05, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=27, silent=True, subsample=0.09)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1.fit(train_X_train, train_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE 값을 측정한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8296296296296299"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(train_Y_val, xgb1.predict(train_X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance를 그래프로 그려 확인한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a47fd49160>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAEWCAYAAABG5QDSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a47fbc5978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xgb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.3132\n",
      "AUC Score (Train): 0.495268\n"
     ]
    }
   ],
   "source": [
    "#Predict training set:\n",
    "dtrain_predictions = xgb1.predict(train_X_train)\n",
    "dtrain_predprob = xgb1.predict_proba(train_X_train)\n",
    "#Print model report:\n",
    "print(\"\\nModel Report\")\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(train_Y_train, dtrain_predictions))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_X_train, dtrain_predprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 성능을 좀 더 올리기 위해 Learning rate을 낮추고 n_estimator를 통해 tree를 더 추가한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb2 = XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " n_estimators=10000,\n",
    " max_depth=2,\n",
    " min_child_weight=8,\n",
    " gamma=0.01,\n",
    " subsample=0.09,\n",
    " colsample_bytree=0.06,\n",
    " reg_alpha = 1e-05,\n",
    " objective= 'multi:softmax',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.06,\n",
       "       gamma=0.01, learning_rate=0.01, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=8, missing=None, n_estimators=10000, nthread=4,\n",
       "       objective='multi:softprob', reg_alpha=1e-05, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=27, silent=True, subsample=0.09)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb2.fit(train_X_train, train_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE 값을 측정한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7555555555555555"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(train_Y_val, xgb2.predict(train_X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance를 그래프로 그려 확인한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a47fc7d908>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAEWCAYAAABG5QDSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZiXwgNrMzMzMrAQeUJuZmZmZleD/AHM8r2Y04QVlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a47fc154a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xgb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.3296\n",
      "AUC Score (Train): 0.492288\n"
     ]
    }
   ],
   "source": [
    "#Predict training set:\n",
    "dtrain_predictions = xgb2.predict(train_X_val)\n",
    "dtrain_predprob = xgb2.predict_proba(train_X_val)\n",
    "#Print model report:\n",
    "print(\"\\nModel Report\")\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(train_Y_val, dtrain_predictions))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_X_val, dtrain_predprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set의 예측: [ 5.  5.  5.  5.  5.  5.  5.  5.  5.  5.]\n",
      "실제 Validation set: [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
      "Validation Set의 정확도: 0.33\n",
      "Test set의 예측: [ 4.  5.  5.  5.  5.  5.  5.  5.  5.  5.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation set의 예측: {}\".format(xgb2.predict(train_X_val)[:10]))\n",
    "print(\"실제 Validation set: {}\".format(train_Y_val[:10]))\n",
    "print(\"Validation Set의 정확도: {:.2f}\".format(xgb2.score(train_X_val, train_Y_val)))\n",
    "print(\"Test set의 예측: {}\".format(xgb2.predict(test_X)[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result에 대해 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.06,\n",
      "       gamma=0.01, learning_rate=0.01, max_delta_step=0, max_depth=2,\n",
      "       min_child_weight=8, missing=None, n_estimators=10000, nthread=4,\n",
      "       objective='multi:softprob', reg_alpha=1e-05, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=27, silent=True, subsample=0.09)\n"
     ]
    }
   ],
   "source": [
    "best_xgb_result = xgb2\n",
    "print(best_xgb_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model의 MAE 값을 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54814814815\n"
     ]
    }
   ],
   "source": [
    "best_xgb_model = best_xgb_result\n",
    "best_xgb_model = best_xgb_model.fit(train_X_train, train_Y_train)\n",
    "print(metrics.mean_absolute_error(best_xgb_model.predict(train_X_val), train_Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확률:\n",
      "[[ 0.04597149  0.04575544  0.04738645  0.13210092  0.13366631  0.23773444\n",
      "   0.09812886  0.11085046  0.05152455  0.04585176  0.05102929]\n",
      " [ 0.04475919  0.04454884  0.04613684  0.12861733  0.13014142  0.25783595\n",
      "   0.09554113  0.10792726  0.05016581  0.04464261  0.04968361]\n",
      " [ 0.04911323  0.04888242  0.05062489  0.14112884  0.14280121  0.18564042\n",
      "   0.1048351   0.11842611  0.0550458   0.04898531  0.05451669]\n",
      " [ 0.04324971  0.04304646  0.0445809   0.12427977  0.12545304  0.2831645\n",
      "   0.09231906  0.10428747  0.04847399  0.04313706  0.04800806]\n",
      " [ 0.04243613  0.0422367   0.04374228  0.12194193  0.12338693  0.29635522\n",
      "   0.09058244  0.1023257   0.04756214  0.0423256   0.04710497]\n",
      " [ 0.04450689  0.04429772  0.04587676  0.12789232  0.12940782  0.26201952\n",
      "   0.09500258  0.10731888  0.04988303  0.04439097  0.04940355]]\n",
      "합: [ 1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# predict_proba 결과 중 앞부분 6개에 대해서만 확인한다.\n",
    "print(\"예측 확률:\\n{}\".format(best_xgb_model.predict_proba(train_X_val)[:6]))\n",
    "\n",
    "# 행 방향으로 확률을 더하면 모두 1이 된다.\n",
    "print(\"합: {}\".format(best_xgb_model.predict_proba(train_X_val)[:6].sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_proba의 결과에 argmax 함수를 적용해서 예측을 재연할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 큰 예측 확률의 인덱스:\n",
      "[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 5 5 5 4 5 5 4 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 4 5 5 5 5 5 5 5 5 5]\n",
      "예측:\n",
      "[ 5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  4.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  4.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  4.  5.  5.  5.  4.  5.\n",
      "  5.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.]\n"
     ]
    }
   ],
   "source": [
    "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(np.argmax(best_xgb_model.predict_proba(train_X_val), axis=1)))\n",
    "print(\"예측:\\n{}\".format(best_xgb_model.predict(train_X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      "Validation set의 예측: [ 5.  5.  5.  5.  5.  5.  5.  5.  5.  5.]\n",
      "실제 Validation set: [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
      "Validation Set의 정확도: 0.33\n",
      "Test set의 예측: [ 4.  5.  5.  5.  5.  5.  5.  5.  5.  5.]\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 데이터에 있는 클래스 종류: {}\".format(best_xgb_model.classes_))\n",
    "print(\"Validation set의 예측: {}\".format(best_xgb_model.predict(train_X_val)[:10]))\n",
    "print(\"실제 Validation set: {}\".format(train_Y_val[:10]))\n",
    "print(\"Validation Set의 정확도: {:.2f}\".format(best_xgb_model.score(train_X_val, train_Y_val)))\n",
    "print(\"Test set의 예측: {}\".format(best_xgb_model.predict(test_X)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set의 예측: [ 4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  4.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  4.  5.  5.  5.  5.  5.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  4.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set의 전체 예측: {}\".format(best_xgb_model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 SoftMax\n",
    "\n",
    "Manual for `Soft Max`: [click](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/)\n",
    "\n",
    "\n",
    "SoftMax 모델을 통해 X와 Y variable들을 One Hot Encoding 해주고 최적 Accuracy를 찾아낸다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('new_data.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899, 15) (899, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y 값의 클래스가 11개이므로 nb_classes를 11개로 잡는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classes = 11  # 0 ~ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 15])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot Tensor(\"one_hot:0\", shape=(?, 1, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot\", Y_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape Tensor(\"Reshape:0\", shape=(?, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape\", Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([15, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross entropy cost/loss\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                 labels=Y_one_hot)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tLoss: 12.367\tAcc: 8.34%\n",
      "Step:   100\tLoss: 2.730\tAcc: 23.58%\n",
      "Step:   200\tLoss: 2.395\tAcc: 23.69%\n",
      "Step:   300\tLoss: 2.233\tAcc: 27.03%\n",
      "Step:   400\tLoss: 2.133\tAcc: 29.48%\n",
      "Step:   500\tLoss: 2.063\tAcc: 30.70%\n",
      "Step:   600\tLoss: 2.014\tAcc: 30.81%\n",
      "Step:   700\tLoss: 1.977\tAcc: 31.92%\n",
      "Step:   800\tLoss: 1.948\tAcc: 32.70%\n",
      "Step:   900\tLoss: 1.926\tAcc: 32.93%\n",
      "Step:  1000\tLoss: 1.909\tAcc: 34.15%\n",
      "Step:  1100\tLoss: 1.895\tAcc: 34.48%\n",
      "Step:  1200\tLoss: 1.884\tAcc: 34.93%\n",
      "Step:  1300\tLoss: 1.875\tAcc: 35.04%\n",
      "Step:  1400\tLoss: 1.868\tAcc: 35.26%\n",
      "Step:  1500\tLoss: 1.862\tAcc: 35.48%\n",
      "Step:  1600\tLoss: 1.857\tAcc: 35.71%\n",
      "Step:  1700\tLoss: 1.852\tAcc: 35.82%\n",
      "Step:  1800\tLoss: 1.848\tAcc: 35.71%\n",
      "Step:  1900\tLoss: 1.845\tAcc: 35.71%\n",
      "Step:  2000\tLoss: 1.842\tAcc: 35.60%\n",
      "Step:  2100\tLoss: 1.840\tAcc: 35.93%\n",
      "Step:  2200\tLoss: 1.837\tAcc: 36.04%\n",
      "Step:  2300\tLoss: 1.835\tAcc: 36.26%\n",
      "Step:  2400\tLoss: 1.834\tAcc: 36.15%\n",
      "Step:  2500\tLoss: 1.832\tAcc: 36.26%\n",
      "Step:  2600\tLoss: 1.830\tAcc: 36.48%\n",
      "Step:  2700\tLoss: 1.829\tAcc: 36.48%\n",
      "Step:  2800\tLoss: 1.828\tAcc: 36.48%\n",
      "Step:  2900\tLoss: 1.827\tAcc: 36.60%\n",
      "Step:  3000\tLoss: 1.826\tAcc: 36.60%\n",
      "Step:  3100\tLoss: 1.825\tAcc: 36.48%\n",
      "Step:  3200\tLoss: 1.824\tAcc: 36.48%\n",
      "Step:  3300\tLoss: 1.823\tAcc: 36.48%\n",
      "Step:  3400\tLoss: 1.822\tAcc: 36.48%\n",
      "Step:  3500\tLoss: 1.822\tAcc: 36.48%\n",
      "Step:  3600\tLoss: 1.821\tAcc: 36.48%\n",
      "Step:  3700\tLoss: 1.821\tAcc: 36.37%\n",
      "Step:  3800\tLoss: 1.820\tAcc: 36.48%\n",
      "Step:  3900\tLoss: 1.819\tAcc: 36.37%\n",
      "Step:  4000\tLoss: 1.819\tAcc: 36.26%\n",
      "Step:  4100\tLoss: 1.818\tAcc: 36.37%\n",
      "Step:  4200\tLoss: 1.818\tAcc: 36.15%\n",
      "Step:  4300\tLoss: 1.818\tAcc: 36.37%\n",
      "Step:  4400\tLoss: 1.817\tAcc: 36.37%\n",
      "Step:  4500\tLoss: 1.817\tAcc: 36.37%\n",
      "Step:  4600\tLoss: 1.816\tAcc: 36.48%\n",
      "Step:  4700\tLoss: 1.816\tAcc: 36.48%\n",
      "Step:  4800\tLoss: 1.816\tAcc: 36.60%\n",
      "Step:  4900\tLoss: 1.815\tAcc: 36.60%\n",
      "Step:  5000\tLoss: 1.815\tAcc: 36.60%\n",
      "Step:  5100\tLoss: 1.815\tAcc: 36.60%\n",
      "Step:  5200\tLoss: 1.815\tAcc: 36.48%\n",
      "Step:  5300\tLoss: 1.814\tAcc: 36.37%\n",
      "Step:  5400\tLoss: 1.814\tAcc: 36.26%\n",
      "Step:  5500\tLoss: 1.814\tAcc: 36.26%\n",
      "Step:  5600\tLoss: 1.814\tAcc: 36.26%\n",
      "Step:  5700\tLoss: 1.813\tAcc: 36.15%\n",
      "Step:  5800\tLoss: 1.813\tAcc: 36.15%\n",
      "Step:  5900\tLoss: 1.813\tAcc: 36.15%\n",
      "Step:  6000\tLoss: 1.813\tAcc: 36.15%\n",
      "Step:  6100\tLoss: 1.812\tAcc: 36.26%\n",
      "Step:  6200\tLoss: 1.812\tAcc: 36.37%\n",
      "Step:  6300\tLoss: 1.812\tAcc: 36.37%\n",
      "Step:  6400\tLoss: 1.812\tAcc: 36.37%\n",
      "Step:  6500\tLoss: 1.812\tAcc: 36.37%\n",
      "Step:  6600\tLoss: 1.812\tAcc: 36.37%\n",
      "Step:  6700\tLoss: 1.811\tAcc: 36.48%\n",
      "Step:  6800\tLoss: 1.811\tAcc: 36.48%\n",
      "Step:  6900\tLoss: 1.811\tAcc: 36.48%\n",
      "Step:  7000\tLoss: 1.811\tAcc: 36.48%\n",
      "Step:  7100\tLoss: 1.811\tAcc: 36.48%\n",
      "Step:  7200\tLoss: 1.811\tAcc: 36.48%\n",
      "Step:  7300\tLoss: 1.810\tAcc: 36.48%\n",
      "Step:  7400\tLoss: 1.810\tAcc: 36.48%\n",
      "Step:  7500\tLoss: 1.810\tAcc: 36.37%\n",
      "Step:  7600\tLoss: 1.810\tAcc: 36.37%\n",
      "Step:  7700\tLoss: 1.810\tAcc: 36.37%\n",
      "Step:  7800\tLoss: 1.810\tAcc: 36.26%\n",
      "Step:  7900\tLoss: 1.810\tAcc: 36.26%\n",
      "Step:  8000\tLoss: 1.810\tAcc: 36.26%\n",
      "Step:  8100\tLoss: 1.809\tAcc: 36.26%\n",
      "Step:  8200\tLoss: 1.809\tAcc: 36.37%\n",
      "Step:  8300\tLoss: 1.809\tAcc: 36.37%\n",
      "Step:  8400\tLoss: 1.809\tAcc: 36.37%\n",
      "Step:  8500\tLoss: 1.809\tAcc: 36.37%\n",
      "Step:  8600\tLoss: 1.809\tAcc: 36.37%\n",
      "Step:  8700\tLoss: 1.809\tAcc: 36.37%\n",
      "Step:  8800\tLoss: 1.809\tAcc: 36.37%\n",
      "Step:  8900\tLoss: 1.809\tAcc: 36.37%\n",
      "Step:  9000\tLoss: 1.809\tAcc: 36.37%\n",
      "Step:  9100\tLoss: 1.808\tAcc: 36.37%\n",
      "Step:  9200\tLoss: 1.808\tAcc: 36.48%\n",
      "Step:  9300\tLoss: 1.808\tAcc: 36.48%\n",
      "Step:  9400\tLoss: 1.808\tAcc: 36.60%\n",
      "Step:  9500\tLoss: 1.808\tAcc: 36.48%\n",
      "Step:  9600\tLoss: 1.808\tAcc: 36.48%\n",
      "Step:  9700\tLoss: 1.808\tAcc: 36.48%\n",
      "Step:  9800\tLoss: 1.808\tAcc: 36.48%\n",
      "Step:  9900\tLoss: 1.808\tAcc: 36.48%\n",
      "Step: 10000\tLoss: 1.808\tAcc: 36.48%\n",
      "Step: 10100\tLoss: 1.808\tAcc: 36.48%\n",
      "Step: 10200\tLoss: 1.808\tAcc: 36.48%\n",
      "Step: 10300\tLoss: 1.808\tAcc: 36.48%\n",
      "Step: 10400\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 10500\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 10600\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 10700\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 10800\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 10900\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 11000\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 11100\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 11200\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 11300\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 11400\tLoss: 1.807\tAcc: 36.37%\n",
      "Step: 11500\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 11600\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 11700\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 11800\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 11900\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 12000\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 12100\tLoss: 1.807\tAcc: 36.48%\n",
      "Step: 12200\tLoss: 1.806\tAcc: 36.48%\n",
      "Step: 12300\tLoss: 1.806\tAcc: 36.48%\n",
      "Step: 12400\tLoss: 1.806\tAcc: 36.48%\n",
      "Step: 12500\tLoss: 1.806\tAcc: 36.48%\n",
      "Step: 12600\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 12700\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 12800\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 12900\tLoss: 1.806\tAcc: 36.82%\n",
      "Step: 13000\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 13100\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 13200\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 13300\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 13400\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 13500\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 13600\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 13700\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 13800\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 13900\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 14000\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 14100\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 14200\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 14300\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 14400\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 14500\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 14600\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 14700\tLoss: 1.806\tAcc: 36.71%\n",
      "Step: 14800\tLoss: 1.805\tAcc: 36.71%\n",
      "Step: 14900\tLoss: 1.805\tAcc: 36.71%\n",
      "Step: 15000\tLoss: 1.805\tAcc: 36.82%\n",
      "Step: 15100\tLoss: 1.805\tAcc: 36.82%\n",
      "Step: 15200\tLoss: 1.805\tAcc: 36.82%\n",
      "Step: 15300\tLoss: 1.805\tAcc: 36.82%\n",
      "Step: 15400\tLoss: 1.805\tAcc: 36.82%\n",
      "Step: 15500\tLoss: 1.805\tAcc: 36.82%\n",
      "Step: 15600\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 15700\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 15800\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 15900\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 16000\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 16100\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 16200\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 16300\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 16400\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 16500\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 16600\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 16700\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 16800\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 16900\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 17000\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 17100\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 17200\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 17300\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 17400\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 17500\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 17600\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 17700\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 17800\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 17900\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 18000\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 18100\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 18200\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 18300\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 18400\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 18500\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 18600\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 18700\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 18800\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 18900\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 19000\tLoss: 1.805\tAcc: 37.04%\n",
      "Step: 19100\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 19200\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 19300\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 19400\tLoss: 1.805\tAcc: 36.93%\n",
      "Step: 19500\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 19600\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 19700\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 19800\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 19900\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 20000\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 20100\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 20200\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 20300\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 20400\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 20500\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 20600\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 20700\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 20800\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 20900\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 21000\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 21100\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 21200\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 21300\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 21400\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 21500\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 21600\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 21700\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 21800\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 21900\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 22000\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 22100\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 22200\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 22300\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 22400\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 22500\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 22600\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 22700\tLoss: 1.804\tAcc: 36.93%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 22800\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 22900\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 23000\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 23100\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 23200\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 23300\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 23400\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 23500\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 23600\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 23700\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 23800\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 23900\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 24000\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 24100\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 24200\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 24300\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 24400\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 24500\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 24600\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 24700\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 24800\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 24900\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 25000\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 25100\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 25200\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 25300\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 25400\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 25500\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 25600\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 25700\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 25800\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 25900\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 26000\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 26100\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 26200\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 26300\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 26400\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 26500\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 26600\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 26700\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 26800\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 26900\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 27000\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 27100\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 27200\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 27300\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 27400\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 27500\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 27600\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 27700\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 27800\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 27900\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 28000\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 28100\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 28200\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 28300\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 28400\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 28500\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 28600\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 28700\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 28800\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 28900\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 29000\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 29100\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 29200\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 29300\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 29400\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 29500\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 29600\tLoss: 1.804\tAcc: 36.93%\n",
      "Step: 29700\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 29800\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 29900\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 30000\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 30100\tLoss: 1.804\tAcc: 36.82%\n",
      "Step: 30200\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 30300\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 30400\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 30500\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 30600\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 30700\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 30800\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 30900\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 31000\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 31100\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 31200\tLoss: 1.804\tAcc: 36.71%\n",
      "Step: 31300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 31400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 31500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 31600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 31700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 31800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 31900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 32000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 32100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 32200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 32300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 32400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 32500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 32600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 32700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 32800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 32900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 33000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 33100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 33200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 33300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 33400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 33500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 33600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 33700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 33800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 33900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 34000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 34100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 34200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 34300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 34400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 34500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 34600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 34700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 34800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 34900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 35000\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 35100\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 35200\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 35300\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 35400\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 35500\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 35600\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 35700\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 35800\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 35900\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 36000\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 36100\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 36200\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 36300\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 36400\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 36500\tLoss: 1.803\tAcc: 36.82%\n",
      "Step: 36600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 36700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 36800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 36900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 37000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 37100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 37200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 37300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 37400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 37500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 37600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 37700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 37800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 37900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 38000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 38100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 38200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 38300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 38400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 38500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 38600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 38700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 38800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 38900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 39000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 39100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 39200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 39300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 39400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 39500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 39600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 39700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 39800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 39900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 40000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 40100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 40200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 40300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 40400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 40500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 40600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 40700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 40800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 40900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 41000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 41100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 41200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 41300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 41400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 41500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 41600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 41700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 41800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 41900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 42000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 42100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 42200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 42300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 42400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 42500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 42600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 42700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 42800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 42900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 43000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 43100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 43200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 43300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 43400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 43500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 43600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 43700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 43800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 43900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 44000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 44100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 44200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 44300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 44400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 44500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 44600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 44700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 44800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 44900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 45000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 45100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 45200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 45300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 45400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 45500\tLoss: 1.803\tAcc: 36.71%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 45600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 45700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 45800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 45900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 46000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 46100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 46200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 46300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 46400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 46500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 46600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 46700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 46800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 46900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 47000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 47100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 47200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 47300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 47400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 47500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 47600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 47700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 47800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 47900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 48000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 48100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 48200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 48300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 48400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 48500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 48600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 48700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 48800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 48900\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 49000\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 49100\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 49200\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 49300\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 49400\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 49500\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 49600\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 49700\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 49800\tLoss: 1.803\tAcc: 36.71%\n",
      "Step: 49900\tLoss: 1.803\tAcc: 36.71%\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 6 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 9 True Y: 8\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 5 True Y: 9\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 8 True Y: 3\n",
      "[False] Prediction: 8 True Y: 9\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 10 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 1 True Y: 0\n",
      "[False] Prediction: 7 True Y: 2\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[False] Prediction: 10 True Y: 5\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 7 True Y: 0\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 4 True Y: 3\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 2 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 8 True Y: 6\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 0\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[False] Prediction: 3 True Y: 8\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 10 True Y: 7\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 10 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 6 True Y: 7\n",
      "[False] Prediction: 8 True Y: 6\n",
      "[True] Prediction: 10 True Y: 10\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 6\n",
      "[False] Prediction: 10 True Y: 6\n",
      "[False] Prediction: 3 True Y: 0\n",
      "[False] Prediction: 6 True Y: 4\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 10 True Y: 10\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 7 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 6\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 7 True Y: 5\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 3 True Y: 8\n",
      "[False] Prediction: 10 True Y: 7\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 7 True Y: 8\n",
      "[False] Prediction: 6 True Y: 10\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 10 True Y: 10\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 2 True Y: 5\n",
      "[False] Prediction: 10 True Y: 4\n",
      "[False] Prediction: 9 True Y: 8\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 10 True Y: 7\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 0 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 7 True Y: 5\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 4 True Y: 2\n",
      "[False] Prediction: 7 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 7\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 7 True Y: 6\n",
      "[False] Prediction: 6 True Y: 0\n",
      "[False] Prediction: 3 True Y: 1\n",
      "[False] Prediction: 6 True Y: 8\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[False] Prediction: 7 True Y: 4\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 7 True Y: 6\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 7 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 10 True Y: 9\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 10 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 10 True Y: 7\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 10 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 4 True Y: 1\n",
      "[False] Prediction: 8 True Y: 10\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 8 True Y: 4\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 7 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 4 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 10 True Y: 8\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 8 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 0\n",
      "[False] Prediction: 3 True Y: 6\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[False] Prediction: 2 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 10 True Y: 7\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 10 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 8 True Y: 10\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 7 True Y: 10\n",
      "[False] Prediction: 6 True Y: 9\n",
      "[False] Prediction: 8 True Y: 9\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 7 True Y: 5\n",
      "[False] Prediction: 6 True Y: 9\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 4 True Y: 3\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 7 True Y: 6\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 3 True Y: 8\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 8 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 5 True Y: 9\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 6 True Y: 1\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 10 True Y: 0\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 3 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 9\n",
      "[False] Prediction: 6 True Y: 2\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 7 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 7 True Y: 6\n",
      "[False] Prediction: 8 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[True] Prediction: 9 True Y: 9\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 10 True Y: 10\n",
      "[False] Prediction: 8 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 10 True Y: 8\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 7 True Y: 5\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 3 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 4 True Y: 6\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 8 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 4\n",
      "[False] Prediction: 7 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 10 True Y: 8\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 10 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 2 True Y: 4\n",
      "[False] Prediction: 8 True Y: 9\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 4 True Y: 5\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 4 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 7 True Y: 9\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 6 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 3 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 8 True Y: 5\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 6 True Y: 4\n",
      "[False] Prediction: 5 True Y: 9\n",
      "[False] Prediction: 10 True Y: 6\n",
      "[False] Prediction: 10 True Y: 8\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[False] Prediction: 8 True Y: 4\n",
      "[False] Prediction: 4 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 8 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[False] Prediction: 8 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 7 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 6 True Y: 8\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 4 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 7 True Y: 10\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 9\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 6 True Y: 9\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 6 True Y: 8\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 2 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 9 True Y: 9\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 10 True Y: 9\n",
      "[False] Prediction: 10 True Y: 4\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 7 True Y: 10\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 7 True Y: 1\n",
      "[False] Prediction: 8 True Y: 5\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 3 True Y: 7\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 2 True Y: 3\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 10 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 6 True Y: 7\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 10 True Y: 8\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 10\n",
      "[False] Prediction: 7 True Y: 8\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 9\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 9 True Y: 7\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 3 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 6 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 8 True Y: 10\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 7 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 10 True Y: 9\n",
      "[False] Prediction: 9 True Y: 4\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 10 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 10 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 9\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 6 True Y: 4\n",
      "[False] Prediction: 8 True Y: 6\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[False] Prediction: 4 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[False] Prediction: 6 True Y: 9\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 4 True Y: 0\n",
      "[False] Prediction: 3 True Y: 7\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 7 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 7 True Y: 8\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 8 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 2 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 10 True Y: 8\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 7 True Y: 4\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 10 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 4 True Y: 8\n",
      "[True] Prediction: 10 True Y: 10\n",
      "[False] Prediction: 10 True Y: 8\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 6 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 3 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 4 True Y: 0\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[False] Prediction: 8 True Y: 6\n",
      "[False] Prediction: 7 True Y: 6\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[False] Prediction: 5 True Y: 0\n",
      "[False] Prediction: 3 True Y: 4\n",
      "[False] Prediction: 8 True Y: 6\n",
      "[False] Prediction: 8 True Y: 10\n",
      "[False] Prediction: 3 True Y: 2\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 8 True Y: 8\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 10\n",
      "[False] Prediction: 5 True Y: 8\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 10 True Y: 6\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 1\n",
      "[False] Prediction: 8 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 10 True Y: 10\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 8 True Y: 5\n",
      "[True] Prediction: 9 True Y: 9\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[False] Prediction: 5 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[False] Prediction: 5 True Y: 10\n",
      "[True] Prediction: 7 True Y: 7\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[False] Prediction: 5 True Y: 3\n",
      "[False] Prediction: 5 True Y: 2\n",
      "[False] Prediction: 5 True Y: 4\n",
      "[True] Prediction: 5 True Y: 5\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    result8=[]\n",
    "    for step in range(50000):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                                 X: x_data, Y: y_data})\n",
    "            result8.append([hypothesis, step, loss, acc])\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n",
    "\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm_result = sorted(result8, key=lambda x: x[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  15900,\n",
       "  1.8051865,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  16000,\n",
       "  1.8051617,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  16100,\n",
       "  1.805137,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  16200,\n",
       "  1.8051133,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  16300,\n",
       "  1.8050896,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  16800,\n",
       "  1.8049773,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  16900,\n",
       "  1.8049555,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  17000,\n",
       "  1.8049343,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  17100,\n",
       "  1.804913,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  17200,\n",
       "  1.8048927,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  17300,\n",
       "  1.8048718,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  17400,\n",
       "  1.8048519,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  17500,\n",
       "  1.8048327,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  17600,\n",
       "  1.8048126,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  17700,\n",
       "  1.8047936,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  17800,\n",
       "  1.8047748,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  18000,\n",
       "  1.8047376,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  18100,\n",
       "  1.8047196,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  18200,\n",
       "  1.8047014,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  18300,\n",
       "  1.8046844,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  18400,\n",
       "  1.8046664,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  18500,\n",
       "  1.8046496,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  18600,\n",
       "  1.8046327,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  18700,\n",
       "  1.8046159,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  18800,\n",
       "  1.8045993,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  18900,\n",
       "  1.8045828,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  19000,\n",
       "  1.8045673,\n",
       "  0.37041157],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  15600,\n",
       "  1.8052626,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  15700,\n",
       "  1.8052367,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  15800,\n",
       "  1.8052115,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  16400,\n",
       "  1.8050665,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  16500,\n",
       "  1.805043,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  16600,\n",
       "  1.8050207,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  16700,\n",
       "  1.8049988,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  17900,\n",
       "  1.8047563,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  19100,\n",
       "  1.8045511,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  19200,\n",
       "  1.8045357,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  19300,\n",
       "  1.8045202,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  19400,\n",
       "  1.8045052,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  19500,\n",
       "  1.8044901,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  19600,\n",
       "  1.8044753,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  19700,\n",
       "  1.8044605,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  19800,\n",
       "  1.8044466,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  19900,\n",
       "  1.8044319,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  20000,\n",
       "  1.8044181,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  20100,\n",
       "  1.804404,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  20200,\n",
       "  1.8043904,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  20300,\n",
       "  1.8043766,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  20400,\n",
       "  1.8043634,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  20500,\n",
       "  1.80435,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  20600,\n",
       "  1.804337,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  20700,\n",
       "  1.8043243,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  20800,\n",
       "  1.8043118,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  20900,\n",
       "  1.8042991,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  21000,\n",
       "  1.8042864,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  21100,\n",
       "  1.8042736,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  21200,\n",
       "  1.8042625,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  21300,\n",
       "  1.80425,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  21400,\n",
       "  1.8042377,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  21500,\n",
       "  1.8042263,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  21600,\n",
       "  1.8042147,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  21700,\n",
       "  1.8042033,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  21800,\n",
       "  1.8041921,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  21900,\n",
       "  1.8041809,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  22000,\n",
       "  1.8041698,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  22100,\n",
       "  1.8041592,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  22200,\n",
       "  1.8041476,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  22300,\n",
       "  1.8041375,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  22400,\n",
       "  1.8041266,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  22500,\n",
       "  1.8041162,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  22600,\n",
       "  1.8041056,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  22700,\n",
       "  1.8040957,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  22800,\n",
       "  1.8040853,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  22900,\n",
       "  1.8040755,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  23000,\n",
       "  1.8040653,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  23100,\n",
       "  1.8040558,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  23200,\n",
       "  1.8040463,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  23300,\n",
       "  1.8040365,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  23400,\n",
       "  1.8040267,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  23500,\n",
       "  1.8040173,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  23600,\n",
       "  1.8040081,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  23700,\n",
       "  1.8039993,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  23800,\n",
       "  1.8039902,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  23900,\n",
       "  1.8039813,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  24000,\n",
       "  1.8039719,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  24100,\n",
       "  1.8039631,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  24200,\n",
       "  1.8039542,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  24300,\n",
       "  1.8039455,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  24400,\n",
       "  1.8039373,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  24500,\n",
       "  1.8039286,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  24600,\n",
       "  1.8039206,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  24700,\n",
       "  1.8039122,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  24800,\n",
       "  1.8039042,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  24900,\n",
       "  1.8038956,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  25000,\n",
       "  1.8038877,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  25100,\n",
       "  1.8038796,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  25200,\n",
       "  1.8038719,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  25300,\n",
       "  1.803864,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  25800,\n",
       "  1.8038256,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  25900,\n",
       "  1.803818,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  26000,\n",
       "  1.8038108,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  26100,\n",
       "  1.8038036,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  26200,\n",
       "  1.8037963,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  26300,\n",
       "  1.8037894,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  26400,\n",
       "  1.8037823,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  26500,\n",
       "  1.8037754,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  26600,\n",
       "  1.8037685,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  26700,\n",
       "  1.8037615,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  26800,\n",
       "  1.8037547,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  26900,\n",
       "  1.803748,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  27000,\n",
       "  1.803741,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  27100,\n",
       "  1.8037345,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  27200,\n",
       "  1.8037277,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  27300,\n",
       "  1.8037213,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  27400,\n",
       "  1.803715,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  27500,\n",
       "  1.8037084,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  27600,\n",
       "  1.8037024,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  27700,\n",
       "  1.803696,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  27800,\n",
       "  1.8036891,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  27900,\n",
       "  1.803683,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  28000,\n",
       "  1.803677,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  28100,\n",
       "  1.8036711,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  28900,\n",
       "  1.8036242,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  29000,\n",
       "  1.803618,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  29100,\n",
       "  1.8036127,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  29200,\n",
       "  1.8036072,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  29300,\n",
       "  1.8036013,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  29400,\n",
       "  1.8035958,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  29500,\n",
       "  1.8035908,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  29600,\n",
       "  1.8035848,\n",
       "  0.36929923],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  12900,\n",
       "  1.8061516,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  15000,\n",
       "  1.8054268,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  15100,\n",
       "  1.8053983,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  15200,\n",
       "  1.8053705,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  15300,\n",
       "  1.8053427,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  15400,\n",
       "  1.8053159,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  15500,\n",
       "  1.805289,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  25400,\n",
       "  1.8038559,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  25500,\n",
       "  1.8038484,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  25600,\n",
       "  1.8038408,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  25700,\n",
       "  1.8038332,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  28200,\n",
       "  1.8036652,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  28300,\n",
       "  1.8036588,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  28400,\n",
       "  1.8036532,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  28500,\n",
       "  1.8036468,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  28600,\n",
       "  1.8036414,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  28700,\n",
       "  1.8036355,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  28800,\n",
       "  1.8036296,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  29700,\n",
       "  1.8035797,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  29800,\n",
       "  1.8035744,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  29900,\n",
       "  1.8035697,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  30000,\n",
       "  1.8035641,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  30100,\n",
       "  1.8035587,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  35000,\n",
       "  1.8033437,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  35100,\n",
       "  1.8033401,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  35200,\n",
       "  1.803336,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  35300,\n",
       "  1.8033328,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  35400,\n",
       "  1.8033289,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  35500,\n",
       "  1.8033254,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  35600,\n",
       "  1.8033214,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  35700,\n",
       "  1.803318,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  35800,\n",
       "  1.8033147,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  35900,\n",
       "  1.8033106,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  36000,\n",
       "  1.8033077,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  36100,\n",
       "  1.8033044,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  36200,\n",
       "  1.8033007,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  36300,\n",
       "  1.803297,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  36400,\n",
       "  1.8032937,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  36500,\n",
       "  1.8032907,\n",
       "  0.36818686],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  12600,\n",
       "  1.8062782,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  12700,\n",
       "  1.8062357,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  12800,\n",
       "  1.8061932,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  13000,\n",
       "  1.8061105,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  13100,\n",
       "  1.8060704,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  13200,\n",
       "  1.8060309,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  13300,\n",
       "  1.8059926,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  13400,\n",
       "  1.8059542,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  13500,\n",
       "  1.805917,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  13600,\n",
       "  1.8058803,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  13700,\n",
       "  1.8058443,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  13800,\n",
       "  1.8058089,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  13900,\n",
       "  1.8057736,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  14000,\n",
       "  1.8057395,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  14100,\n",
       "  1.805706,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  14200,\n",
       "  1.8056728,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  14300,\n",
       "  1.8056407,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  14400,\n",
       "  1.8056083,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  14500,\n",
       "  1.8055766,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  14600,\n",
       "  1.8055457,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  14700,\n",
       "  1.8055154,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  14800,\n",
       "  1.8054856,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  14900,\n",
       "  1.8054562,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  30200,\n",
       "  1.8035533,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  30300,\n",
       "  1.8035487,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  30400,\n",
       "  1.8035432,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  30500,\n",
       "  1.8035383,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  30600,\n",
       "  1.8035333,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  30700,\n",
       "  1.8035282,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  30800,\n",
       "  1.8035232,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  30900,\n",
       "  1.8035184,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  31000,\n",
       "  1.8035133,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  31100,\n",
       "  1.8035082,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  31200,\n",
       "  1.8035038,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  31300,\n",
       "  1.8034989,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  31400,\n",
       "  1.8034943,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  31500,\n",
       "  1.8034894,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  31600,\n",
       "  1.8034848,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  31700,\n",
       "  1.8034803,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  31800,\n",
       "  1.8034759,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  31900,\n",
       "  1.803471,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  32000,\n",
       "  1.8034668,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  32100,\n",
       "  1.8034623,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  32200,\n",
       "  1.8034576,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  32300,\n",
       "  1.803453,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  32400,\n",
       "  1.8034488,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  32500,\n",
       "  1.8034441,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  32600,\n",
       "  1.8034402,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  32700,\n",
       "  1.8034354,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  32800,\n",
       "  1.8034313,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  32900,\n",
       "  1.8034271,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  33000,\n",
       "  1.8034228,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  33100,\n",
       "  1.8034183,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  33200,\n",
       "  1.8034141,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  33300,\n",
       "  1.8034105,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  33400,\n",
       "  1.8034064,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  33500,\n",
       "  1.8034021,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  33600,\n",
       "  1.8033981,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  33700,\n",
       "  1.8033938,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  33800,\n",
       "  1.8033903,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  33900,\n",
       "  1.803386,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  34000,\n",
       "  1.8033822,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  34100,\n",
       "  1.8033776,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  34200,\n",
       "  1.8033742,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  34300,\n",
       "  1.8033704,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  34400,\n",
       "  1.8033662,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  34500,\n",
       "  1.8033624,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  34600,\n",
       "  1.8033586,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  34700,\n",
       "  1.8033547,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  34800,\n",
       "  1.8033509,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  34900,\n",
       "  1.8033475,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  36600,\n",
       "  1.8032868,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  36700,\n",
       "  1.8032835,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  36800,\n",
       "  1.8032802,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  36900,\n",
       "  1.8032764,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  37000,\n",
       "  1.8032737,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  37100,\n",
       "  1.8032701,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  37200,\n",
       "  1.8032669,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  37300,\n",
       "  1.8032637,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  37400,\n",
       "  1.8032603,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  37500,\n",
       "  1.8032572,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  37600,\n",
       "  1.8032542,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  37700,\n",
       "  1.8032507,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  37800,\n",
       "  1.8032475,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  37900,\n",
       "  1.8032445,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  38000,\n",
       "  1.8032416,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  38100,\n",
       "  1.803238,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  38200,\n",
       "  1.8032351,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  38300,\n",
       "  1.8032318,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  38400,\n",
       "  1.8032292,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  38500,\n",
       "  1.8032258,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  38600,\n",
       "  1.803223,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  38700,\n",
       "  1.8032199,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  38800,\n",
       "  1.8032167,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  38900,\n",
       "  1.8032138,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  39000,\n",
       "  1.8032112,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  39100,\n",
       "  1.8032079,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  39200,\n",
       "  1.8032053,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  39300,\n",
       "  1.8032024,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  39400,\n",
       "  1.8031994,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  39500,\n",
       "  1.8031964,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  39600,\n",
       "  1.8031936,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  39700,\n",
       "  1.8031906,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  39800,\n",
       "  1.8031877,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  39900,\n",
       "  1.803185,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  40000,\n",
       "  1.8031819,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  40100,\n",
       "  1.8031793,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  40200,\n",
       "  1.8031766,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  40300,\n",
       "  1.8031738,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  40400,\n",
       "  1.803171,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  40500,\n",
       "  1.8031685,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  40600,\n",
       "  1.8031657,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  40700,\n",
       "  1.8031627,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  40800,\n",
       "  1.8031603,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  40900,\n",
       "  1.8031573,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  41000,\n",
       "  1.8031549,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  41100,\n",
       "  1.8031522,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  41200,\n",
       "  1.8031498,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  41300,\n",
       "  1.8031472,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  41400,\n",
       "  1.8031443,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  41500,\n",
       "  1.8031417,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  41600,\n",
       "  1.8031391,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  41700,\n",
       "  1.8031365,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  41800,\n",
       "  1.8031341,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  41900,\n",
       "  1.8031316,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  42000,\n",
       "  1.803129,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  42100,\n",
       "  1.8031267,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  42200,\n",
       "  1.803124,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  42300,\n",
       "  1.8031213,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  42400,\n",
       "  1.8031191,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  42500,\n",
       "  1.8031163,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  42600,\n",
       "  1.8031141,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  42700,\n",
       "  1.8031113,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  42800,\n",
       "  1.8031092,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  42900,\n",
       "  1.8031065,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  43000,\n",
       "  1.8031045,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  43100,\n",
       "  1.8031021,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  43200,\n",
       "  1.8030996,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  43300,\n",
       "  1.803097,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  43400,\n",
       "  1.8030946,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  43500,\n",
       "  1.8030926,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  43600,\n",
       "  1.8030897,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  43700,\n",
       "  1.8030876,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  43800,\n",
       "  1.8030853,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  43900,\n",
       "  1.8030832,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  44000,\n",
       "  1.8030812,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  44100,\n",
       "  1.8030784,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  44200,\n",
       "  1.8030764,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  44300,\n",
       "  1.8030744,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  44400,\n",
       "  1.8030719,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  44500,\n",
       "  1.8030692,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  44600,\n",
       "  1.8030674,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  44700,\n",
       "  1.8030654,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  44800,\n",
       "  1.8030628,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  44900,\n",
       "  1.8030604,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  45000,\n",
       "  1.8030585,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  45100,\n",
       "  1.8030562,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  45200,\n",
       "  1.8030541,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  45300,\n",
       "  1.8030517,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  45400,\n",
       "  1.8030496,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  45500,\n",
       "  1.8030478,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  45600,\n",
       "  1.8030455,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  45700,\n",
       "  1.8030432,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  45800,\n",
       "  1.8030413,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  45900,\n",
       "  1.8030392,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  46000,\n",
       "  1.803037,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  46100,\n",
       "  1.8030351,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  46200,\n",
       "  1.8030329,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  46300,\n",
       "  1.8030305,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  46400,\n",
       "  1.8030283,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  46500,\n",
       "  1.8030264,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  46600,\n",
       "  1.8030246,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  46700,\n",
       "  1.8030225,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  46800,\n",
       "  1.8030202,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  46900,\n",
       "  1.8030187,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  47000,\n",
       "  1.8030164,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  47100,\n",
       "  1.8030149,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  47200,\n",
       "  1.8030124,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  47300,\n",
       "  1.8030105,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  47400,\n",
       "  1.8030086,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  47500,\n",
       "  1.8030065,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  47600,\n",
       "  1.8030045,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  47700,\n",
       "  1.8030027,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  47800,\n",
       "  1.8030007,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  47900,\n",
       "  1.8029987,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  48000,\n",
       "  1.8029968,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  48100,\n",
       "  1.8029951,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  48200,\n",
       "  1.8029931,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  48300,\n",
       "  1.802991,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  48400,\n",
       "  1.8029896,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  48500,\n",
       "  1.8029871,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  48600,\n",
       "  1.8029854,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  48700,\n",
       "  1.8029833,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  48800,\n",
       "  1.8029816,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  48900,\n",
       "  1.8029792,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  49000,\n",
       "  1.8029778,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  49100,\n",
       "  1.802976,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  49200,\n",
       "  1.8029742,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  49300,\n",
       "  1.802973,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  49400,\n",
       "  1.8029709,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  49500,\n",
       "  1.8029687,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  49600,\n",
       "  1.8029671,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  49700,\n",
       "  1.8029655,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  49800,\n",
       "  1.8029639,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  49900,\n",
       "  1.8029618,\n",
       "  0.36707452],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  2900,\n",
       "  1.8268178,\n",
       "  0.36596218],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  3000,\n",
       "  1.8258014,\n",
       "  0.36596218],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  4800,\n",
       "  1.8157166,\n",
       "  0.36596218],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  4900,\n",
       "  1.8153985,\n",
       "  0.36596218],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  5000,\n",
       "  1.8150945,\n",
       "  0.36596218],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  5100,\n",
       "  1.8148028,\n",
       "  0.36596218],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  9400,\n",
       "  1.8082112,\n",
       "  0.36596218],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  2600,\n",
       "  1.8304677,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  2700,\n",
       "  1.8291373,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  2800,\n",
       "  1.8279259,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  3100,\n",
       "  1.8248651,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  3200,\n",
       "  1.8240004,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  3300,\n",
       "  1.8231993,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  3400,\n",
       "  1.8224547,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  3500,\n",
       "  1.8217615,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  3600,\n",
       "  1.8211145,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  3800,\n",
       "  1.8199404,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  4600,\n",
       "  1.8163984,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  4700,\n",
       "  1.8160495,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  5200,\n",
       "  1.8145232,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  6700,\n",
       "  1.8113564,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  6800,\n",
       "  1.8111956,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  6900,\n",
       "  1.8110391,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  7000,\n",
       "  1.8108873,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  7100,\n",
       "  1.81074,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  7200,\n",
       "  1.8105963,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  7300,\n",
       "  1.8104563,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  7400,\n",
       "  1.8103205,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  9200,\n",
       "  1.8083805,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  9300,\n",
       "  1.8082949,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  9500,\n",
       "  1.8081297,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  9600,\n",
       "  1.8080494,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  9700,\n",
       "  1.8079716,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  9800,\n",
       "  1.8078952,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  9900,\n",
       "  1.8078196,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  10000,\n",
       "  1.8077466,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  10100,\n",
       "  1.8076748,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  10200,\n",
       "  1.8076044,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  10300,\n",
       "  1.8075355,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  10400,\n",
       "  1.8074678,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  10500,\n",
       "  1.8074018,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  10600,\n",
       "  1.8073375,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  10700,\n",
       "  1.8072742,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  10800,\n",
       "  1.8072119,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  10900,\n",
       "  1.807151,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  11000,\n",
       "  1.8070909,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  11100,\n",
       "  1.8070326,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  11200,\n",
       "  1.8069754,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  11300,\n",
       "  1.8069191,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  11500,\n",
       "  1.8068101,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  11600,\n",
       "  1.8067567,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  11700,\n",
       "  1.8067048,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  11800,\n",
       "  1.8066541,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  11900,\n",
       "  1.8066036,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  12000,\n",
       "  1.8065548,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  12100,\n",
       "  1.806506,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  12200,\n",
       "  1.8064588,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  12300,\n",
       "  1.8064131,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  12400,\n",
       "  1.806367,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  12500,\n",
       "  1.8063223,\n",
       "  0.36484984],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  3700,\n",
       "  1.8205087,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  3900,\n",
       "  1.8194056,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  4100,\n",
       "  1.818427,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  4300,\n",
       "  1.8175521,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  4400,\n",
       "  1.8171475,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  4500,\n",
       "  1.8167636,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  5300,\n",
       "  1.8142545,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  6200,\n",
       "  1.8122395,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  6300,\n",
       "  1.8120517,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  6400,\n",
       "  1.8118694,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  6500,\n",
       "  1.8116933,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  6600,\n",
       "  1.8115225,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  7500,\n",
       "  1.8101885,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  7600,\n",
       "  1.8100597,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  7700,\n",
       "  1.8099337,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  8200,\n",
       "  1.8093526,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  8300,\n",
       "  1.8092445,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  8400,\n",
       "  1.8091393,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  8500,\n",
       "  1.8090361,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  8600,\n",
       "  1.8089364,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  8700,\n",
       "  1.8088379,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  8800,\n",
       "  1.8087419,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  8900,\n",
       "  1.8086487,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  9000,\n",
       "  1.8085573,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  9100,\n",
       "  1.8084677,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  11400,\n",
       "  1.8068641,\n",
       "  0.36373749],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  2300,\n",
       "  1.8353591,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  2500,\n",
       "  1.831933,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  4000,\n",
       "  1.8189021,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  5400,\n",
       "  1.8139961,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  5500,\n",
       "  1.8137479,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  5600,\n",
       "  1.8135087,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  6100,\n",
       "  1.812433,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  7800,\n",
       "  1.8098117,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  7900,\n",
       "  1.8096925,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  8000,\n",
       "  1.8095759,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  8100,\n",
       "  1.8094628,\n",
       "  0.36262515],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  2400,\n",
       "  1.8335556,\n",
       "  0.36151278],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  4200,\n",
       "  1.8179777,\n",
       "  0.36151278],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  5700,\n",
       "  1.813278,\n",
       "  0.36151278],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  5800,\n",
       "  1.8130562,\n",
       "  0.36151278],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  5900,\n",
       "  1.8128414,\n",
       "  0.36151278],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  6000,\n",
       "  1.8126338,\n",
       "  0.36151278],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  2200,\n",
       "  1.8373749,\n",
       "  0.36040044],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  2100,\n",
       "  1.8396388,\n",
       "  0.3592881],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  1700,\n",
       "  1.8522638,\n",
       "  0.35817575],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  1600,\n",
       "  1.8567168,\n",
       "  0.35706341],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  1800,\n",
       "  1.8484299,\n",
       "  0.35706341],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  1900,\n",
       "  1.8451034,\n",
       "  0.35706341],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  2000,\n",
       "  1.8421959,\n",
       "  0.35595107],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  1500,\n",
       "  1.8619319,\n",
       "  0.3548387],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  1400,\n",
       "  1.8680986,\n",
       "  0.35261402],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  1300,\n",
       "  1.8754704,\n",
       "  0.35038933],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  1200,\n",
       "  1.884406,\n",
       "  0.34927696],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  1100,\n",
       "  1.895407,\n",
       "  0.34482759],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  1000,\n",
       "  1.9091349,\n",
       "  0.34149054],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  900,\n",
       "  1.9264174,\n",
       "  0.32925472],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  800,\n",
       "  1.9483397,\n",
       "  0.32703003],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  700,\n",
       "  1.9765173,\n",
       "  0.31924361],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  600,\n",
       "  2.0135338,\n",
       "  0.30812013],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  500,\n",
       "  2.0634892,\n",
       "  0.30700779],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  400,\n",
       "  2.1328132,\n",
       "  0.29477197],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  300,\n",
       "  2.2333074,\n",
       "  0.27030033],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  200,\n",
       "  2.3950171,\n",
       "  0.23692992],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  100,\n",
       "  2.7298045,\n",
       "  0.23581758],\n",
       " [<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>,\n",
       "  0,\n",
       "  12.36657,\n",
       "  0.083426028]]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Best Result에 대해 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<tf.Tensor 'Softmax:0' shape=(?, 11) dtype=float32>, 15900, 1.8051865, 0.37041157]]\n"
     ]
    }
   ],
   "source": [
    "best_sm_result = sm_result[:1]\n",
    "print(best_sm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 63)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set의 예측한 Y 값을 보여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 3 \n",
      "Prediction: 3 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 3 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 3 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 3 \n",
      "Prediction: 3 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 3 \n",
      "Prediction: 4 \n",
      "Prediction: 3 \n",
      "Prediction: 3 \n",
      "Prediction: 4 \n",
      "Prediction: 3 \n",
      "Prediction: 4 \n",
      "Prediction: 3 \n",
      "Prediction: 3 \n",
      "Prediction: 3 \n",
      "Prediction: 4 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 3 \n",
      "Prediction: 4 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 3 \n",
      "Prediction: 3 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 1 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 1 \n",
      "Prediction: 4 \n",
      "Prediction: 1 \n",
      "Prediction: 4 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 4 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 4 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 3 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 4 \n",
      "Prediction: 3 \n",
      "Prediction: 1 \n",
      "Prediction: 3 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 3 \n",
      "Prediction: 3 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 3 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n",
      "Prediction: 1 \n"
     ]
    }
   ],
   "source": [
    "test_X = np.loadtxt('test_X.csv', delimiter=',', dtype=np.float32)\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: test_X})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p in pred:\n",
    "        print(\"Prediction: {} \".format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Keras + Relu + SoftMax\n",
    "\n",
    "Description for `Keras + Relu + SoftMax`: \n",
    "[click](https://keras.io/)\n",
    "[click](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)\n",
    "[click](https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/)\n",
    "\n",
    "Keras를 통해 고속 구현을 하고  Relu Activation Function을 통해 hidden layer에서 값을 찾아내 RNN을 구현한 후 SoftMax로 결과값을 처리해준 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "1\n",
    "2\n",
    "3\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"new_data.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:15].astype(float)\n",
    "Y = dataset[:,15]\n",
    "dataframe2 = pandas.read_csv(\"test_X.csv\", header = None)\n",
    "dataset2 = dataframe2.values\n",
    "test_x = dataset2[:,0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(11, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 31.60% (4.97%)\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=15, activation='relu'))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "899/899 [==============================] - 0s - loss: 2.3226 - acc: 0.2581     \n",
      "Epoch 2/1000\n",
      "899/899 [==============================] - 0s - loss: 2.0894 - acc: 0.3103     \n",
      "Epoch 3/1000\n",
      "899/899 [==============================] - 0s - loss: 2.0464 - acc: 0.3148     \n",
      "Epoch 4/1000\n",
      "899/899 [==============================] - 0s - loss: 2.0059 - acc: 0.3159     \n",
      "Epoch 5/1000\n",
      "899/899 [==============================] - 0s - loss: 1.9787 - acc: 0.3270     \n",
      "Epoch 6/1000\n",
      "899/899 [==============================] - 0s - loss: 1.9583 - acc: 0.3370     \n",
      "Epoch 7/1000\n",
      "899/899 [==============================] - 0s - loss: 1.9408 - acc: 0.3248     \n",
      "Epoch 8/1000\n",
      "899/899 [==============================] - 0s - loss: 1.9209 - acc: 0.3370     \n",
      "Epoch 9/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8985 - acc: 0.3404     \n",
      "Epoch 10/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8864 - acc: 0.3426     \n",
      "Epoch 11/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8758 - acc: 0.3482     \n",
      "Epoch 12/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8605 - acc: 0.3571     \n",
      "Epoch 13/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8519 - acc: 0.3604     \n",
      "Epoch 14/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8632 - acc: 0.3693     \n",
      "Epoch 15/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8453 - acc: 0.3526     \n",
      "Epoch 16/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8394 - acc: 0.3648     \n",
      "Epoch 17/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8388 - acc: 0.3615     \n",
      "Epoch 18/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8318 - acc: 0.3626     \n",
      "Epoch 19/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8226 - acc: 0.3715     \n",
      "Epoch 20/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8168 - acc: 0.3682     \n",
      "Epoch 21/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8076 - acc: 0.3782     \n",
      "Epoch 22/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8019 - acc: 0.3760     \n",
      "Epoch 23/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8054 - acc: 0.3660     \n",
      "Epoch 24/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8016 - acc: 0.3726     \n",
      "Epoch 25/1000\n",
      "899/899 [==============================] - 0s - loss: 1.8009 - acc: 0.3637     \n",
      "Epoch 26/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7883 - acc: 0.3737     \n",
      "Epoch 27/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7977 - acc: 0.3793     \n",
      "Epoch 28/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7855 - acc: 0.3726     \n",
      "Epoch 29/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7830 - acc: 0.3726     \n",
      "Epoch 30/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7784 - acc: 0.3737     \n",
      "Epoch 31/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7831 - acc: 0.3660     \n",
      "Epoch 32/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7674 - acc: 0.3826     \n",
      "Epoch 33/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7659 - acc: 0.3749     \n",
      "Epoch 34/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7638 - acc: 0.3793     \n",
      "Epoch 35/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7732 - acc: 0.3838     \n",
      "Epoch 36/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7570 - acc: 0.3882     \n",
      "Epoch 37/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7515 - acc: 0.3815     \n",
      "Epoch 38/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7531 - acc: 0.3860     \n",
      "Epoch 39/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7527 - acc: 0.3927     \n",
      "Epoch 40/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7500 - acc: 0.3982     \n",
      "Epoch 41/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7538 - acc: 0.3860     \n",
      "Epoch 42/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7511 - acc: 0.3904     \n",
      "Epoch 43/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7366 - acc: 0.3982     \n",
      "Epoch 44/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7331 - acc: 0.3915     \n",
      "Epoch 45/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7366 - acc: 0.3893     \n",
      "Epoch 46/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7266 - acc: 0.3971     \n",
      "Epoch 47/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7217 - acc: 0.3971     \n",
      "Epoch 48/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7238 - acc: 0.3982     \n",
      "Epoch 49/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7204 - acc: 0.3882     \n",
      "Epoch 50/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7183 - acc: 0.3971     \n",
      "Epoch 51/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7162 - acc: 0.3915     \n",
      "Epoch 52/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7120 - acc: 0.3904     \n",
      "Epoch 53/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7150 - acc: 0.3971     \n",
      "Epoch 54/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7049 - acc: 0.3938     \n",
      "Epoch 55/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7034 - acc: 0.3971     \n",
      "Epoch 56/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7017 - acc: 0.4060     \n",
      "Epoch 57/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7070 - acc: 0.3904     \n",
      "Epoch 58/1000\n",
      "899/899 [==============================] - 0s - loss: 1.7004 - acc: 0.4060     \n",
      "Epoch 59/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6964 - acc: 0.3993     \n",
      "Epoch 60/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6913 - acc: 0.4060     \n",
      "Epoch 61/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6911 - acc: 0.4049     \n",
      "Epoch 62/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6932 - acc: 0.3904     \n",
      "Epoch 63/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6819 - acc: 0.4116     \n",
      "Epoch 64/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6775 - acc: 0.4105     \n",
      "Epoch 65/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6767 - acc: 0.4127     \n",
      "Epoch 66/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6759 - acc: 0.4016     \n",
      "Epoch 67/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6697 - acc: 0.4171     \n",
      "Epoch 68/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6735 - acc: 0.3993     \n",
      "Epoch 69/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6686 - acc: 0.4305     \n",
      "Epoch 70/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6634 - acc: 0.4227     \n",
      "Epoch 71/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6719 - acc: 0.4160     \n",
      "Epoch 72/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6542 - acc: 0.4294     \n",
      "Epoch 73/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6622 - acc: 0.4271     \n",
      "Epoch 74/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6616 - acc: 0.4127     \n",
      "Epoch 75/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6624 - acc: 0.4071     \n",
      "Epoch 76/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6527 - acc: 0.4249     \n",
      "Epoch 77/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6509 - acc: 0.4294     \n",
      "Epoch 78/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6508 - acc: 0.4249     \n",
      "Epoch 79/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6417 - acc: 0.4238     \n",
      "Epoch 80/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6436 - acc: 0.4216     \n",
      "Epoch 81/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6394 - acc: 0.4327     \n",
      "Epoch 82/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6365 - acc: 0.4349     \n",
      "Epoch 83/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6376 - acc: 0.4127     \n",
      "Epoch 84/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6305 - acc: 0.4205     \n",
      "Epoch 85/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6409 - acc: 0.4194     \n",
      "Epoch 86/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6351 - acc: 0.4205     \n",
      "Epoch 87/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6341 - acc: 0.4283     \n",
      "Epoch 88/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6273 - acc: 0.4394     \n",
      "Epoch 89/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6238 - acc: 0.4394     \n",
      "Epoch 90/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6175 - acc: 0.4238     \n",
      "Epoch 91/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6192 - acc: 0.4427     \n",
      "Epoch 92/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6139 - acc: 0.4416     \n",
      "Epoch 93/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6142 - acc: 0.4405     \n",
      "Epoch 94/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6089 - acc: 0.4449     \n",
      "Epoch 95/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6082 - acc: 0.4405     \n",
      "Epoch 96/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6100 - acc: 0.4549     \n",
      "Epoch 97/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6097 - acc: 0.4483     \n",
      "Epoch 98/1000\n",
      "899/899 [==============================] - 0s - loss: 1.6019 - acc: 0.4449     \n",
      "Epoch 99/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5976 - acc: 0.4316     \n",
      "Epoch 100/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5935 - acc: 0.4483     \n",
      "Epoch 101/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5948 - acc: 0.4416     \n",
      "Epoch 102/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5840 - acc: 0.4461     \n",
      "Epoch 103/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5826 - acc: 0.4472     \n",
      "Epoch 104/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5773 - acc: 0.4505     \n",
      "Epoch 105/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5849 - acc: 0.4394     \n",
      "Epoch 106/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5877 - acc: 0.4472     \n",
      "Epoch 107/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5822 - acc: 0.4327     \n",
      "Epoch 108/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5756 - acc: 0.4461     \n",
      "Epoch 109/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5751 - acc: 0.4438     \n",
      "Epoch 110/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5756 - acc: 0.4427     \n",
      "Epoch 111/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5698 - acc: 0.4449     \n",
      "Epoch 112/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5670 - acc: 0.4483     \n",
      "Epoch 113/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5696 - acc: 0.4461     \n",
      "Epoch 114/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5619 - acc: 0.4583     \n",
      "Epoch 115/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5578 - acc: 0.4516     \n",
      "Epoch 116/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5637 - acc: 0.4516     \n",
      "Epoch 117/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5539 - acc: 0.4583     \n",
      "Epoch 118/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5514 - acc: 0.4494     \n",
      "Epoch 119/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5530 - acc: 0.4538     \n",
      "Epoch 120/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5495 - acc: 0.4505     \n",
      "Epoch 121/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5506 - acc: 0.4494     \n",
      "Epoch 122/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5471 - acc: 0.4705     \n",
      "Epoch 123/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5472 - acc: 0.4527     \n",
      "Epoch 124/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5441 - acc: 0.4561     \n",
      "Epoch 125/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5410 - acc: 0.4672     \n",
      "Epoch 126/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5304 - acc: 0.4705     \n",
      "Epoch 127/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5375 - acc: 0.4516     \n",
      "Epoch 128/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5316 - acc: 0.4549     \n",
      "Epoch 129/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5331 - acc: 0.4616     \n",
      "Epoch 130/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5275 - acc: 0.4650     \n",
      "Epoch 131/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5309 - acc: 0.4427     \n",
      "Epoch 132/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5181 - acc: 0.4627     \n",
      "Epoch 133/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5211 - acc: 0.4739     \n",
      "Epoch 134/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5146 - acc: 0.4783     \n",
      "Epoch 135/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5118 - acc: 0.4694     \n",
      "Epoch 136/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5120 - acc: 0.4705     \n",
      "Epoch 137/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5095 - acc: 0.4716     \n",
      "Epoch 138/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5127 - acc: 0.4694     \n",
      "Epoch 139/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5190 - acc: 0.4594     \n",
      "Epoch 140/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5138 - acc: 0.4750     \n",
      "Epoch 141/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5031 - acc: 0.4661     \n",
      "Epoch 142/1000\n",
      "899/899 [==============================] - 0s - loss: 1.5074 - acc: 0.4794     \n",
      "Epoch 143/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4968 - acc: 0.4716     \n",
      "Epoch 144/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4951 - acc: 0.4705     \n",
      "Epoch 145/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4986 - acc: 0.4727     \n",
      "Epoch 146/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4897 - acc: 0.4794     \n",
      "Epoch 147/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4992 - acc: 0.4739     \n",
      "Epoch 148/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4832 - acc: 0.4805     \n",
      "Epoch 149/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4851 - acc: 0.4705     \n",
      "Epoch 150/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4804 - acc: 0.4816     \n",
      "Epoch 151/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4816 - acc: 0.4772     \n",
      "Epoch 152/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4742 - acc: 0.4794     \n",
      "Epoch 153/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4780 - acc: 0.4661     \n",
      "Epoch 154/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4737 - acc: 0.4772     \n",
      "Epoch 155/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4774 - acc: 0.4739     \n",
      "Epoch 156/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4685 - acc: 0.4805     \n",
      "Epoch 157/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4669 - acc: 0.4905     \n",
      "Epoch 158/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4649 - acc: 0.4872     \n",
      "Epoch 159/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4642 - acc: 0.4861     \n",
      "Epoch 160/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4621 - acc: 0.4816     \n",
      "Epoch 161/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4635 - acc: 0.4805     \n",
      "Epoch 162/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4655 - acc: 0.4928     \n",
      "Epoch 163/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4565 - acc: 0.4850     \n",
      "Epoch 164/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4595 - acc: 0.4761     \n",
      "Epoch 165/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4473 - acc: 0.4839     \n",
      "Epoch 166/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4661 - acc: 0.4672     \n",
      "Epoch 167/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4492 - acc: 0.4861     \n",
      "Epoch 168/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4481 - acc: 0.4850     \n",
      "Epoch 169/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4511 - acc: 0.4805     \n",
      "Epoch 170/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4451 - acc: 0.4750     \n",
      "Epoch 171/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4472 - acc: 0.4861     \n",
      "Epoch 172/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4413 - acc: 0.4805     \n",
      "Epoch 173/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4298 - acc: 0.4950     \n",
      "Epoch 174/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 0s - loss: 1.4298 - acc: 0.4961     \n",
      "Epoch 175/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4263 - acc: 0.5072     \n",
      "Epoch 176/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4291 - acc: 0.5028     \n",
      "Epoch 177/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4300 - acc: 0.4950     \n",
      "Epoch 178/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4251 - acc: 0.4939     \n",
      "Epoch 179/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4297 - acc: 0.4950     \n",
      "Epoch 180/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4213 - acc: 0.4894     \n",
      "Epoch 181/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4384 - acc: 0.4805     \n",
      "Epoch 182/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4225 - acc: 0.4905     \n",
      "Epoch 183/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4185 - acc: 0.4928     \n",
      "Epoch 184/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4151 - acc: 0.4950     \n",
      "Epoch 185/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4124 - acc: 0.4905     \n",
      "Epoch 186/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4121 - acc: 0.5006     \n",
      "Epoch 187/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4093 - acc: 0.5006     \n",
      "Epoch 188/1000\n",
      "899/899 [==============================] - ETA: 0s - loss: 1.5463 - acc: 0.375 - 0s - loss: 1.3990 - acc: 0.5161     \n",
      "Epoch 189/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4030 - acc: 0.4972     \n",
      "Epoch 190/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3988 - acc: 0.5006     \n",
      "Epoch 191/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3983 - acc: 0.5083     \n",
      "Epoch 192/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3982 - acc: 0.4972     \n",
      "Epoch 193/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4049 - acc: 0.5195     \n",
      "Epoch 194/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3995 - acc: 0.5117     \n",
      "Epoch 195/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3976 - acc: 0.5039     \n",
      "Epoch 196/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3989 - acc: 0.5028     \n",
      "Epoch 197/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3979 - acc: 0.4961     \n",
      "Epoch 198/1000\n",
      "899/899 [==============================] - 0s - loss: 1.4107 - acc: 0.5006     \n",
      "Epoch 199/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3925 - acc: 0.5017     \n",
      "Epoch 200/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3856 - acc: 0.5039     \n",
      "Epoch 201/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3859 - acc: 0.5061     \n",
      "Epoch 202/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3927 - acc: 0.5072     \n",
      "Epoch 203/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3854 - acc: 0.5106     \n",
      "Epoch 204/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3789 - acc: 0.5206     \n",
      "Epoch 205/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3767 - acc: 0.5117     \n",
      "Epoch 206/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3804 - acc: 0.5117     \n",
      "Epoch 207/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3894 - acc: 0.5017     \n",
      "Epoch 208/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3769 - acc: 0.4994     \n",
      "Epoch 209/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3674 - acc: 0.5128     \n",
      "Epoch 210/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3701 - acc: 0.5206     \n",
      "Epoch 211/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3694 - acc: 0.5150     \n",
      "Epoch 212/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3693 - acc: 0.5006     \n",
      "Epoch 213/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3711 - acc: 0.5072     \n",
      "Epoch 214/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3661 - acc: 0.5039     \n",
      "Epoch 215/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3565 - acc: 0.5217     \n",
      "Epoch 216/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3702 - acc: 0.5172     \n",
      "Epoch 217/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3565 - acc: 0.5050     \n",
      "Epoch 218/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3641 - acc: 0.5139     \n",
      "Epoch 219/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3562 - acc: 0.5161     \n",
      "Epoch 220/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3547 - acc: 0.5195     \n",
      "Epoch 221/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3563 - acc: 0.5172     \n",
      "Epoch 222/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3608 - acc: 0.5228     \n",
      "Epoch 223/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3511 - acc: 0.5206     \n",
      "Epoch 224/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3471 - acc: 0.5128     \n",
      "Epoch 225/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3486 - acc: 0.5273     \n",
      "Epoch 226/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3427 - acc: 0.5184     \n",
      "Epoch 227/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3419 - acc: 0.5095     \n",
      "Epoch 228/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3458 - acc: 0.5317     \n",
      "Epoch 229/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3529 - acc: 0.5195     \n",
      "Epoch 230/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3397 - acc: 0.5384     \n",
      "Epoch 231/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3394 - acc: 0.5161     \n",
      "Epoch 232/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3435 - acc: 0.5083     \n",
      "Epoch 233/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3329 - acc: 0.5317     \n",
      "Epoch 234/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3446 - acc: 0.5128     \n",
      "Epoch 235/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3313 - acc: 0.5428     \n",
      "Epoch 236/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3262 - acc: 0.5295     \n",
      "Epoch 237/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3278 - acc: 0.5339     \n",
      "Epoch 238/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3327 - acc: 0.5295     \n",
      "Epoch 239/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3203 - acc: 0.5339     \n",
      "Epoch 240/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3281 - acc: 0.5228     \n",
      "Epoch 241/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3284 - acc: 0.5406     \n",
      "Epoch 242/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3259 - acc: 0.5284     \n",
      "Epoch 243/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3178 - acc: 0.5384     \n",
      "Epoch 244/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3220 - acc: 0.5328     \n",
      "Epoch 245/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3275 - acc: 0.5217     \n",
      "Epoch 246/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3154 - acc: 0.5384     \n",
      "Epoch 247/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3200 - acc: 0.5228     \n",
      "Epoch 248/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3082 - acc: 0.5373     \n",
      "Epoch 249/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3249 - acc: 0.5328     \n",
      "Epoch 250/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3266 - acc: 0.5395     \n",
      "Epoch 251/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3090 - acc: 0.5306     \n",
      "Epoch 252/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3081 - acc: 0.5406     \n",
      "Epoch 253/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3204 - acc: 0.5150     \n",
      "Epoch 254/1000\n",
      "899/899 [==============================] - 0s - loss: 1.3004 - acc: 0.5306     \n",
      "Epoch 255/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2997 - acc: 0.5406     \n",
      "Epoch 256/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2964 - acc: 0.5328     \n",
      "Epoch 257/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2991 - acc: 0.5428     \n",
      "Epoch 258/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2920 - acc: 0.5551     \n",
      "Epoch 259/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2974 - acc: 0.5417     \n",
      "Epoch 260/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 0s - loss: 1.2961 - acc: 0.5528     \n",
      "Epoch 261/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2950 - acc: 0.5428     \n",
      "Epoch 262/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2926 - acc: 0.5462     \n",
      "Epoch 263/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2945 - acc: 0.5373     \n",
      "Epoch 264/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2912 - acc: 0.5439     \n",
      "Epoch 265/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2982 - acc: 0.5362     \n",
      "Epoch 266/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2861 - acc: 0.5473     \n",
      "Epoch 267/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2861 - acc: 0.5451     \n",
      "Epoch 268/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2854 - acc: 0.5395     \n",
      "Epoch 269/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2801 - acc: 0.5573     \n",
      "Epoch 270/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2817 - acc: 0.5462     \n",
      "Epoch 271/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2880 - acc: 0.5473     \n",
      "Epoch 272/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2887 - acc: 0.5506     \n",
      "Epoch 273/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2771 - acc: 0.5484     \n",
      "Epoch 274/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2875 - acc: 0.5384     \n",
      "Epoch 275/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2792 - acc: 0.5517     \n",
      "Epoch 276/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2755 - acc: 0.5517     \n",
      "Epoch 277/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2781 - acc: 0.5462     \n",
      "Epoch 278/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2717 - acc: 0.5551     \n",
      "Epoch 279/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2668 - acc: 0.5606     \n",
      "Epoch 280/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2701 - acc: 0.5684     \n",
      "Epoch 281/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2739 - acc: 0.5517     \n",
      "Epoch 282/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2644 - acc: 0.5528     \n",
      "Epoch 283/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2660 - acc: 0.5595     \n",
      "Epoch 284/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2691 - acc: 0.5551     \n",
      "Epoch 285/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2703 - acc: 0.5617     \n",
      "Epoch 286/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2660 - acc: 0.5473     \n",
      "Epoch 287/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2673 - acc: 0.5439     \n",
      "Epoch 288/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2592 - acc: 0.5562     \n",
      "Epoch 289/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2575 - acc: 0.5495     \n",
      "Epoch 290/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2568 - acc: 0.5584     \n",
      "Epoch 291/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2479 - acc: 0.5684     \n",
      "Epoch 292/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2489 - acc: 0.5617     \n",
      "Epoch 293/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2529 - acc: 0.5539     \n",
      "Epoch 294/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2529 - acc: 0.5495     \n",
      "Epoch 295/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2573 - acc: 0.5406     \n",
      "Epoch 296/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2489 - acc: 0.5706     \n",
      "Epoch 297/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2686 - acc: 0.5573     \n",
      "Epoch 298/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2444 - acc: 0.5573     \n",
      "Epoch 299/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2388 - acc: 0.5717     \n",
      "Epoch 300/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2482 - acc: 0.5628     \n",
      "Epoch 301/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2391 - acc: 0.5684     \n",
      "Epoch 302/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2429 - acc: 0.5640     \n",
      "Epoch 303/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2359 - acc: 0.5606     \n",
      "Epoch 304/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2393 - acc: 0.5506     \n",
      "Epoch 305/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2417 - acc: 0.5684     \n",
      "Epoch 306/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2372 - acc: 0.5573     \n",
      "Epoch 307/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2295 - acc: 0.5706     \n",
      "Epoch 308/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2328 - acc: 0.5695     \n",
      "Epoch 309/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2358 - acc: 0.5673     \n",
      "Epoch 310/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2391 - acc: 0.5673     \n",
      "Epoch 311/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2326 - acc: 0.5617     \n",
      "Epoch 312/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2252 - acc: 0.5662     \n",
      "Epoch 313/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2217 - acc: 0.5717     \n",
      "Epoch 314/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2278 - acc: 0.5695     \n",
      "Epoch 315/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2397 - acc: 0.5551     \n",
      "Epoch 316/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2138 - acc: 0.5684     \n",
      "Epoch 317/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2203 - acc: 0.5684     \n",
      "Epoch 318/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2146 - acc: 0.5684     \n",
      "Epoch 319/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2127 - acc: 0.5673     \n",
      "Epoch 320/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2138 - acc: 0.5695     \n",
      "Epoch 321/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2183 - acc: 0.5684     \n",
      "Epoch 322/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2161 - acc: 0.5706     \n",
      "Epoch 323/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2135 - acc: 0.5806     \n",
      "Epoch 324/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2154 - acc: 0.5717     \n",
      "Epoch 325/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2024 - acc: 0.5806     \n",
      "Epoch 326/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2063 - acc: 0.5773     \n",
      "Epoch 327/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2118 - acc: 0.5773     \n",
      "Epoch 328/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2023 - acc: 0.5806     \n",
      "Epoch 329/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2110 - acc: 0.5662     \n",
      "Epoch 330/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2010 - acc: 0.5829     \n",
      "Epoch 331/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2095 - acc: 0.5662     \n",
      "Epoch 332/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1992 - acc: 0.5907     \n",
      "Epoch 333/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2151 - acc: 0.5784     \n",
      "Epoch 334/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1999 - acc: 0.5762     \n",
      "Epoch 335/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1981 - acc: 0.5806     \n",
      "Epoch 336/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2014 - acc: 0.5806     \n",
      "Epoch 337/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1897 - acc: 0.5918     \n",
      "Epoch 338/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1986 - acc: 0.5751     \n",
      "Epoch 339/1000\n",
      "899/899 [==============================] - 0s - loss: 1.2025 - acc: 0.5806     \n",
      "Epoch 340/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1837 - acc: 0.5873     \n",
      "Epoch 341/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1920 - acc: 0.5729     \n",
      "Epoch 342/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1913 - acc: 0.5873     \n",
      "Epoch 343/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1933 - acc: 0.5829     \n",
      "Epoch 344/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1884 - acc: 0.5873     \n",
      "Epoch 345/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1861 - acc: 0.5773     \n",
      "Epoch 346/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1847 - acc: 0.5840     \n",
      "Epoch 347/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1775 - acc: 0.5907     \n",
      "Epoch 348/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1813 - acc: 0.5840     \n",
      "Epoch 349/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1872 - acc: 0.5884     \n",
      "Epoch 350/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1794 - acc: 0.5829     \n",
      "Epoch 351/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1783 - acc: 0.5862     \n",
      "Epoch 352/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1803 - acc: 0.5751     \n",
      "Epoch 353/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1791 - acc: 0.5818     \n",
      "Epoch 354/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1728 - acc: 0.6007     \n",
      "Epoch 355/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1755 - acc: 0.5929     \n",
      "Epoch 356/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1722 - acc: 0.5951     \n",
      "Epoch 357/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1728 - acc: 0.5829     \n",
      "Epoch 358/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1721 - acc: 0.5996     \n",
      "Epoch 359/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1779 - acc: 0.5751     \n",
      "Epoch 360/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1791 - acc: 0.5829     \n",
      "Epoch 361/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1654 - acc: 0.5873     \n",
      "Epoch 362/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1759 - acc: 0.5940     \n",
      "Epoch 363/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1749 - acc: 0.5929     \n",
      "Epoch 364/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1668 - acc: 0.5862     \n",
      "Epoch 365/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1612 - acc: 0.5895     \n",
      "Epoch 366/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1585 - acc: 0.5962     \n",
      "Epoch 367/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1714 - acc: 0.5851     \n",
      "Epoch 368/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1710 - acc: 0.5840     \n",
      "Epoch 369/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1571 - acc: 0.5996     \n",
      "Epoch 370/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1659 - acc: 0.5873     \n",
      "Epoch 371/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1499 - acc: 0.6051     \n",
      "Epoch 372/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1590 - acc: 0.5895     \n",
      "Epoch 373/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1498 - acc: 0.5984     \n",
      "Epoch 374/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1532 - acc: 0.6073     \n",
      "Epoch 375/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1582 - acc: 0.5951     \n",
      "Epoch 376/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1523 - acc: 0.6040     \n",
      "Epoch 377/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1473 - acc: 0.6029     \n",
      "Epoch 378/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1458 - acc: 0.6062     \n",
      "Epoch 379/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1728 - acc: 0.5862     \n",
      "Epoch 380/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1496 - acc: 0.6029     \n",
      "Epoch 381/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1442 - acc: 0.5996     \n",
      "Epoch 382/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1440 - acc: 0.6007     \n",
      "Epoch 383/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1430 - acc: 0.5951     \n",
      "Epoch 384/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1402 - acc: 0.6051     \n",
      "Epoch 385/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1463 - acc: 0.5996     \n",
      "Epoch 386/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1362 - acc: 0.6062     \n",
      "Epoch 387/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1401 - acc: 0.6085     \n",
      "Epoch 388/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1418 - acc: 0.5984     \n",
      "Epoch 389/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1274 - acc: 0.6107     \n",
      "Epoch 390/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1372 - acc: 0.6096     \n",
      "Epoch 391/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1338 - acc: 0.5984     \n",
      "Epoch 392/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1347 - acc: 0.6029     \n",
      "Epoch 393/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1248 - acc: 0.5984     \n",
      "Epoch 394/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1384 - acc: 0.6029     \n",
      "Epoch 395/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1378 - acc: 0.5884     \n",
      "Epoch 396/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1224 - acc: 0.6040     \n",
      "Epoch 397/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1292 - acc: 0.6073     \n",
      "Epoch 398/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1253 - acc: 0.5884     \n",
      "Epoch 399/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1283 - acc: 0.6040     \n",
      "Epoch 400/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1258 - acc: 0.6029     \n",
      "Epoch 401/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1301 - acc: 0.5951     \n",
      "Epoch 402/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1253 - acc: 0.6118     \n",
      "Epoch 403/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1228 - acc: 0.6040     \n",
      "Epoch 404/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1213 - acc: 0.6118     \n",
      "Epoch 405/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1136 - acc: 0.6218     \n",
      "Epoch 406/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1271 - acc: 0.6018     \n",
      "Epoch 407/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1144 - acc: 0.6118     \n",
      "Epoch 408/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1225 - acc: 0.6107     \n",
      "Epoch 409/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1162 - acc: 0.6129     \n",
      "Epoch 410/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1138 - acc: 0.6073     \n",
      "Epoch 411/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1177 - acc: 0.6218     \n",
      "Epoch 412/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1158 - acc: 0.6107     \n",
      "Epoch 413/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1133 - acc: 0.6085     \n",
      "Epoch 414/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1047 - acc: 0.6140     \n",
      "Epoch 415/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1060 - acc: 0.6140     \n",
      "Epoch 416/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1104 - acc: 0.6140     \n",
      "Epoch 417/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1080 - acc: 0.6029     \n",
      "Epoch 418/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1056 - acc: 0.6162     \n",
      "Epoch 419/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1063 - acc: 0.6107     \n",
      "Epoch 420/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1126 - acc: 0.6029     \n",
      "Epoch 421/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1069 - acc: 0.5973     \n",
      "Epoch 422/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1022 - acc: 0.6207     \n",
      "Epoch 423/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1052 - acc: 0.6162     \n",
      "Epoch 424/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1065 - acc: 0.6129     \n",
      "Epoch 425/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0949 - acc: 0.6218     \n",
      "Epoch 426/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0987 - acc: 0.6129     \n",
      "Epoch 427/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1110 - acc: 0.6151     \n",
      "Epoch 428/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1002 - acc: 0.6251     \n",
      "Epoch 429/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0997 - acc: 0.6162     \n",
      "Epoch 430/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0910 - acc: 0.6151     \n",
      "Epoch 431/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0958 - acc: 0.6029     \n",
      "Epoch 432/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 0s - loss: 1.0897 - acc: 0.6174     \n",
      "Epoch 433/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1135 - acc: 0.6029     \n",
      "Epoch 434/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1004 - acc: 0.6162     \n",
      "Epoch 435/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0951 - acc: 0.6073     \n",
      "Epoch 436/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0909 - acc: 0.6251     \n",
      "Epoch 437/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0986 - acc: 0.6162     \n",
      "Epoch 438/1000\n",
      "899/899 [==============================] - 0s - loss: 1.1015 - acc: 0.6096     \n",
      "Epoch 439/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0878 - acc: 0.6251     \n",
      "Epoch 440/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0838 - acc: 0.6229     \n",
      "Epoch 441/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0795 - acc: 0.6229     \n",
      "Epoch 442/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0823 - acc: 0.6207     \n",
      "Epoch 443/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0874 - acc: 0.6151     \n",
      "Epoch 444/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0760 - acc: 0.6440     \n",
      "Epoch 445/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0803 - acc: 0.6218     \n",
      "Epoch 446/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0765 - acc: 0.6296     \n",
      "Epoch 447/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0922 - acc: 0.6140     \n",
      "Epoch 448/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0764 - acc: 0.6274     \n",
      "Epoch 449/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0758 - acc: 0.6307     \n",
      "Epoch 450/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0835 - acc: 0.6196     \n",
      "Epoch 451/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0709 - acc: 0.6285     \n",
      "Epoch 452/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0758 - acc: 0.6274     \n",
      "Epoch 453/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0724 - acc: 0.6352     \n",
      "Epoch 454/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0687 - acc: 0.6251     \n",
      "Epoch 455/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0670 - acc: 0.6363     \n",
      "Epoch 456/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0752 - acc: 0.6263     \n",
      "Epoch 457/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0665 - acc: 0.6274     \n",
      "Epoch 458/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0814 - acc: 0.6174     \n",
      "Epoch 459/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0707 - acc: 0.6296     \n",
      "Epoch 460/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0894 - acc: 0.6196     \n",
      "Epoch 461/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0579 - acc: 0.6285     \n",
      "Epoch 462/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0628 - acc: 0.6240     \n",
      "Epoch 463/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0867 - acc: 0.6085     \n",
      "Epoch 464/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0661 - acc: 0.6140     \n",
      "Epoch 465/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0619 - acc: 0.6285     \n",
      "Epoch 466/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0749 - acc: 0.6251     \n",
      "Epoch 467/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0523 - acc: 0.6229     \n",
      "Epoch 468/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0620 - acc: 0.6352     \n",
      "Epoch 469/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0538 - acc: 0.6374     \n",
      "Epoch 470/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0565 - acc: 0.6329     \n",
      "Epoch 471/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0555 - acc: 0.6307     \n",
      "Epoch 472/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0542 - acc: 0.6318     \n",
      "Epoch 473/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0541 - acc: 0.6240     \n",
      "Epoch 474/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0581 - acc: 0.6285     \n",
      "Epoch 475/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0516 - acc: 0.6229     \n",
      "Epoch 476/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0540 - acc: 0.6352     \n",
      "Epoch 477/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0598 - acc: 0.6307     \n",
      "Epoch 478/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0607 - acc: 0.6196     \n",
      "Epoch 479/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0471 - acc: 0.6396     \n",
      "Epoch 480/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0438 - acc: 0.6407     \n",
      "Epoch 481/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0415 - acc: 0.6318     \n",
      "Epoch 482/1000\n",
      "899/899 [==============================] - ETA: 0s - loss: 1.2645 - acc: 0.500 - 0s - loss: 1.0418 - acc: 0.6307     \n",
      "Epoch 483/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0452 - acc: 0.6363     \n",
      "Epoch 484/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0492 - acc: 0.6218     \n",
      "Epoch 485/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0527 - acc: 0.6329     \n",
      "Epoch 486/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0610 - acc: 0.6207     \n",
      "Epoch 487/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0545 - acc: 0.6363     \n",
      "Epoch 488/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0324 - acc: 0.6485     \n",
      "Epoch 489/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0445 - acc: 0.6307     \n",
      "Epoch 490/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0369 - acc: 0.6440     \n",
      "Epoch 491/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0314 - acc: 0.6452     \n",
      "Epoch 492/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0442 - acc: 0.6340     \n",
      "Epoch 493/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0403 - acc: 0.6318     \n",
      "Epoch 494/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0427 - acc: 0.6340     \n",
      "Epoch 495/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0330 - acc: 0.6307     \n",
      "Epoch 496/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0227 - acc: 0.6385     \n",
      "Epoch 497/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0414 - acc: 0.6318     \n",
      "Epoch 498/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0238 - acc: 0.6407     \n",
      "Epoch 499/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0294 - acc: 0.6374     \n",
      "Epoch 500/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0201 - acc: 0.6440     \n",
      "Epoch 501/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0175 - acc: 0.6385     \n",
      "Epoch 502/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0248 - acc: 0.6474     \n",
      "Epoch 503/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0266 - acc: 0.6318     \n",
      "Epoch 504/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0254 - acc: 0.6274     \n",
      "Epoch 505/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0211 - acc: 0.6429     \n",
      "Epoch 506/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0240 - acc: 0.6496     \n",
      "Epoch 507/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0192 - acc: 0.6407     \n",
      "Epoch 508/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0213 - acc: 0.6352     \n",
      "Epoch 509/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0134 - acc: 0.6496     \n",
      "Epoch 510/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0182 - acc: 0.6418     \n",
      "Epoch 511/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0110 - acc: 0.6529     \n",
      "Epoch 512/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0243 - acc: 0.6385     \n",
      "Epoch 513/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0101 - acc: 0.6607     \n",
      "Epoch 514/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0086 - acc: 0.6552     \n",
      "Epoch 515/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0142 - acc: 0.6507     \n",
      "Epoch 516/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0234 - acc: 0.6307     \n",
      "Epoch 517/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0085 - acc: 0.6474     \n",
      "Epoch 518/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 0s - loss: 1.0101 - acc: 0.6507     \n",
      "Epoch 519/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0032 - acc: 0.6407     \n",
      "Epoch 520/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0223 - acc: 0.6418     \n",
      "Epoch 521/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0157 - acc: 0.6418     \n",
      "Epoch 522/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0091 - acc: 0.6385     \n",
      "Epoch 523/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0217 - acc: 0.6440     \n",
      "Epoch 524/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0123 - acc: 0.6374     \n",
      "Epoch 525/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9953 - acc: 0.6440     \n",
      "Epoch 526/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9993 - acc: 0.6496     \n",
      "Epoch 527/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0067 - acc: 0.6352     \n",
      "Epoch 528/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0120 - acc: 0.6396     \n",
      "Epoch 529/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0076 - acc: 0.6463     \n",
      "Epoch 530/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9988 - acc: 0.6607     \n",
      "Epoch 531/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9957 - acc: 0.6607     \n",
      "Epoch 532/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9950 - acc: 0.6563     \n",
      "Epoch 533/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0000 - acc: 0.6463     \n",
      "Epoch 534/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9966 - acc: 0.6596     \n",
      "Epoch 535/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9960 - acc: 0.6563     \n",
      "Epoch 536/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9903 - acc: 0.6563     \n",
      "Epoch 537/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0033 - acc: 0.6440     \n",
      "Epoch 538/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9874 - acc: 0.6552     \n",
      "Epoch 539/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0042 - acc: 0.6429     \n",
      "Epoch 540/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9942 - acc: 0.6618     \n",
      "Epoch 541/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9893 - acc: 0.6463     \n",
      "Epoch 542/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9821 - acc: 0.6618     \n",
      "Epoch 543/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9971 - acc: 0.6440     \n",
      "Epoch 544/1000\n",
      "899/899 [==============================] - 0s - loss: 1.0023 - acc: 0.6463     \n",
      "Epoch 545/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9975 - acc: 0.6407     \n",
      "Epoch 546/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9847 - acc: 0.6685     \n",
      "Epoch 547/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9857 - acc: 0.6630     \n",
      "Epoch 548/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9944 - acc: 0.6485     \n",
      "Epoch 549/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9857 - acc: 0.6529     \n",
      "Epoch 550/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9803 - acc: 0.6630     \n",
      "Epoch 551/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9774 - acc: 0.6652     \n",
      "Epoch 552/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9766 - acc: 0.6663     \n",
      "Epoch 553/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9764 - acc: 0.6696     \n",
      "Epoch 554/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9812 - acc: 0.6774     \n",
      "Epoch 555/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9846 - acc: 0.6452     \n",
      "Epoch 556/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9858 - acc: 0.6440     \n",
      "Epoch 557/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9748 - acc: 0.6596     \n",
      "Epoch 558/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9832 - acc: 0.6429     \n",
      "Epoch 559/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9675 - acc: 0.6785     \n",
      "Epoch 560/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9903 - acc: 0.6529     \n",
      "Epoch 561/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9686 - acc: 0.6741     \n",
      "Epoch 562/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9780 - acc: 0.6541     \n",
      "Epoch 563/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9685 - acc: 0.6685     \n",
      "Epoch 564/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9675 - acc: 0.6641     \n",
      "Epoch 565/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9686 - acc: 0.6641     \n",
      "Epoch 566/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9702 - acc: 0.6641     \n",
      "Epoch 567/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9656 - acc: 0.6641     \n",
      "Epoch 568/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9659 - acc: 0.6585     \n",
      "Epoch 569/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9720 - acc: 0.6641     \n",
      "Epoch 570/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9773 - acc: 0.6529     \n",
      "Epoch 571/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9707 - acc: 0.6696     \n",
      "Epoch 572/1000\n",
      "899/899 [==============================] - ETA: 0s - loss: 1.0208 - acc: 0.656 - 0s - loss: 0.9575 - acc: 0.6607     \n",
      "Epoch 573/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9669 - acc: 0.6630     \n",
      "Epoch 574/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9581 - acc: 0.6607     \n",
      "Epoch 575/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9558 - acc: 0.6596     \n",
      "Epoch 576/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9580 - acc: 0.6607     \n",
      "Epoch 577/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9573 - acc: 0.6663     \n",
      "Epoch 578/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9731 - acc: 0.6518     \n",
      "Epoch 579/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9628 - acc: 0.6674     \n",
      "Epoch 580/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9568 - acc: 0.6641     \n",
      "Epoch 581/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9572 - acc: 0.6719     \n",
      "Epoch 582/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9587 - acc: 0.6474     \n",
      "Epoch 583/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9533 - acc: 0.6630     \n",
      "Epoch 584/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9462 - acc: 0.6763     \n",
      "Epoch 585/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9615 - acc: 0.6641     \n",
      "Epoch 586/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9524 - acc: 0.6719     \n",
      "Epoch 587/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9594 - acc: 0.6730     \n",
      "Epoch 588/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9567 - acc: 0.6674     \n",
      "Epoch 589/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9572 - acc: 0.6696     \n",
      "Epoch 590/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9485 - acc: 0.6785     \n",
      "Epoch 591/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9412 - acc: 0.6841     \n",
      "Epoch 592/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9451 - acc: 0.6552     \n",
      "Epoch 593/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9528 - acc: 0.6730     \n",
      "Epoch 594/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9463 - acc: 0.6752     \n",
      "Epoch 595/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9516 - acc: 0.6641     \n",
      "Epoch 596/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9456 - acc: 0.6841     \n",
      "Epoch 597/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9428 - acc: 0.6808     \n",
      "Epoch 598/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9488 - acc: 0.6652     \n",
      "Epoch 599/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9398 - acc: 0.6785     \n",
      "Epoch 600/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9390 - acc: 0.6908     \n",
      "Epoch 601/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9405 - acc: 0.6796     \n",
      "Epoch 602/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9355 - acc: 0.6841     \n",
      "Epoch 603/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9339 - acc: 0.6852     \n",
      "Epoch 604/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 0s - loss: 0.9305 - acc: 0.6852     \n",
      "Epoch 605/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9509 - acc: 0.6785     \n",
      "Epoch 606/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9302 - acc: 0.6785     \n",
      "Epoch 607/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9298 - acc: 0.6841     \n",
      "Epoch 608/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9447 - acc: 0.6763     \n",
      "Epoch 609/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9398 - acc: 0.6774     \n",
      "Epoch 610/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9441 - acc: 0.6897     \n",
      "Epoch 611/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9299 - acc: 0.6863     \n",
      "Epoch 612/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9247 - acc: 0.6830     \n",
      "Epoch 613/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9389 - acc: 0.6808     \n",
      "Epoch 614/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9360 - acc: 0.6830     \n",
      "Epoch 615/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9342 - acc: 0.6796     \n",
      "Epoch 616/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9247 - acc: 0.6908     \n",
      "Epoch 617/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9149 - acc: 0.6863     \n",
      "Epoch 618/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9215 - acc: 0.6819     \n",
      "Epoch 619/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9354 - acc: 0.6752     \n",
      "Epoch 620/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9289 - acc: 0.6819     \n",
      "Epoch 621/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9209 - acc: 0.6952     \n",
      "Epoch 622/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9131 - acc: 0.7041     \n",
      "Epoch 623/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9133 - acc: 0.6874     \n",
      "Epoch 624/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9175 - acc: 0.6885     \n",
      "Epoch 625/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9201 - acc: 0.6919     \n",
      "Epoch 626/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9268 - acc: 0.6908     \n",
      "Epoch 627/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9163 - acc: 0.6819     \n",
      "Epoch 628/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9166 - acc: 0.6808     \n",
      "Epoch 629/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9145 - acc: 0.6908     \n",
      "Epoch 630/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9235 - acc: 0.6874     \n",
      "Epoch 631/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9140 - acc: 0.6885     \n",
      "Epoch 632/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9199 - acc: 0.6785     \n",
      "Epoch 633/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9143 - acc: 0.6885     \n",
      "Epoch 634/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9116 - acc: 0.6863     \n",
      "Epoch 635/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9117 - acc: 0.6852     \n",
      "Epoch 636/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9057 - acc: 0.6874     \n",
      "Epoch 637/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9066 - acc: 0.6885     \n",
      "Epoch 638/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9189 - acc: 0.6897     \n",
      "Epoch 639/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9103 - acc: 0.6897     \n",
      "Epoch 640/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9067 - acc: 0.6974     \n",
      "Epoch 641/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9028 - acc: 0.6808     \n",
      "Epoch 642/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9024 - acc: 0.6930     \n",
      "Epoch 643/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9172 - acc: 0.6852     \n",
      "Epoch 644/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9119 - acc: 0.6830     \n",
      "Epoch 645/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9129 - acc: 0.6941     \n",
      "Epoch 646/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8988 - acc: 0.7019     \n",
      "Epoch 647/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9062 - acc: 0.6941     \n",
      "Epoch 648/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8980 - acc: 0.6952     \n",
      "Epoch 649/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8937 - acc: 0.6986     \n",
      "Epoch 650/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8944 - acc: 0.7008     \n",
      "Epoch 651/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9097 - acc: 0.6874     \n",
      "Epoch 652/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9075 - acc: 0.6919     \n",
      "Epoch 653/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9069 - acc: 0.6819     \n",
      "Epoch 654/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8990 - acc: 0.6930     \n",
      "Epoch 655/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8919 - acc: 0.7008     \n",
      "Epoch 656/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8979 - acc: 0.6919     \n",
      "Epoch 657/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8925 - acc: 0.6974     \n",
      "Epoch 658/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8893 - acc: 0.6930     \n",
      "Epoch 659/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8978 - acc: 0.7008     \n",
      "Epoch 660/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8881 - acc: 0.6952     \n",
      "Epoch 661/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9006 - acc: 0.6941     \n",
      "Epoch 662/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9104 - acc: 0.6774     \n",
      "Epoch 663/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8851 - acc: 0.7063     \n",
      "Epoch 664/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8793 - acc: 0.7119     \n",
      "Epoch 665/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8811 - acc: 0.7030     \n",
      "Epoch 666/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8766 - acc: 0.7175     \n",
      "Epoch 667/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8866 - acc: 0.7086     \n",
      "Epoch 668/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8960 - acc: 0.6808     \n",
      "Epoch 669/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8848 - acc: 0.7097     \n",
      "Epoch 670/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8882 - acc: 0.6874     \n",
      "Epoch 671/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8873 - acc: 0.7008     \n",
      "Epoch 672/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8774 - acc: 0.7019     \n",
      "Epoch 673/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8769 - acc: 0.7063     \n",
      "Epoch 674/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8804 - acc: 0.6986     \n",
      "Epoch 675/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8914 - acc: 0.7075     \n",
      "Epoch 676/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8775 - acc: 0.7075     \n",
      "Epoch 677/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8817 - acc: 0.6986     \n",
      "Epoch 678/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8786 - acc: 0.7063     \n",
      "Epoch 679/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8820 - acc: 0.7108     \n",
      "Epoch 680/1000\n",
      "899/899 [==============================] - 0s - loss: 0.9113 - acc: 0.6852     \n",
      "Epoch 681/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8803 - acc: 0.6997     \n",
      "Epoch 682/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8896 - acc: 0.6930     \n",
      "Epoch 683/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8842 - acc: 0.6963     \n",
      "Epoch 684/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8718 - acc: 0.7041     \n",
      "Epoch 685/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8741 - acc: 0.6986     \n",
      "Epoch 686/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8680 - acc: 0.7075     \n",
      "Epoch 687/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8686 - acc: 0.7041     \n",
      "Epoch 688/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8661 - acc: 0.7075     \n",
      "Epoch 689/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8724 - acc: 0.7019     \n",
      "Epoch 690/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8653 - acc: 0.7186     \n",
      "Epoch 691/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8659 - acc: 0.7130     \n",
      "Epoch 692/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8616 - acc: 0.7219     \n",
      "Epoch 693/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8590 - acc: 0.7030     \n",
      "Epoch 694/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8672 - acc: 0.6986     \n",
      "Epoch 695/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8677 - acc: 0.7175     \n",
      "Epoch 696/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8636 - acc: 0.7063     \n",
      "Epoch 697/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8631 - acc: 0.7141     \n",
      "Epoch 698/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8643 - acc: 0.7130     \n",
      "Epoch 699/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8651 - acc: 0.7175     \n",
      "Epoch 700/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8661 - acc: 0.7086     \n",
      "Epoch 701/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8625 - acc: 0.7152     \n",
      "Epoch 702/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8558 - acc: 0.7175     \n",
      "Epoch 703/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8646 - acc: 0.7041     \n",
      "Epoch 704/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8484 - acc: 0.7208     \n",
      "Epoch 705/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8499 - acc: 0.7063     \n",
      "Epoch 706/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8650 - acc: 0.7230     \n",
      "Epoch 707/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8526 - acc: 0.7152     \n",
      "Epoch 708/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8590 - acc: 0.7175     \n",
      "Epoch 709/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8532 - acc: 0.7219     \n",
      "Epoch 710/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8597 - acc: 0.7052     \n",
      "Epoch 711/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8485 - acc: 0.7175     \n",
      "Epoch 712/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8452 - acc: 0.7230     \n",
      "Epoch 713/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8458 - acc: 0.7152     \n",
      "Epoch 714/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8439 - acc: 0.7108     \n",
      "Epoch 715/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8550 - acc: 0.7152     \n",
      "Epoch 716/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8538 - acc: 0.7186     \n",
      "Epoch 717/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8453 - acc: 0.7197     \n",
      "Epoch 718/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8554 - acc: 0.6997     \n",
      "Epoch 719/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8468 - acc: 0.7275     \n",
      "Epoch 720/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8403 - acc: 0.7308     \n",
      "Epoch 721/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8401 - acc: 0.7230     \n",
      "Epoch 722/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8405 - acc: 0.7275     \n",
      "Epoch 723/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8396 - acc: 0.7186     \n",
      "Epoch 724/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8457 - acc: 0.7152     \n",
      "Epoch 725/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8387 - acc: 0.7175     \n",
      "Epoch 726/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8463 - acc: 0.7241     \n",
      "Epoch 727/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8348 - acc: 0.7219     \n",
      "Epoch 728/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8462 - acc: 0.7164     \n",
      "Epoch 729/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8378 - acc: 0.7241     \n",
      "Epoch 730/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8406 - acc: 0.7152     \n",
      "Epoch 731/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8383 - acc: 0.7208     \n",
      "Epoch 732/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8454 - acc: 0.7075     \n",
      "Epoch 733/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8680 - acc: 0.7008     \n",
      "Epoch 734/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8333 - acc: 0.7375     \n",
      "Epoch 735/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8306 - acc: 0.7253     \n",
      "Epoch 736/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8311 - acc: 0.7419     \n",
      "Epoch 737/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8367 - acc: 0.7330     \n",
      "Epoch 738/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8437 - acc: 0.7219     \n",
      "Epoch 739/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8340 - acc: 0.7308     \n",
      "Epoch 740/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8330 - acc: 0.7353     \n",
      "Epoch 741/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8403 - acc: 0.7275     \n",
      "Epoch 742/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8325 - acc: 0.7330     \n",
      "Epoch 743/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8287 - acc: 0.7297     \n",
      "Epoch 744/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8202 - acc: 0.7464     \n",
      "Epoch 745/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8272 - acc: 0.7353     \n",
      "Epoch 746/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8291 - acc: 0.7375     \n",
      "Epoch 747/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8270 - acc: 0.7219     \n",
      "Epoch 748/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8287 - acc: 0.7275     \n",
      "Epoch 749/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8180 - acc: 0.7341     \n",
      "Epoch 750/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8390 - acc: 0.7097     \n",
      "Epoch 751/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8224 - acc: 0.7364     \n",
      "Epoch 752/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8309 - acc: 0.7308     \n",
      "Epoch 753/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8098 - acc: 0.7364     \n",
      "Epoch 754/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8196 - acc: 0.7319     \n",
      "Epoch 755/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8274 - acc: 0.7308     \n",
      "Epoch 756/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8121 - acc: 0.7297     \n",
      "Epoch 757/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8109 - acc: 0.7219     \n",
      "Epoch 758/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8227 - acc: 0.7308     \n",
      "Epoch 759/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8128 - acc: 0.7375     \n",
      "Epoch 760/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8228 - acc: 0.7286     \n",
      "Epoch 761/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8302 - acc: 0.7319     \n",
      "Epoch 762/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8156 - acc: 0.7341     \n",
      "Epoch 763/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8187 - acc: 0.7275     \n",
      "Epoch 764/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8157 - acc: 0.7264     \n",
      "Epoch 765/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8254 - acc: 0.7197     \n",
      "Epoch 766/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8324 - acc: 0.7108     \n",
      "Epoch 767/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8064 - acc: 0.7397     \n",
      "Epoch 768/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8149 - acc: 0.7386     \n",
      "Epoch 769/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8153 - acc: 0.7330     \n",
      "Epoch 770/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8040 - acc: 0.7364     \n",
      "Epoch 771/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8124 - acc: 0.7353     \n",
      "Epoch 772/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8059 - acc: 0.7408     \n",
      "Epoch 773/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7934 - acc: 0.7508     \n",
      "Epoch 774/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7999 - acc: 0.7386     \n",
      "Epoch 775/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8041 - acc: 0.7275     \n",
      "Epoch 776/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 0s - loss: 0.8197 - acc: 0.7197     \n",
      "Epoch 777/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8032 - acc: 0.7430     \n",
      "Epoch 778/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8017 - acc: 0.7397     \n",
      "Epoch 779/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7963 - acc: 0.7519     \n",
      "Epoch 780/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8057 - acc: 0.7375     \n",
      "Epoch 781/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7959 - acc: 0.7408     \n",
      "Epoch 782/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8010 - acc: 0.7486     \n",
      "Epoch 783/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7967 - acc: 0.7375     \n",
      "Epoch 784/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8066 - acc: 0.7364     \n",
      "Epoch 785/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7931 - acc: 0.7486     \n",
      "Epoch 786/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8180 - acc: 0.7375     \n",
      "Epoch 787/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8282 - acc: 0.7130     \n",
      "Epoch 788/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7967 - acc: 0.7408     \n",
      "Epoch 789/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8050 - acc: 0.7419     \n",
      "Epoch 790/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7926 - acc: 0.7519     \n",
      "Epoch 791/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7888 - acc: 0.7408     \n",
      "Epoch 792/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7987 - acc: 0.7375     \n",
      "Epoch 793/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8020 - acc: 0.7386     \n",
      "Epoch 794/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7945 - acc: 0.7397     \n",
      "Epoch 795/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7951 - acc: 0.7430     \n",
      "Epoch 796/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7823 - acc: 0.7464     \n",
      "Epoch 797/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7969 - acc: 0.7475     \n",
      "Epoch 798/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7872 - acc: 0.7642     \n",
      "Epoch 799/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7829 - acc: 0.7453     \n",
      "Epoch 800/1000\n",
      "899/899 [==============================] - 0s - loss: 0.8009 - acc: 0.7508     \n",
      "Epoch 801/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7813 - acc: 0.7564     \n",
      "Epoch 802/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7807 - acc: 0.7497     \n",
      "Epoch 803/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7782 - acc: 0.7597     \n",
      "Epoch 804/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7820 - acc: 0.7497     \n",
      "Epoch 805/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7878 - acc: 0.7475     \n",
      "Epoch 806/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7759 - acc: 0.7653     \n",
      "Epoch 807/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7826 - acc: 0.7486     \n",
      "Epoch 808/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7823 - acc: 0.7586     \n",
      "Epoch 809/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7873 - acc: 0.7453     \n",
      "Epoch 810/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7830 - acc: 0.7453     \n",
      "Epoch 811/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7864 - acc: 0.7508     \n",
      "Epoch 812/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7874 - acc: 0.7419     \n",
      "Epoch 813/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7950 - acc: 0.7375     \n",
      "Epoch 814/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7879 - acc: 0.7430     \n",
      "Epoch 815/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7811 - acc: 0.7453     \n",
      "Epoch 816/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7754 - acc: 0.7542     \n",
      "Epoch 817/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7748 - acc: 0.7586     \n",
      "Epoch 818/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7793 - acc: 0.7542     \n",
      "Epoch 819/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7884 - acc: 0.7375     \n",
      "Epoch 820/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7704 - acc: 0.7753     \n",
      "Epoch 821/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7751 - acc: 0.7531     \n",
      "Epoch 822/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7844 - acc: 0.7531     \n",
      "Epoch 823/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7764 - acc: 0.7542     \n",
      "Epoch 824/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7705 - acc: 0.7497     \n",
      "Epoch 825/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7868 - acc: 0.7386     \n",
      "Epoch 826/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7923 - acc: 0.7330     \n",
      "Epoch 827/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7592 - acc: 0.7597     \n",
      "Epoch 828/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7641 - acc: 0.7508     \n",
      "Epoch 829/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7641 - acc: 0.7675     \n",
      "Epoch 830/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7632 - acc: 0.7453     \n",
      "Epoch 831/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7703 - acc: 0.7542     \n",
      "Epoch 832/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7796 - acc: 0.7442     \n",
      "Epoch 833/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7723 - acc: 0.7575     \n",
      "Epoch 834/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7559 - acc: 0.7586     \n",
      "Epoch 835/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7627 - acc: 0.7731     \n",
      "Epoch 836/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7591 - acc: 0.7686     \n",
      "Epoch 837/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7609 - acc: 0.7442     \n",
      "Epoch 838/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7670 - acc: 0.7364     \n",
      "Epoch 839/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7589 - acc: 0.7486     \n",
      "Epoch 840/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7512 - acc: 0.7664     \n",
      "Epoch 841/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7678 - acc: 0.7486     \n",
      "Epoch 842/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7627 - acc: 0.7653     \n",
      "Epoch 843/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7612 - acc: 0.7575     \n",
      "Epoch 844/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7627 - acc: 0.7553     \n",
      "Epoch 845/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7495 - acc: 0.7597     \n",
      "Epoch 846/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7518 - acc: 0.7620     \n",
      "Epoch 847/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7617 - acc: 0.7497     \n",
      "Epoch 848/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7595 - acc: 0.7653     \n",
      "Epoch 849/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7512 - acc: 0.7620     \n",
      "Epoch 850/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7584 - acc: 0.7653     \n",
      "Epoch 851/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7500 - acc: 0.7631     \n",
      "Epoch 852/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7480 - acc: 0.7709     \n",
      "Epoch 853/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7717 - acc: 0.7475     \n",
      "Epoch 854/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7802 - acc: 0.7475     \n",
      "Epoch 855/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7561 - acc: 0.7564     \n",
      "Epoch 856/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7446 - acc: 0.7620     \n",
      "Epoch 857/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7419 - acc: 0.7764     \n",
      "Epoch 858/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7522 - acc: 0.7564     \n",
      "Epoch 859/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7624 - acc: 0.7497     \n",
      "Epoch 860/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7485 - acc: 0.7631     \n",
      "Epoch 861/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7603 - acc: 0.7430     \n",
      "Epoch 862/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7462 - acc: 0.7575     \n",
      "Epoch 863/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7489 - acc: 0.7608     \n",
      "Epoch 864/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7607 - acc: 0.7408     \n",
      "Epoch 865/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7465 - acc: 0.7620     \n",
      "Epoch 866/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7446 - acc: 0.7675     \n",
      "Epoch 867/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7430 - acc: 0.7508     \n",
      "Epoch 868/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7395 - acc: 0.7742     \n",
      "Epoch 869/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7459 - acc: 0.7675     \n",
      "Epoch 870/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7421 - acc: 0.7586     \n",
      "Epoch 871/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7462 - acc: 0.7620     \n",
      "Epoch 872/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7433 - acc: 0.7564     \n",
      "Epoch 873/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7322 - acc: 0.7642     \n",
      "Epoch 874/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7333 - acc: 0.7686     \n",
      "Epoch 875/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7359 - acc: 0.7731     \n",
      "Epoch 876/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7317 - acc: 0.7742     \n",
      "Epoch 877/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7368 - acc: 0.7742     \n",
      "Epoch 878/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7308 - acc: 0.7831     \n",
      "Epoch 879/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7485 - acc: 0.7475     \n",
      "Epoch 880/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7423 - acc: 0.7608     \n",
      "Epoch 881/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7404 - acc: 0.7642     \n",
      "Epoch 882/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7243 - acc: 0.7775     \n",
      "Epoch 883/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7516 - acc: 0.7586     \n",
      "Epoch 884/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7291 - acc: 0.7675     \n",
      "Epoch 885/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7218 - acc: 0.7764     \n",
      "Epoch 886/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7203 - acc: 0.7720     \n",
      "Epoch 887/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7297 - acc: 0.7709     \n",
      "Epoch 888/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7216 - acc: 0.7820     \n",
      "Epoch 889/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7191 - acc: 0.7720     \n",
      "Epoch 890/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7308 - acc: 0.7631     \n",
      "Epoch 891/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7137 - acc: 0.7798     \n",
      "Epoch 892/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7308 - acc: 0.7653     \n",
      "Epoch 893/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7228 - acc: 0.7697     \n",
      "Epoch 894/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7315 - acc: 0.7697     \n",
      "Epoch 895/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7196 - acc: 0.7798     \n",
      "Epoch 896/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7300 - acc: 0.7709     \n",
      "Epoch 897/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7254 - acc: 0.7786     \n",
      "Epoch 898/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7198 - acc: 0.7798     \n",
      "Epoch 899/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7341 - acc: 0.7586     \n",
      "Epoch 900/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7301 - acc: 0.7664     \n",
      "Epoch 901/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7147 - acc: 0.7742     \n",
      "Epoch 902/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7393 - acc: 0.7508     \n",
      "Epoch 903/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7139 - acc: 0.7809     \n",
      "Epoch 904/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7184 - acc: 0.7742     \n",
      "Epoch 905/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7168 - acc: 0.7709     \n",
      "Epoch 906/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7202 - acc: 0.7786     \n",
      "Epoch 907/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7147 - acc: 0.7764     \n",
      "Epoch 908/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7092 - acc: 0.7809     \n",
      "Epoch 909/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7104 - acc: 0.7764     \n",
      "Epoch 910/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7087 - acc: 0.7798     \n",
      "Epoch 911/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7156 - acc: 0.7720     \n",
      "Epoch 912/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7100 - acc: 0.7909     \n",
      "Epoch 913/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7144 - acc: 0.7809     \n",
      "Epoch 914/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7153 - acc: 0.7664     \n",
      "Epoch 915/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7134 - acc: 0.7831     \n",
      "Epoch 916/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7107 - acc: 0.7831     \n",
      "Epoch 917/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7128 - acc: 0.7664     \n",
      "Epoch 918/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7258 - acc: 0.7720     \n",
      "Epoch 919/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7313 - acc: 0.7608     \n",
      "Epoch 920/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6997 - acc: 0.7842     \n",
      "Epoch 921/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7134 - acc: 0.7720     \n",
      "Epoch 922/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7230 - acc: 0.7553     \n",
      "Epoch 923/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7186 - acc: 0.7620     \n",
      "Epoch 924/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7012 - acc: 0.7875     \n",
      "Epoch 925/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7009 - acc: 0.7786     \n",
      "Epoch 926/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7134 - acc: 0.7697     \n",
      "Epoch 927/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7010 - acc: 0.7920     \n",
      "Epoch 928/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7073 - acc: 0.7942     \n",
      "Epoch 929/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7081 - acc: 0.7753     \n",
      "Epoch 930/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6990 - acc: 0.7887     \n",
      "Epoch 931/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7012 - acc: 0.7842     \n",
      "Epoch 932/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7090 - acc: 0.7697     \n",
      "Epoch 933/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7002 - acc: 0.7820     \n",
      "Epoch 934/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7090 - acc: 0.7709     \n",
      "Epoch 935/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6906 - acc: 0.7953     \n",
      "Epoch 936/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6892 - acc: 0.7942     \n",
      "Epoch 937/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7072 - acc: 0.7731     \n",
      "Epoch 938/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7094 - acc: 0.7686     \n",
      "Epoch 939/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6916 - acc: 0.7875     \n",
      "Epoch 940/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6980 - acc: 0.7764     \n",
      "Epoch 941/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7128 - acc: 0.7664     \n",
      "Epoch 942/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6899 - acc: 0.7942     \n",
      "Epoch 943/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6902 - acc: 0.7764     \n",
      "Epoch 944/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7005 - acc: 0.7820     \n",
      "Epoch 945/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7041 - acc: 0.7753     \n",
      "Epoch 946/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6979 - acc: 0.7764     \n",
      "Epoch 947/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6894 - acc: 0.7920     \n",
      "Epoch 948/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 0s - loss: 0.6852 - acc: 0.7864     \n",
      "Epoch 949/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6849 - acc: 0.7875     \n",
      "Epoch 950/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6899 - acc: 0.7864     \n",
      "Epoch 951/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6995 - acc: 0.7831     \n",
      "Epoch 952/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6825 - acc: 0.7898     \n",
      "Epoch 953/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6875 - acc: 0.7753     \n",
      "Epoch 954/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7083 - acc: 0.7709     \n",
      "Epoch 955/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7018 - acc: 0.7686     \n",
      "Epoch 956/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6905 - acc: 0.7786     \n",
      "Epoch 957/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7004 - acc: 0.7853     \n",
      "Epoch 958/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6948 - acc: 0.7775     \n",
      "Epoch 959/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6849 - acc: 0.7920     \n",
      "Epoch 960/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6799 - acc: 0.7998     \n",
      "Epoch 961/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6909 - acc: 0.7786     \n",
      "Epoch 962/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6860 - acc: 0.7898     \n",
      "Epoch 963/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6890 - acc: 0.7831     \n",
      "Epoch 964/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6780 - acc: 0.7964     \n",
      "Epoch 965/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6845 - acc: 0.7875     \n",
      "Epoch 966/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6791 - acc: 0.7909     \n",
      "Epoch 967/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6830 - acc: 0.7775     \n",
      "Epoch 968/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6785 - acc: 0.7898     \n",
      "Epoch 969/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6847 - acc: 0.7842     \n",
      "Epoch 970/1000\n",
      "899/899 [==============================] - 0s - loss: 0.7068 - acc: 0.7709     \n",
      "Epoch 971/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6768 - acc: 0.7820     \n",
      "Epoch 972/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6787 - acc: 0.7920     \n",
      "Epoch 973/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6767 - acc: 0.7875     \n",
      "Epoch 974/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6870 - acc: 0.7887     \n",
      "Epoch 975/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6855 - acc: 0.7842     \n",
      "Epoch 976/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6704 - acc: 0.7998     \n",
      "Epoch 977/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6663 - acc: 0.7920     \n",
      "Epoch 978/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6687 - acc: 0.7887     \n",
      "Epoch 979/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6685 - acc: 0.7909     \n",
      "Epoch 980/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6701 - acc: 0.7731     \n",
      "Epoch 981/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6676 - acc: 0.7898     \n",
      "Epoch 982/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6618 - acc: 0.7920     \n",
      "Epoch 983/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6645 - acc: 0.7964     \n",
      "Epoch 984/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6616 - acc: 0.7964     \n",
      "Epoch 985/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6788 - acc: 0.7864     \n",
      "Epoch 986/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6560 - acc: 0.7953     \n",
      "Epoch 987/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6622 - acc: 0.8020     \n",
      "Epoch 988/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6761 - acc: 0.7976     \n",
      "Epoch 989/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6796 - acc: 0.7898     \n",
      "Epoch 990/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6629 - acc: 0.8053     \n",
      "Epoch 991/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6677 - acc: 0.7809     \n",
      "Epoch 992/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6655 - acc: 0.8009     \n",
      "Epoch 993/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6648 - acc: 0.7909     \n",
      "Epoch 994/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6692 - acc: 0.7875     \n",
      "Epoch 995/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6726 - acc: 0.7831     \n",
      "Epoch 996/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6626 - acc: 0.7931     \n",
      "Epoch 997/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6639 - acc: 0.7909     \n",
      "Epoch 998/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6713 - acc: 0.8053     \n",
      "Epoch 999/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6606 - acc: 0.7964     \n",
      "Epoch 1000/1000\n",
      "899/899 [==============================] - 0s - loss: 0.6550 - acc: 0.8020     \n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X,dummy_y,epochs = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data를 토대로 구한 Prediction 예측 값이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/899 [>.............................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5,  7, 10,  3,  6,  5,  5,  6,  5,  9,  3,  3,  5,  1,  7,  4,  3,\n",
       "        4,  9,  5,  5,  3,  8,  6,  3,  7,  9,  5,  3,  4,  4,  5,  5,  5,\n",
       "       10,  5,  0,  2,  5,  5,  6,  0,  2,  7,  2,  2,  5,  4,  6,  2,  4,\n",
       "        6,  5,  5,  9,  1,  8,  7,  4,  3,  3,  4,  5,  8,  5,  5,  5,  6,\n",
       "        3,  3,  7,  5,  7,  3,  5,  0,  5,  5,  6,  2,  8,  5,  3,  6,  3,\n",
       "        5,  4,  0,  5,  5,  6,  8,  5,  3,  6,  5,  6,  0,  4,  7,  6,  6,\n",
       "        5,  6,  4,  5,  3,  5,  9,  1,  6,  4,  4,  3,  7,  7, 10,  5,  5,\n",
       "        7,  6,  7,  0,  4,  3, 10,  3,  1,  7,  4,  6,  6,  6,  4,  5,  5,\n",
       "        5,  5,  7,  1,  5,  3,  4,  6,  5,  4,  5,  3,  7,  6,  5,  5,  5,\n",
       "        7,  4,  8, 10, 10,  3,  7,  5,  6, 10,  5,  3,  8,  8,  3,  5,  6,\n",
       "        5,  8,  7,  7,  6,  4,  5,  5,  0,  5,  5,  6,  5,  6,  5,  2,  0,\n",
       "        2,  6,  3,  5,  2,  6,  5,  3,  2,  6,  5,  5, 10,  5,  1,  7,  5,\n",
       "        6,  8,  5,  7,  3,  3,  3,  7,  4,  5,  6,  0,  1,  6,  4,  5,  5,\n",
       "        6,  6,  5,  3,  6,  5,  4,  5,  5,  5,  7,  6,  6,  8,  4,  5,  1,\n",
       "        7,  5,  5,  7,  3,  5,  4,  4,  6, 10,  5,  6,  5,  7,  5,  7,  5,\n",
       "        4,  5,  5,  5,  5,  5,  4,  6,  6,  6,  7,  8,  7,  5,  6,  7,  0,\n",
       "        5,  8,  6,  1, 10,  4,  7,  7,  7,  3,  4,  5,  7,  8,  5,  3,  7,\n",
       "        6,  3,  6,  7,  3,  5,  5,  5,  8,  2,  8,  0,  6,  5,  3,  6, 10,\n",
       "        5,  5,  7,  3,  6,  6,  5,  3,  7,  4,  8,  5,  5,  5,  5,  3,  5,\n",
       "        5,  3,  6,  8,  6,  6,  7,  2,  5,  7,  9,  8,  3,  5,  5,  6,  5,\n",
       "        7,  9,  7,  7,  3, 10,  7,  4,  4,  5,  5,  7,  3,  4,  6,  4,  5,\n",
       "        1,  5,  8,  3,  3,  2,  5,  8,  5,  5,  5,  3,  5,  7,  9,  6,  5,\n",
       "        6,  5,  4,  7,  2,  3,  4,  6, 10,  4,  3,  9,  7,  3,  5, 10,  7,\n",
       "        5, 10,  7,  5,  5,  5,  5,  3,  6,  5,  5,  5,  5,  4,  0,  2, 10,\n",
       "        6,  6,  7,  1,  7,  5,  7,  1,  5,  0,  1,  4,  5,  3,  8,  7,  5,\n",
       "        5,  9,  2,  5,  7,  2,  3,  4,  8,  5,  1,  6,  5,  5,  4,  9,  2,\n",
       "        3,  8,  7, 10,  5,  5,  5,  4,  7,  7,  5,  5,  5,  6,  3,  3,  7,\n",
       "        7,  5,  3,  8,  6,  2,  5,  6,  3,  4,  7,  6,  8,  5,  5,  8,  3,\n",
       "        3,  5,  5,  5,  5,  8,  5,  5,  5, 10,  8,  5,  5,  8,  6,  6,  6,\n",
       "        6,  5,  5,  5,  5,  5,  8,  7,  5,  0,  5,  5,  5, 10,  5,  5,  4,\n",
       "        7,  5,  3,  8,  7,  8,  6,  5,  4,  7,  7,  5,  5,  0,  5,  5,  3,\n",
       "        5,  4,  7,  3,  8,  4,  3,  5,  5,  3,  4,  9,  5,  0,  5,  5,  5,\n",
       "       10,  2,  5,  5,  8, 10,  5,  6,  3,  5,  5,  5, 10,  8,  7,  5,  8,\n",
       "        5,  2, 10,  5,  0,  5, 10,  7, 10,  4,  9,  6,  8,  3,  3, 10, 10,\n",
       "        5,  7,  6,  3,  4,  6,  5,  2,  5,  6,  8, 10,  5,  4,  5,  2,  5,\n",
       "        5,  3,  5,  8,  6,  3,  1,  7,  5,  3,  8,  4,  5,  7,  3,  3,  5,\n",
       "        2,  5,  3,  7,  5,  5,  5,  2,  3,  6,  5,  4,  7,  5,  5,  5,  6,\n",
       "        5,  6,  5,  5,  5,  4,  8,  6,  5, 10,  8,  5,  5,  6,  4,  5,  9,\n",
       "        5,  3,  2,  2,  5,  5,  2,  8,  0,  4,  3,  3,  5,  3,  5,  9,  5,\n",
       "        3,  9,  4,  4,  8,  5, 10,  3,  7,  6,  2,  6,  0,  1,  5,  1,  6,\n",
       "        1,  7,  7,  3,  5,  3,  7,  5,  2,  7,  5,  0,  4,  6,  5,  6,  8,\n",
       "        3,  5,  5,  5,  7,  5,  4,  5,  7,  1,  8,  3,  4,  5,  8,  8,  0,\n",
       "        5,  9,  4,  3,  7,  3,  3,  4,  1,  5,  2,  0,  5,  5,  5,  6,  6,\n",
       "        5,  3, 10,  5,  6,  7,  5,  6,  6,  7,  5,  3,  8,  4,  7,  4,  8,\n",
       "        5, 10,  4,  8,  3,  1,  5,  5,  5,  7,  5, 10,  5,  5,  9,  5,  3,\n",
       "        7,  3,  2,  4,  5,  5, 10,  3,  5,  7,  5,  4,  6,  4,  7,  3,  6,\n",
       "        5,  5,  3,  5,  5,  6,  5,  3,  6,  9,  7,  0,  7,  4,  3,  5,  5,\n",
       "        5,  8,  5,  4,  8,  4,  8,  8,  5,  5,  7,  2,  3,  7,  8,  8,  7,\n",
       "        7,  5,  5,  4,  5,  7,  3,  5,  3,  4,  2, 10,  3, 10,  5,  4,  1,\n",
       "       10,  8,  5,  2,  5,  5,  6,  5,  7,  2,  5,  7,  5,  7,  0,  2,  4,\n",
       "        6,  7,  5,  8,  7,  4,  5,  0,  4,  6, 10,  2,  5,  8,  3,  3,  5,\n",
       "        3,  7,  7,  5,  3, 10,  8, 10,  5,  6,  5,  1,  3,  5,  5,  5,  7,\n",
       "        5,  9,  9,  3,  7,  6,  7,  4, 10,  7,  7,  3,  2,  6,  5], dtype=int64)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/899 [>.............................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.02007186e-06,   1.77738280e-03,   1.46753073e-03, ...,\n",
       "          2.99268775e-03,   1.65467995e-09,   5.11392672e-03],\n",
       "       [  4.52406748e-05,   1.39105810e-10,   9.22930710e-09, ...,\n",
       "          2.89429468e-03,   3.42403859e-04,   1.97233952e-04],\n",
       "       [  8.11468344e-04,   3.59599881e-07,   5.29041653e-03, ...,\n",
       "          2.22583640e-06,   1.34378002e-04,   6.09360099e-01],\n",
       "       ..., \n",
       "       [  7.29348892e-09,   3.60597647e-03,   8.33716869e-01, ...,\n",
       "          1.46212697e-03,   7.49616248e-19,   1.30419963e-07],\n",
       "       [  1.22919760e-03,   3.06906038e-08,   3.35362131e-07, ...,\n",
       "          2.18422394e-02,   2.39874911e-03,   1.79638300e-05],\n",
       "       [  1.64378226e-01,   2.66652738e-17,   1.02656266e-13, ...,\n",
       "          2.37487452e-08,   7.84304664e-02,   2.83786212e-04]], dtype=float32)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set의 예측값이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/155 [=====>........................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  5,  1,  8,  5,  7,  7,  5,  5,  6,  5,  6,  6,  9,  5,  6, 10,\n",
       "        6,  7,  6,  1,  7,  7,  7,  7,  1,  3,  5,  5,  6,  5, 10,  4,  5,\n",
       "        5,  7,  6,  5,  3,  3,  5,  6,  5,  5,  5,  3,  7,  7,  6, 10,  8,\n",
       "       10,  7,  3,  7,  5,  5,  4,  8,  5,  8,  5,  5,  6,  3,  5,  6,  2,\n",
       "        5,  5,  5, 10,  5,  5,  5,  5,  5,  5,  4,  3,  5,  3,  4,  5,  6,\n",
       "        5,  5,  2,  3,  6,  5,  6,  5,  5,  5,  4,  5,  5,  5,  6,  0,  7,\n",
       "        7,  7,  0,  3,  5,  5,  3,  7,  5,  3,  3,  4,  0,  5,  5,  5,  4,\n",
       "        5,  4,  0,  5,  4,  4,  5,  6,  5,  5,  7,  4,  5,  4,  6,  6,  6,\n",
       "        3,  5,  5,  7,  3,  9,  5,  3,  5,  5,  5,  5,  6,  5,  5,  5,  0,\n",
       "        0,  4], dtype=int64)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Data 전처리 과정\n",
    "\n",
    "### Data set의 모든 변수가 categorical number이기 때문에 One Hot Encoding을 통해 numerical 의미를 같는 dummy 변수로 바꿔주어야 했다. \n",
    "### K2 ~ K14까지의 질문들에 Missing Value들은 MICE 모델을 통해 Imputation 해주었다. SoftImpute과 KNN 방법을 써 동일한 모델을 돌려보았으나 성능 차이가 크게 나지 않아 MICE Imputation을 썼다. \n",
    "### Ideo_self column의 Missing Value 유무에 따라 Train set과 Test set으로 나눠주었다.  \n",
    "\n",
    "## - Data Modeling 과정\n",
    "\n",
    "### Train set을 토대로 위의 9개의 모델들의 성능을 평가해 최적 모델의 정확도와 AUC 값은 다음과 같다. \n",
    "\n",
    "### ***`Parameter 조정으로 찾은 최적 Logistic Classifier의 결과값`***\n",
    "    - MAE: 2.607\n",
    "    - 훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
    "    - 가장 큰 결정 함수의 인덱스: [ 0  1 10  8  0  6  9  1  2  3]\n",
    "    - 인덱스를 classses_에 연결: [  0.   1.  10.   8.   0.   6.   9.   1.   2.   3.]\n",
    "    - Validation set의 예측(상위 10개): [  5.   5.   5.   3.  10.  10.   4.   5.  10.   5.]\n",
    "    - 실제 Validation set(상위 10개): [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
    "    - Validation Set의 정확도: 0.14\n",
    "    - Test set의 예측(상위 10개): [  8.  10.   1.   8.   7.  10.   8.   0.   1.   9.]\n",
    "\n",
    "### ***`Parameter 조정으로 찾은 최적 KNN의 결과값`***\n",
    "    - MAE:1.445\n",
    "    - 훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
    "    - Validation set의 예측(상위 10개): [  5.   5.   5.   3.  10.  10.   4.   5.  10.   5.]\n",
    "    - 실제 Validation set(상위 10개): [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
    "    - Validation Set의 정확도: 0.30\n",
    "    - Test set의 예측(상위 10개): [ 8.  8.  5.  8.  5.  8.  6.  5.  5.  9.]\n",
    "\n",
    "### ***`Parameter 조정으로 찾은 최적 Naive Bayes classifier의 결과값`***\n",
    "    - MAE: 3.633\n",
    "    - 훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
    "    - Validation set의 예측(상위 10개): [  4.   1.  10.   7.  10.   9.   0.   1.  10.   1.]\n",
    "    - 실제 Validation set(상위 10개): [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
    "    - Validation Set의 정확도: 0.04\n",
    "    - Test set의 예측(상위 10개): [  9.  10.   1.   1.   9.   9.   9.   1.   1.   9.]\n",
    "\n",
    "### ***`Parameter 조정으로 찾은 최적 Decision Tree의 결과값`***\n",
    "    - MAE: 1.611\n",
    "    - 훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
    "    - Validation set의 예측(상위 10개): [  4.   5.   5.   5.   5.   8.   5.   3.  10.   5.]\n",
    "    - 실제 Validation set(상위 10개): [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
    "    - Validation Set의 정확도: 0.29\n",
    "    - Test set의 예측(상위 10개): [  8.  10.   3.   8.   5.   8.   8.   3.   5.   8.]\n",
    "\n",
    "### ***`Parameter 조정으로 찾은 최적 Random Forest의 결과값`***\n",
    "    - MAE: 1.478\n",
    "    - 훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
    "    - Validation set의 예측(상위 10개): [ 5.  5.  5.  5.  5.  8.  5.  5.  5.  5.]\n",
    "    - 실제 Validation set(상위 10개): [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
    "    - Validation Set의 정확도: 0.35\n",
    "    - Test set의 예측(상위 10개): [  8.  10.   1.   8.   7.  10.   8.   0.   1.   9.]\n",
    "\n",
    "### ***`Parameter 조정으로 찾은 최적 SVM의 결과값`***\n",
    "    - MAE: 1.415\n",
    "    - 훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
    "    - 가장 큰 결정 함수의 인덱스: [31 40 38 43 50 52 43 43 41 43]\n",
    "    - Validation set의 예측(상위 10개): [ 5.  5.  5.  5.  5.  8.  5.  5.  5.  5.]\n",
    "    - 실제 Validation set(상위 10개): [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
    "    - Validation Set의 정확도: 0.34\n",
    "    - Test set의 예측(상위 10개): [  8.  10.   1.   8.   7.  10.   8.   0.   1.   9.]\n",
    "\n",
    "### ***`Parameter 조정으로 찾은 최적 Xgboost의 결과값`***\n",
    "    - MAE: 1.548\n",
    "    - 훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
    "    - Validation set의 예측(상위 10개): [ 5.  5.  5.  5.  5.  5.  5.  5.  5.  5.]\n",
    "    - 실제 Validation set(상위 10개): [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
    "    - Validation Set의 정확도: 0.33\n",
    "    - Test set의 예측(상위 10개): [ 4.  5.  5.  5.  5.  5.  5.  5.  5.  5.]\n",
    "    \n",
    "### ***`Parameter 조정으로 찾은 최적 SoftMax의 결과값`***\n",
    "    - 훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
    "    - Validation set의 예측(상위 10개): [ 5.  7.  5.  6.  5.  3.  5.  5.  5.  9.]\n",
    "    - 실제 Validation set(상위 10개): [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
    "    - Validation Set의 정확도: 0.37\n",
    "    - Test set의 예측(상위 10개): [ 3.  3.  4.  4.  4.  4.  3.  4.  4.  3.]\n",
    "\n",
    "### ***`Parameter 조정으로 찾은 최적 Keras + Relu + SoftMax의 결과값`***\n",
    "    - 훈련 데이터에 있는 클래스 종류: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
    "    - Validation set의 예측(상위 10개): [ 5.  7. 10.  3.  6.  5.  5.  6.  5.  9.]\n",
    "    - 실제 Validation set(상위 10개): [  0.   7.   7.   5.  10.  10.   5.   5.   7.   5.]\n",
    "    - Validation Set의 정확도: 0.32\n",
    "    - Test set의 예측(상위 10개): [ 10.  5.  1.  8.  5.  7.  7.  5.  5.  6.] \n",
    "\n",
    "\n",
    "## - Data Interpretation \n",
    "\n",
    "### 9개의 모델을 돌려본 결과 지도 학습 모델에서는 Random Forest, SVM, Xgboost가 가장 좋은 Validation set 정확도를 보여줬다. 비지도 학습 모델에서는 SoftMax 모델이 37%의 Validation set 정확도를 보여줌으로써 9개의 모델 중 가장 좋은 결과물을 보여줬다.  \n",
    "### SoftMax 모델을 쓴 이유는 일단 모든 변수들이 Categorical value이기에 One Hot Encoding을 해 numerical type으로 바꿔줄 필요가 있었다. Softmax 모델의 입력받은 값은  0~1사이의 값으로 모두 정규화되며 출력 값들의 총합은 항상 1이 되는 특성을 가진다. 출력은 분류하고 싶은 클래수의 수 만큼 구성되며 가장 큰 출력 값을 부여받은 클래스가 확률이 가장 높은 것으로 이용했다. 그러나 성능이 생각보다 높게 나오지 않았기 때문에 보완의 필요성을 느꼈다. \n",
    "### SoftMax 모델을 보완하기 위해 Keras를 통해 고속 계산을 하고 Relu Activation Function을 사용해 hidden layer 값을 구해 SoftMax 모델에 적용시켜 보았다. 그러나 오히려 SoftMax 모델 하나만을 사용한 것보다 성능이 떨어지는 결과물을 보여줬다. \n",
    "### 각 모델의 Parameter들을 Grid Search를 통해 조절해가며 최적 모델을 찾아 예측을 해보았지만 데이터가 총 1000개 정도 밖에 되지 않기에 더 이상의 성능을 끌어올릴 수 없었다. Parameter들을 조금 더 세밀하게 수정한다면 약간의 상승 효과는 볼 수 있을 것으로 예상되나 그 차이가 미미할 것으로 보이기에 더 이상 시도하지 않았다. 더 많은 데이터 셋을 이용해 모델링을 해본다면 분명 더 높은 성능을 보일 것이라고 생각된다. \n",
    "\n",
    "\n",
    "## 결과 비교\n",
    "\n",
    "### 실제 데이터와 예측 값을 비교하려면 위의 모델링마다 마지막에 test set에 의한 예측값을 뽑아놨다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
