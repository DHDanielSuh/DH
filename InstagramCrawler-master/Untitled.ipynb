{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-d DIR_PREFIX] [-q QUERY] [-t CRAWL_TYPE]\n",
      "                             [-n NUMBER] [-c] [-l] [-a AUTHENTICATION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Daniel\\AppData\\Roaming\\jupyter\\runtime\\kernel-fc949491-1316-4e95-9829-05d7efc68bae.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "try:\n",
    "    from urlparse import urljoin\n",
    "    from urllib import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib.parse import urljoin\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# HOST\n",
    "HOST = 'http://www.instagram.com'\n",
    "\n",
    "# SELENIUM CSS SELECTOR\n",
    "CSS_LOAD_MORE = \"a._8imhp._glz1g\"\n",
    "CSS_RIGHT_ARROW = \"a[class='_de018 coreSpriteRightPaginationArrow']\"\n",
    "FIREFOX_FIRST_POST_PATH = \"//div[contains(@class, '_8mlbc _vbtk2 _t5r8b')]\"\n",
    "TIME_TO_CAPTION_PATH = \"../../../div/ul/li/span\"\n",
    "\n",
    "# FOLLOWERS/FOLLOWING RELATED\n",
    "CSS_EXPLORE = \"a[href='/explore/']\"\n",
    "CSS_LOGIN = \"a[href='/accounts/login/']\"\n",
    "CSS_FOLLOWERS = \"a[href='/{}/followers/']\"\n",
    "CSS_FOLLOWING = \"a[href='/{}/following/']\"\n",
    "FOLLOWER_PATH = \"//div[contains(text(), 'Followers')]\"\n",
    "FOLLOWING_PATH = \"//div[contains(text(), 'Following')]\"\n",
    "\n",
    "# JAVASCRIPT COMMANDS\n",
    "SCROLL_UP = \"window.scrollTo(0, 0);\"\n",
    "SCROLL_DOWN = \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "\n",
    "class url_change(object):\n",
    "    \"\"\"\n",
    "        Used for caption scraping\n",
    "    \"\"\"\n",
    "    def __init__(self, prev_url):\n",
    "        self.prev_url = prev_url\n",
    "\n",
    "    def __call__(self, driver):\n",
    "        return self.prev_url != driver.current_url\n",
    "\n",
    "class InstagramCrawler(object):\n",
    "    \"\"\"\n",
    "        Crawler class\n",
    "    \"\"\"\n",
    "    def __init__(self, headless=True):\n",
    "        if headless:\n",
    "            print(\"headless mode on\")\n",
    "            self._driver = webdriver.PhantomJS()\n",
    "        else:\n",
    "            self._driver = webdriver.Firefox()\n",
    "\n",
    "        self._driver.implicitly_wait(10)\n",
    "        self.data = defaultdict(list)\n",
    "\n",
    "    def login(self, authentication=None):\n",
    "        \"\"\"\n",
    "            authentication: path to authentication json file\n",
    "        \"\"\"\n",
    "        self._driver.get(urljoin(HOST, \"accounts/login/\"))\n",
    "\n",
    "        if authentication:\n",
    "            print(\"Username and password loaded from {}\".format(authentication))\n",
    "            with open(authentication, 'r') as fin:\n",
    "                auth_dict = json.loads(fin.read())\n",
    "            # Input username\n",
    "            username_input = WebDriverWait(self._driver, 5).until(\n",
    "                EC.presence_of_element_located((By.NAME, 'username'))\n",
    "            )\n",
    "            username_input.send_keys(auth_dict['username'])\n",
    "            # Input password\n",
    "            password_input = WebDriverWait(self._driver, 5).until(\n",
    "                EC.presence_of_element_located((By.NAME, 'password'))\n",
    "            )\n",
    "            password_input.send_keys(auth_dict['password'])\n",
    "            # Submit\n",
    "            password_input.submit()\n",
    "        else:\n",
    "            print(\"Type your username and password by hand to login!\")\n",
    "            print(\"You have a minute to do so!\")\n",
    "\n",
    "        print(\"\")\n",
    "        WebDriverWait(self._driver, 60).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, CSS_EXPLORE))\n",
    "        )\n",
    "\n",
    "    def quit(self):\n",
    "        self._driver.quit()\n",
    "\n",
    "    def crawl(self, dir_prefix, query, crawl_type, number, caption, authentication):\n",
    "        print(\"dir_prefix: {}, query: {}, crawl_type: {}, number: {}, caption: {}, authentication: {}\"\n",
    "              .format(dir_prefix, query, crawl_type, number, caption, authentication))\n",
    "\n",
    "        if crawl_type == \"photos\":\n",
    "            # Browse target page\n",
    "            self.browse_target_page(query)\n",
    "            # Scroll down until target number photos is reached\n",
    "            self.scroll_to_num_of_posts(number)\n",
    "            # Scrape photo links\n",
    "            self.scrape_photo_links(number, is_hashtag=query.startswith(\"#\"))\n",
    "            # Scrape captions if specified\n",
    "            if caption is True:\n",
    "                self.click_and_scrape_captions(number)\n",
    "\n",
    "        elif crawl_type in [\"followers\", \"following\"]:\n",
    "            # Need to login first before crawling followers/following\n",
    "            print(\"You will need to login to crawl {}\".format(crawl_type))\n",
    "            self.login(authentication)\n",
    "\n",
    "            # Then browse target page\n",
    "            assert not query.startswith(\n",
    "                '#'), \"Hashtag does not have followers/following!\"\n",
    "            self.browse_target_page(query)\n",
    "            # Scrape captions\n",
    "            self.scrape_followers_or_following(crawl_type, query, number)\n",
    "        else:\n",
    "            print(\"Unknown crawl type: {}\".format(crawl_type))\n",
    "            self.quit()\n",
    "            return\n",
    "        # Save to directory\n",
    "        print(\"Saving...\")\n",
    "        self.download_and_save(dir_prefix, query, crawl_type)\n",
    "\n",
    "        # Quit driver\n",
    "        print(\"Quitting driver...\")\n",
    "        self.quit()\n",
    "\n",
    "    def browse_target_page(self, query):\n",
    "        # Browse Hashtags\n",
    "        if query.startswith('#'):\n",
    "            relative_url = urljoin('explore/tags/', query.strip('#'))\n",
    "        else:  # Browse user page\n",
    "            relative_url = query\n",
    "\n",
    "        target_url = urljoin(HOST, relative_url)\n",
    "\n",
    "        self._driver.get(target_url)\n",
    "\n",
    "    def scroll_to_num_of_posts(self, number):\n",
    "        # Get total number of posts of page\n",
    "        num_info = re.search(r'\\], \"count\": \\d+',\n",
    "                             self._driver.page_source).group()\n",
    "        num_of_posts = int(re.findall(r'\\d+', num_info)[0])\n",
    "        print(\"posts: {}, number: {}\".format(num_of_posts, number))\n",
    "        number = number if number < num_of_posts else num_of_posts\n",
    "\n",
    "        # scroll page until reached\n",
    "        loadmore = WebDriverWait(self._driver, 10).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, CSS_LOAD_MORE))\n",
    "        )\n",
    "        loadmore.click()\n",
    "\n",
    "        num_to_scroll = int((number - 12) / 12) + 1\n",
    "        for _ in range(num_to_scroll):\n",
    "            self._driver.execute_script(SCROLL_DOWN)\n",
    "            time.sleep(0.2)\n",
    "            self._driver.execute_script(SCROLL_UP)\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    def scrape_photo_links(self, number, is_hashtag=False):\n",
    "        print(\"Scraping photo links...\")\n",
    "        encased_photo_links = re.finditer(r'src=\"([https]+:...[\\/\\w \\.-]*..[\\/\\w \\.-]*'\n",
    "                                          r'..[\\/\\w \\.-]*..[\\/\\w \\.-].jpg)', self._driver.page_source)\n",
    "\n",
    "        photo_links = [m.group(1) for m in encased_photo_links]\n",
    "\n",
    "        print(\"Number of photo_links: {}\".format(len(photo_links)))\n",
    "\n",
    "        begin = 0 if is_hashtag else 1\n",
    "\n",
    "        self.data['photo_links'] = photo_links[begin:number + begin]\n",
    "\n",
    "    def click_and_scrape_captions(self, number):\n",
    "        print(\"Scraping captions...\")\n",
    "        captions = []\n",
    "\n",
    "        for post_num in range(number):\n",
    "            sys.stdout.write(\"\\033[F\")\n",
    "            print(\"Scraping captions {} / {}\".format(post_num+1,number))\n",
    "            if post_num == 0:  # Click on the first post\n",
    "                # Chrome\n",
    "                # self._driver.find_element_by_class_name('_ovg3g').click()\n",
    "                self._driver.find_element_by_xpath(\n",
    "                    FIREFOX_FIRST_POST_PATH).click()\n",
    "\n",
    "                if number != 1:  #\n",
    "                    WebDriverWait(self._driver, 5).until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (By.CSS_SELECTOR, CSS_RIGHT_ARROW)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            elif number != 1:  # Click Right Arrow to move to next post\n",
    "                url_before = self._driver.current_url\n",
    "                self._driver.find_element_by_css_selector(\n",
    "                    CSS_RIGHT_ARROW).click()\n",
    "\n",
    "                # Wait until the page has loaded\n",
    "                try:\n",
    "                    WebDriverWait(self._driver, 10).until(\n",
    "                        url_change(url_before))\n",
    "                except TimeoutException:\n",
    "                    print(\"Time out in caption scraping at number {}\".format(post_num))\n",
    "                    break\n",
    "\n",
    "            # Parse caption\n",
    "            try:\n",
    "                time_element = WebDriverWait(self._driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"time\"))\n",
    "                )\n",
    "                caption = time_element.find_element_by_xpath(\n",
    "                    TIME_TO_CAPTION_PATH).text\n",
    "            except NoSuchElementException:  # Forbidden\n",
    "                print(\"Caption not found in the {} photo\".format(post_num))\n",
    "                caption = \"\"\n",
    "\n",
    "            captions.append(caption)\n",
    "\n",
    "        self.data['captions'] = captions\n",
    "\n",
    "    def scrape_followers_or_following(self, crawl_type, query, number):\n",
    "        print(\"Scraping {}...\".format(crawl_type))\n",
    "        if crawl_type == \"followers\":\n",
    "            FOLLOW_ELE = CSS_FOLLOWERS\n",
    "            FOLLOW_PATH = FOLLOWER_PATH\n",
    "        elif crawl_type == \"following\":\n",
    "            FOLLOW_ELE = CSS_FOLLOWING\n",
    "            FOLLOW_PATH = FOLLOWING_PATH\n",
    "\n",
    "        # Locate follow list\n",
    "        follow_ele = WebDriverWait(self._driver, 5).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, FOLLOW_ELE.format(query)))\n",
    "        )\n",
    "\n",
    "        # when no number defined, check the total items\n",
    "        if number is 0:\n",
    "            number = int(filter(str.isdigit, str(follow_ele.text)))\n",
    "            print(\"getting all \" + str(number) + \" items\")\n",
    "\n",
    "        # open desired list\n",
    "        follow_ele.click()\n",
    "\n",
    "        title_ele = WebDriverWait(self._driver, 5).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, FOLLOW_PATH))\n",
    "        )\n",
    "        List = title_ele.find_element_by_xpath(\n",
    "            '..').find_element_by_tag_name('ul')\n",
    "        List.click()\n",
    "\n",
    "        # Loop through list till target number is reached\n",
    "        num_of_shown_follow = len(List.find_elements_by_xpath('*'))\n",
    "        while len(List.find_elements_by_xpath('*')) < number:\n",
    "            element = List.find_elements_by_xpath('*')[-1]\n",
    "            # Work around for now => should use selenium's Expected Conditions!\n",
    "            try:\n",
    "                element.send_keys(Keys.PAGE_DOWN)\n",
    "            except Exception as e:\n",
    "                time.sleep(0.1)\n",
    "\n",
    "        follow_items = []\n",
    "        for ele in List.find_elements_by_xpath('*')[:number]:\n",
    "            follow_items.append(ele.text.split('\\n')[0])\n",
    "\n",
    "        self.data[crawl_type] = follow_items\n",
    "\n",
    "    def download_and_save(self, dir_prefix, query, crawl_type):\n",
    "        # Check if is hashtag\n",
    "        dir_name = query.lstrip(\n",
    "            '#') + '.hashtag' if query.startswith('#') else query\n",
    "\n",
    "        dir_path = os.path.join(dir_prefix, dir_name)\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "        print(\"Saving to directory: {}\".format(dir_path))\n",
    "\n",
    "        # Save Photos\n",
    "        for idx, photo_link in enumerate(self.data['photo_links'], 0):\n",
    "            sys.stdout.write(\"\\033[F\")\n",
    "            print(\"Downloading {} images to \".format(idx + 1))\n",
    "            # Filename\n",
    "            _, ext = os.path.splitext(photo_link)\n",
    "            filename = str(idx) + ext\n",
    "            filepath = os.path.join(dir_path, filename)\n",
    "            # Send image request\n",
    "            urlretrieve(photo_link, filepath)\n",
    "\n",
    "        # Save Captions\n",
    "        for idx, caption in enumerate(self.data['captions'], 0):\n",
    "\n",
    "            filename = str(idx) + '.txt'\n",
    "            filepath = os.path.join(dir_path, filename)\n",
    "\n",
    "            with codecs.open(filepath, 'w', encoding='utf-8') as fout:\n",
    "                fout.write(caption + '\\n')\n",
    "\n",
    "        # Save followers/following\n",
    "        filename = crawl_type + '.txt'\n",
    "        filepath = os.path.join(dir_path, filename)\n",
    "        if len(self.data[crawl_type]):\n",
    "            with codecs.open(filepath, 'w', encoding='utf-8') as fout:\n",
    "                for fol in self.data[crawl_type]:\n",
    "                    fout.write(fol + '\\n')\n",
    "\n",
    "\n",
    "def main():\n",
    "    #   Arguments  #\n",
    "    parser = argparse.ArgumentParser(description='Instagram Crawler')\n",
    "    parser.add_argument('-d', '--dir_prefix', type=str,\n",
    "                        default='./data/', help='directory to save results')\n",
    "    parser.add_argument('-q', '--query', type=str, default='instagram',\n",
    "                        help=\"target to crawl, add '#' for hashtags\")\n",
    "    parser.add_argument('-t', '--crawl_type', type=str,\n",
    "                        default='photos', help=\"Options: 'photos' | 'followers' | 'following'\")\n",
    "    parser.add_argument('-n', '--number', type=int, default=0,\n",
    "                        help='Number of posts to download: integer')\n",
    "    parser.add_argument('-c', '--caption', action='store_true',\n",
    "                        help='Add this flag to download caption when downloading photos')\n",
    "    parser.add_argument('-l', '--headless', action='store_true',\n",
    "                        help='If set, will use PhantomJS driver to run script as headless')\n",
    "    parser.add_argument('-a', '--authentication', type=str, default=None,\n",
    "                        help='path to authentication json file')\n",
    "    args = parser.parse_args()\n",
    "    #  End Argparse #\n",
    "\n",
    "    crawler = InstagramCrawler(headless=args.headless)\n",
    "    crawler.crawl(dir_prefix=args.dir_prefix,\n",
    "                  query=args.query,\n",
    "                  crawl_type=args.crawl_type,\n",
    "                  number=args.number,\n",
    "                  caption=args.caption,\n",
    "                  authentication=args.authentication)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
